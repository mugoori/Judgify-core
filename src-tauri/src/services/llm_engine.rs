use reqwest::Client;
use serde::{Deserialize, Serialize};
use uuid::Uuid;
use crate::services::judgment_engine::{JudgmentInput, JudgmentResult};
use crate::database::Database;

#[derive(Serialize)]
struct OpenAIRequest {
    model: String,
    messages: Vec<Message>,
    temperature: f32,
}

#[derive(Serialize, Deserialize)]
struct Message {
    role: String,
    content: String,
}

#[derive(Deserialize)]
struct OpenAIResponse {
    choices: Vec<Choice>,
}

#[derive(Deserialize)]
struct Choice {
    message: Message,
}

pub struct LLMEngine {
    client: Client,
    api_key: String,
    db: Database,
}

impl LLMEngine {
    pub fn new() -> anyhow::Result<Self> {
        let api_key = std::env::var("OPENAI_API_KEY")
            .unwrap_or_else(|_| "sk-test-key".to_string());

        Ok(Self {
            client: Client::new(),
            api_key,
            db: Database::new()?,
        })
    }

    pub async fn evaluate(&self, input: &JudgmentInput) -> anyhow::Result<JudgmentResult> {
        // Few-shot í•™ìŠµ ìƒ˜í”Œ ê°€ì ¸ì˜¤ê¸° (10-20ê°œ)
        let few_shot_samples = self.get_few_shot_samples(&input.workflow_id, 15)?;

        let prompt = self.build_prompt(input, &few_shot_samples)?;

        let mut messages = vec![
            Message {
                role: "system".to_string(),
                content: "ë‹¹ì‹ ì€ ì œì¡° í’ˆì§ˆ íŒë‹¨ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ í•©ê²©/ë¶ˆí•©ê²©ì„ íŒë‹¨í•˜ê³ , ê·¸ ì´ìœ ë¥¼ ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.\n\nì‘ë‹µ í˜•ì‹:\níŒë‹¨: [í•©ê²©/ë¶ˆí•©ê²©]\nì´ìœ : [ìƒì„¸ ì„¤ëª…]\nì‹ ë¢°ë„: [0.0-1.0]".to_string(),
            },
        ];

        // Few-shot ì˜ˆì‹œë¥¼ ë©”ì‹œì§€ì— ì¶”ê°€
        for sample in &few_shot_samples {
            messages.push(Message {
                role: "user".to_string(),
                content: format!("ì…ë ¥ ë°ì´í„°:\n{}", sample.input_data),
            });
            messages.push(Message {
                role: "assistant".to_string(),
                content: format!(
                    "íŒë‹¨: {}\nì´ìœ : ì´ì „ ì‚¬ë¡€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ íŒë‹¨ì…ë‹ˆë‹¤.",
                    if sample.expected_result { "í•©ê²©" } else { "ë¶ˆí•©ê²©" }
                ),
            });
        }

        // í˜„ì¬ ìš”ì²­ ì¶”ê°€
        messages.push(Message {
            role: "user".to_string(),
            content: prompt,
        });

        let request = OpenAIRequest {
            model: "gpt-4".to_string(),
            messages,
            temperature: 0.3,
        };

        let response = self
            .client
            .post("https://api.openai.com/v1/chat/completions")
            .header("Authorization", format!("Bearer {}", self.api_key))
            .json(&request)
            .send()
            .await?
            .json::<OpenAIResponse>()
            .await?;

        let llm_response = &response.choices[0].message.content;
        let (result, confidence, explanation) = self.parse_llm_response(llm_response)?;

        // Few-shot ìƒ˜í”Œ ìˆ˜ì— ë”°ë¼ ì‹ ë¢°ë„ ë³´ì •
        let adjusted_confidence = if few_shot_samples.len() >= 10 {
            (confidence * 1.1).min(1.0) // 10ê°œ ì´ìƒ ìƒ˜í”Œì´ ìˆìœ¼ë©´ ì‹ ë¢°ë„ í–¥ìƒ
        } else if few_shot_samples.len() >= 5 {
            confidence
        } else {
            confidence * 0.9 // ìƒ˜í”Œì´ ë¶€ì¡±í•˜ë©´ ì‹ ë¢°ë„ ê°ì†Œ
        };

        Ok(JudgmentResult {
            id: Uuid::new_v4().to_string(),
            workflow_id: input.workflow_id.clone(),
            result,
            confidence: adjusted_confidence,
            method_used: if few_shot_samples.is_empty() { "llm".to_string() } else { "llm_few_shot".to_string() },
            explanation: format!(
                "{}\n\nğŸ“š Few-shot í•™ìŠµ: {} ê°œ ìœ ì‚¬ ì‚¬ë¡€ ì°¸ì¡°",
                explanation,
                few_shot_samples.len()
            ),
        })
    }

    fn get_few_shot_samples(&self, workflow_id: &str, limit: u32) -> anyhow::Result<Vec<crate::database::TrainingSample>> {
        // ì •í™•ë„ê°€ ë†’ì€ í›ˆë ¨ ìƒ˜í”Œë§Œ ê°€ì ¸ì˜¤ê¸° (accuracy >= 0.8)
        Ok(self.db.get_training_samples(workflow_id, limit)
            .unwrap_or_default()
            .into_iter()
            .filter(|s| s.accuracy.unwrap_or(0.0) >= 0.8)
            .collect())
    }

    fn build_prompt(&self, input: &JudgmentInput, few_shot_samples: &[crate::database::TrainingSample]) -> anyhow::Result<String> {
        let mut prompt = String::new();

        if !few_shot_samples.is_empty() {
            prompt.push_str(&format!("ì•„ë˜ {} ê°œì˜ ìœ ì‚¬ ì‚¬ë¡€ë¥¼ ì°¸ê³ í•˜ì—¬ íŒë‹¨í•˜ì„¸ìš”:\n\n", few_shot_samples.len()));
            for (idx, sample) in few_shot_samples.iter().enumerate().take(5) {
                prompt.push_str(&format!(
                    "ì‚¬ë¡€ {}:\nì…ë ¥: {}\nê²°ê³¼: {}\nì •í™•ë„: {:.1}%\n\n",
                    idx + 1,
                    sample.input_data,
                    if sample.expected_result { "í•©ê²©" } else { "ë¶ˆí•©ê²©" },
                    sample.accuracy.unwrap_or(0.0) * 100.0
                ));
            }
            prompt.push_str("---\n\n");
        }

        prompt.push_str(&format!(
            "ë‹¤ìŒ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ í’ˆì§ˆ í•©ê²©/ë¶ˆí•©ê²©ì„ íŒë‹¨í•˜ì„¸ìš”:\n\nì…ë ¥ ë°ì´í„°:\n{}",
            serde_json::to_string_pretty(&input.input_data)?
        ));

        Ok(prompt)
    }

    fn parse_llm_response(&self, response: &str) -> anyhow::Result<(bool, f64, String)> {
        let result = response.contains("í•©ê²©") && !response.contains("ë¶ˆí•©ê²©");

        // ì‹ ë¢°ë„ íŒŒì‹± ì‹œë„
        let confidence = if let Some(conf_str) = response.split("ì‹ ë¢°ë„:").nth(1) {
            conf_str
                .trim()
                .split_whitespace()
                .next()
                .and_then(|s| s.parse::<f64>().ok())
                .unwrap_or(0.8)
        } else {
            0.8 // ê¸°ë³¸ ì‹ ë¢°ë„
        };

        Ok((result, confidence, response.to_string()))
    }
}

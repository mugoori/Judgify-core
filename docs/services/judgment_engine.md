# íŒë‹¨ ì½”ì–´ ì—”ì§„ ìƒì„¸ ì„¤ê³„ì„œ

**ë¬¸ì„œ ë²„ì „**: v2.0
**ì‘ì„±ì¼**: 2024.08.05
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-10-31 (Judgment Service ì „ì²´ ë¬¸ì„œí™” ì™„ë£Œ ğŸ‰)
**ì™„ë£Œìœ¨**: 100% âœ… (ì„¹ì…˜ 1-12 ì „ì²´ ì™„ë£Œ)
**ëŒ€ìƒ**: ë°±ì—”ë“œ ê°œë°œì, AI ì—”ì§€ë‹ˆì–´, ì‹œìŠ¤í…œ ì•„í‚¤í…íŠ¸, Frontend ê°œë°œì
**ëª©ì **: Judgment Serviceì˜ ë‚´ë¶€ êµ¬í˜„ ë° í•µì‹¬ íŒë‹¨ ë¡œì§ ì •ì˜

## ğŸ†• Ver2.0 Final ì£¼ìš” ë³€ê²½ì‚¬í•­ (2025-10-30)
- âœ… **Few-shot í•™ìŠµ í†µí•©**: Learning Serviceì™€ ì—°ë™í•˜ì—¬ ìœ ì‚¬ ì‚¬ë¡€ 15ê°œ ìë™ ê²€ìƒ‰
- âœ… **í•˜ì´ë¸Œë¦¬ë“œ íŒë‹¨ ê°œì„ **: Rule Engine â†’ LLM + Few-shot ìˆœì°¨ ì‹¤í–‰
- âœ… **ì‹ ë¢°ë„ í–¥ìƒ**: Few-shot ìƒ˜í”Œ ê°œìˆ˜ì— ë”°ë¥¸ ë™ì  ì‹ ë¢°ë„ ë³´ì •
- âœ… **í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€**: 3ê°œ í•µì‹¬ í…ŒìŠ¤íŠ¸ ì¶”ê°€ (ì´ 28ê°œ í†µê³¼)

## ğŸ“‹ 1. ê°œìš” ë° ì„¤ê³„ ì›ì¹™

### 1.1 í•µì‹¬ ì±…ì„
- **ì›Œí¬í”Œë¡œìš° í•´ì„**: JSON ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤í–‰ ê°€ëŠ¥í•œ ë¡œì§ìœ¼ë¡œ ë³€í™˜
- **í•˜ì´ë¸Œë¦¬ë“œ íŒë‹¨**: Rule ê¸°ë°˜ê³¼ LLM ê¸°ë°˜ íŒë‹¨ì˜ ì¡°í•©
- **ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬**: MCPë¥¼ í†µí•œ ì™¸ë¶€ ë°ì´í„° ìˆ˜ì§‘ ë° í™œìš©
- **ì‹ ë¢°ë„ í‰ê°€**: íŒë‹¨ ê²°ê³¼ì˜ ì‹ ë¢°ë„ ì‚°ì¶œ
- **ì„¤ëª… ìƒì„±**: íŒë‹¨ ê·¼ê±°ì™€ ì„¤ëª… ì œê³µ

### 1.2 ì„¤ê³„ ì›ì¹™
```python
# SOLID ì›ì¹™ ì ìš©
# S: ê° í´ë˜ìŠ¤ëŠ” ë‹¨ì¼ ì±…ì„
# O: ìƒˆë¡œìš´ íŒë‹¨ ë°©ì‹ í™•ì¥ ê°€ëŠ¥
# L: íŒë‹¨ ì¸í„°í˜ì´ìŠ¤ ì¼ê´€ì„±
# I: í•„ìš”í•œ ì¸í„°í˜ì´ìŠ¤ë§Œ êµ¬í˜„
# D: ì˜ì¡´ì„± ì—­ì „ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ìš©ì´ì„± í™•ë³´
```

## ğŸ— 2. ì•„í‚¤í…ì²˜ êµ¬ì¡°

### 2.1 ì»´í¬ë„ŒíŠ¸ ë‹¤ì´ì–´ê·¸ë¨
```mermaid
graph TB
    subgraph "Judgment Service"
        DISPATCHER[Judgment Dispatcher]
        EXECUTOR[Workflow Executor]
        
        subgraph "Judgment Engines"
            RULE[Rule Engine]
            LLM[LLM Engine]
            HYBRID[Hybrid Engine]
        end
        
        subgraph "Support Components"
            CONTEXT[Context Manager]
            VALIDATOR[Input Validator]
            EXPLAINER[Explanation Generator]
        end
    end
    
    subgraph "External Dependencies"
        MCP[MCP Client]
        OPENAI[OpenAI API]
        DB[(PostgreSQL)]
        CACHE[(Redis)]
    end
    
    DISPATCHER --> EXECUTOR
    EXECUTOR --> RULE
    EXECUTOR --> LLM
    EXECUTOR --> HYBRID
    
    CONTEXT --> MCP
    LLM --> OPENAI
    EXPLAINER --> OPENAI
    
    EXECUTOR --> DB
    CONTEXT --> CACHE
```

### 2.2 í•µì‹¬ í´ë˜ìŠ¤ êµ¬ì¡°
```python
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional
from pydantic import BaseModel
from enum import Enum


class JudgmentMethod(str, Enum):
    RULE = "rule"
    LLM = "llm"
    HYBRID = "hybrid"


class JudgmentInput(BaseModel):
    workflow_id: str
    input_data: Dict[str, Any]
    context: Optional[Dict[str, Any]] = None
    method: Optional[JudgmentMethod] = None


class JudgmentResult(BaseModel):
    result: Any
    confidence: float
    method_used: JudgmentMethod
    execution_time_ms: int
    explanation: Optional[str] = None
    error: Optional[str] = None


class JudgmentEngine(ABC):
    """íŒë‹¨ ì—”ì§„ ì¶”ìƒ í´ë˜ìŠ¤"""
    
    @abstractmethod
    async def judge(self, input_data: JudgmentInput) -> JudgmentResult:
        pass
    
    @abstractmethod
    def validate_input(self, input_data: JudgmentInput) -> bool:
        pass
class DashboardGenerationEngine:
    """ëŒ€ì‹œë³´ë“œ ìë™ ìƒì„± ì—”ì§„"""
    
    def __init__(self, llm_client, data_analyzer):
        self.llm_client = llm_client
        self.data_analyzer = data_analyzer
        self.component_generator = ComponentGenerator()
        
    async def generate_dashboard(self, request: str, context: dict):
        # êµ¬í˜„ ë¡œì§
        pass
```

## ğŸ”§ 3. Rule Engine ìƒì„¸ êµ¬í˜„

### 3.1 ì•ˆì „í•œ Rule DSL íŒŒì„œ
```python
import ast
import operator
from typing import Any, Dict, Set


class SafeRuleEngine(JudgmentEngine):
    """AST ê¸°ë°˜ ì•ˆì „í•œ Rule ì‹¤í–‰ ì—”ì§„"""
    
    # í—ˆìš©ëœ ì—°ì‚°ì ì •ì˜
    ALLOWED_OPERATORS = {
        ast.Gt: operator.gt,
        ast.Lt: operator.lt,
        ast.GtE: operator.ge,
        ast.LtE: operator.le,
        ast.Eq: operator.eq,
        ast.NotEq: operator.ne,
        ast.And: operator.and_,
        ast.Or: operator.or_,
        ast.Not: operator.not_,
        ast.In: lambda x, y: x in y,
        ast.NotIn: lambda x, y: x not in y,
    }
    
    # í—ˆìš©ëœ í•¨ìˆ˜
    ALLOWED_FUNCTIONS = {
        'abs': abs,
        'min': min,
        'max': max,
        'len': len,
        'round': round,
    }
    
    def __init__(self):
        self.variables: Dict[str, Any] = {}
    
    async def judge(self, input_data: JudgmentInput) -> JudgmentResult:
        start_time = time.time()
        
        try:
            # ì›Œí¬í”Œë¡œìš°ì—ì„œ ê·œì¹™ ì¶”ì¶œ
            workflow = await self.get_workflow(input_data.workflow_id)
            rule_expression = workflow.get('rule_expression')
            
            if not rule_expression:
                raise ValueError("Rule expression not found in workflow")
            
            # ë³€ìˆ˜ ë°”ì¸ë”©
            self.variables = input_data.input_data.copy()
            if input_data.context:
                self.variables.update(input_data.context)
            
            # ê·œì¹™ ì‹¤í–‰
            result = self._evaluate_expression(rule_expression)
            
            execution_time = int((time.time() - start_time) * 1000)
            
            return JudgmentResult(
                result=result,
                confidence=1.0,  # Rule ê¸°ë°˜ì€ í•­ìƒ í™•ì‹ 
                method_used=JudgmentMethod.RULE,
                execution_time_ms=execution_time,
                explanation=f"Rule '{rule_expression}' evaluated to {result}"
            )
            
        except Exception as e:
            execution_time = int((time.time() - start_time) * 1000)
            return JudgmentResult(
                result=None,
                confidence=0.0,
                method_used=JudgmentMethod.RULE,
                execution_time_ms=execution_time,
                error=str(e)
            )
    
    def _evaluate_expression(self, expression: str) -> Any:
        """ASTë¥¼ ì‚¬ìš©í•œ ì•ˆì „í•œ í‘œí˜„ì‹ í‰ê°€"""
        try:
            tree = ast.parse(expression, mode='eval')
            return self._evaluate_node(tree.body)
        except SyntaxError as e:
            raise ValueError(f"Invalid syntax in rule expression: {e}")
    
    def _evaluate_node(self, node: ast.AST) -> Any:
        """AST ë…¸ë“œ ì¬ê·€ì  í‰ê°€"""
        if isinstance(node, ast.Constant):
            return node.value
        
        elif isinstance(node, ast.Name):
            if node.id in self.variables:
                return self.variables[node.id]
            else:
                raise NameError(f"Variable '{node.id}' is not defined")
        
        elif isinstance(node, ast.BinOp):
            left = self._evaluate_node(node.left)
            right = self._evaluate_node(node.right)
            op_func = self.ALLOWED_OPERATORS.get(type(node.op))
            
            if op_func:
                return op_func(left, right)
            else:
                raise ValueError(f"Operator {type(node.op)} not allowed")
        
        elif isinstance(node, ast.Compare):
            left = self._evaluate_node(node.left)
            
            for op, comparator in zip(node.ops, node.comparators):
                right = self._evaluate_node(comparator)
                op_func = self.ALLOWED_OPERATORS.get(type(op))
                
                if not op_func:
                    raise ValueError(f"Comparison {type(op)} not allowed")
                
                if not op_func(left, right):
                    return False
                left = right
            
            return True
        
        elif isinstance(node, ast.BoolOp):
            op_func = self.ALLOWED_OPERATORS.get(type(node.op))
            if not op_func:
                raise ValueError(f"Boolean operator {type(node.op)} not allowed")
            
            values = [self._evaluate_node(value) for value in node.values]
            
            if isinstance(node.op, ast.And):
                return all(values)
            elif isinstance(node.op, ast.Or):
                return any(values)
        
        elif isinstance(node, ast.UnaryOp):
            operand = self._evaluate_node(node.operand)
            op_func = self.ALLOWED_OPERATORS.get(type(node.op))
            
            if op_func:
                return op_func(operand)
            else:
                raise ValueError(f"Unary operator {type(node.op)} not allowed")
        
        elif isinstance(node, ast.Call):
            func_name = node.func.id if isinstance(node.func, ast.Name) else None
            
            if func_name in self.ALLOWED_FUNCTIONS:
                args = [self._evaluate_node(arg) for arg in node.args]
                return self.ALLOWED_FUNCTIONS[func_name](*args)
            else:
                raise ValueError(f"Function '{func_name}' not allowed")
        
        elif isinstance(node, ast.List):
            return [self._evaluate_node(item) for item in node.elts]
        
        else:
            raise ValueError(f"Node type {type(node)} not supported")
    
    def validate_input(self, input_data: JudgmentInput) -> bool:
        """ì…ë ¥ ë°ì´í„° ê²€ì¦"""
        required_fields = self._extract_variables_from_workflow(
            input_data.workflow_id
        )
        
        for field in required_fields:
            if field not in input_data.input_data:
                return False
        
        return True
    
    def _extract_variables_from_workflow(self, workflow_id: str) -> Set[str]:
        """ì›Œí¬í”Œë¡œìš°ì—ì„œ í•„ìš”í•œ ë³€ìˆ˜ ì¶”ì¶œ"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì›Œí¬í”Œë¡œìš° ì •ì˜ë¥¼ íŒŒì‹±í•˜ì—¬ ë³€ìˆ˜ ì¶”ì¶œ
        return set()
```

### 3.2 Rule DSL ì˜ˆì‹œ
```python
# ì§€ì›í•˜ëŠ” Rule í‘œí˜„ì‹ ì˜ˆì‹œ
RULE_EXAMPLES = {
    "simple_comparison": "temperature > 85",
    "multiple_conditions": "temperature > 85 and vibration > 40",
    "range_check": "temperature >= 80 and temperature <= 100",
    "list_membership": "status in ['ERROR', 'WARNING']",
    "function_usage": "abs(pressure - target_pressure) > threshold",
    "complex_logic": "(temperature > 85 or pressure > 100) and status != 'MAINTENANCE'"
}
```

## ğŸ¤– 4. LLM Engine ìƒì„¸ êµ¬í˜„

### 4.1 LLM íŒë‹¨ ì—”ì§„
```python
import openai
import json
from typing import Dict, Any
import asyncio


class LLMJudgmentEngine(JudgmentEngine):
    """OpenAI API ê¸°ë°˜ LLM íŒë‹¨ ì—”ì§„"""
    
    def __init__(self, api_key: str, model: str = "gpt-4"):
        self.client = openai.AsyncOpenAI(api_key=api_key)
        self.model = model
        self.max_retries = 3
        self.timeout = 30
    
    async def judge(self, input_data: JudgmentInput) -> JudgmentResult:
        start_time = time.time()
        
        try:
            # ì›Œí¬í”Œë¡œìš°ì—ì„œ íŒë‹¨ ê¸°ì¤€ ì¶”ì¶œ
            workflow = await self.get_workflow(input_data.workflow_id)
            judgment_criteria = workflow.get('llm_criteria', '')
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = await self._build_judgment_prompt(
                input_data.input_data,
                input_data.context or {},
                judgment_criteria
            )
            
            # LLM í˜¸ì¶œ
            response = await self._call_llm_with_retry(prompt)
            
            # ì‘ë‹µ íŒŒì‹±
            parsed_result = self._parse_llm_response(response)
            
            execution_time = int((time.time() - start_time) * 1000)
            
            return JudgmentResult(
                result=parsed_result['result'],
                confidence=parsed_result.get('confidence', 0.5),
                method_used=JudgmentMethod.LLM,
                execution_time_ms=execution_time,
                explanation=parsed_result.get('explanation', '')
            )
            
        except Exception as e:
            execution_time = int((time.time() - start_time) * 1000)
            return JudgmentResult(
                result=None,
                confidence=0.0,
                method_used=JudgmentMethod.LLM,
                execution_time_ms=execution_time,
                error=str(e)
            )
    
    async def _build_judgment_prompt(
        self, 
        input_data: Dict[str, Any],
        context: Dict[str, Any],
        criteria: str
    ) -> str:
        """íŒë‹¨ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        prompt_template = """
ë‹¹ì‹ ì€ ì œì¡°ì—… í˜„ì¥ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ íŒë‹¨ì„ ë‚´ë ¤ì£¼ì„¸ìš”.

## ì…ë ¥ ë°ì´í„°
{input_data_formatted}

## ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸
{context_formatted}

## íŒë‹¨ ê¸°ì¤€
{criteria}

## ì‘ë‹µ í˜•ì‹
ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "result": true/false ë˜ëŠ” êµ¬ì²´ì ì¸ ê°’,
    "confidence": 0.0~1.0 ì‚¬ì´ì˜ ì‹ ë¢°ë„,
    "explanation": "íŒë‹¨ ê·¼ê±°ì— ëŒ€í•œ ìƒì„¸í•œ ì„¤ëª…"
}}

íŒë‹¨ì„ ë‚´ë ¤ì£¼ì„¸ìš”:
        """.strip()
        
        input_data_formatted = json.dumps(input_data, indent=2, ensure_ascii=False)
        context_formatted = json.dumps(context, indent=2, ensure_ascii=False)
        
        return prompt_template.format(
            input_data_formatted=input_data_formatted,
            context_formatted=context_formatted,
            criteria=criteria
        )
    
    async def _call_llm_with_retry(self, prompt: str) -> str:
        """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ LLM í˜¸ì¶œ"""
        
        for attempt in range(self.max_retries):
            try:
                response = await asyncio.wait_for(
                    self.client.chat.completions.create(
                        model=self.model,
                        messages=[
                            {
                                "role": "system", 
                                "content": "ë‹¹ì‹ ì€ ì œì¡°ì—… í˜„ì¥ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” íŒë‹¨ì„ ë‚´ë ¤ì£¼ì„¸ìš”."
                            },
                            {"role": "user", "content": prompt}
                        ],
                        temperature=0.1,
                        max_tokens=500
                    ),
                    timeout=self.timeout
                )
                
                return response.choices[0].message.content
                
            except asyncio.TimeoutError:
                if attempt == self.max_retries - 1:
                    raise TimeoutError("LLM response timeout")
                await asyncio.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
                
            except Exception as e:
                if attempt == self.max_retries - 1:
                    raise e
                await asyncio.sleep(1)
    
    def _parse_llm_response(self, response: str) -> Dict[str, Any]:
        """LLM ì‘ë‹µ íŒŒì‹± ë° ê²€ì¦"""
        try:
            # JSON ì‘ë‹µ íŒŒì‹± ì‹œë„
            if response.strip().startswith('{'):
                parsed = json.loads(response)
                
                # í•„ìˆ˜ í•„ë“œ ê²€ì¦
                if 'result' not in parsed:
                    raise ValueError("Missing 'result' field in LLM response")
                
                # ì‹ ë¢°ë„ ê²€ì¦
                confidence = parsed.get('confidence', 0.5)
                if not 0 <= confidence <= 1:
                    parsed['confidence'] = 0.5
                
                return parsed
            
            else:
                # JSONì´ ì•„ë‹Œ ê²½ìš° íœ´ë¦¬ìŠ¤í‹± íŒŒì‹±
                return self._heuristic_parse(response)
                
        except json.JSONDecodeError:
            return self._heuristic_parse(response)
    
    def _heuristic_parse(self, response: str) -> Dict[str, Any]:
        """JSONì´ ì•„ë‹Œ ì‘ë‹µì— ëŒ€í•œ íœ´ë¦¬ìŠ¤í‹± íŒŒì‹±"""
        
        # ê¸ì •/ë¶€ì • í‚¤ì›Œë“œ ê²€ìƒ‰
        positive_keywords = ['yes', 'true', 'í•„ìš”', 'í•´ì•¼', 'ê¶Œì¥']
        negative_keywords = ['no', 'false', 'ë¶ˆí•„ìš”', 'ì•Šì•„ë„', 'ê¶Œì¥í•˜ì§€']
        
        response_lower = response.lower()
        
        positive_score = sum(1 for keyword in positive_keywords if keyword in response_lower)
        negative_score = sum(1 for keyword in negative_keywords if keyword in response_lower)
        
        if positive_score > negative_score:
            result = True
            confidence = min(0.9, 0.5 + positive_score * 0.1)
        elif negative_score > positive_score:
            result = False
            confidence = min(0.9, 0.5 + negative_score * 0.1)
        else:
            result = None
            confidence = 0.3
        
        return {
            'result': result,
            'confidence': confidence,
            'explanation': response[:200] + '...' if len(response) > 200 else response
        }
    
    def validate_input(self, input_data: JudgmentInput) -> bool:
        """LLM ì…ë ¥ ê²€ì¦"""
        # ê¸°ë³¸ì ì¸ ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ë§Œ í™•ì¸
        return bool(input_data.input_data)
```

## ğŸ”„ 5. Hybrid Engine êµ¬í˜„

### 5.1 í•˜ì´ë¸Œë¦¬ë“œ íŒë‹¨ ë¡œì§
```python
class HybridJudgmentEngine(JudgmentEngine):
    """Ruleê³¼ LLMì„ ì¡°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ íŒë‹¨ ì—”ì§„"""
    
    def __init__(self, rule_engine: SafeRuleEngine, llm_engine: LLMJudgmentEngine):
        self.rule_engine = rule_engine
        self.llm_engine = llm_engine
    
    async def judge(self, input_data: JudgmentInput) -> JudgmentResult:
        start_time = time.time()
        
        try:
            workflow = await self.get_workflow(input_data.workflow_id)
            strategy = workflow.get('hybrid_strategy', 'rule_first')
            
            if strategy == 'rule_first':
                return await self._rule_first_strategy(input_data)
            elif strategy == 'llm_first':
                return await self._llm_first_strategy(input_data)
            elif strategy == 'parallel':
                return await self._parallel_strategy(input_data)
            elif strategy == 'consensus':
                return await self._consensus_strategy(input_data)
            else:
                raise ValueError(f"Unknown hybrid strategy: {strategy}")
                
        except Exception as e:
            execution_time = int((time.time() - start_time) * 1000)
            return JudgmentResult(
                result=None,
                confidence=0.0,
                method_used=JudgmentMethod.HYBRID,
                execution_time_ms=execution_time,
                error=str(e)
            )
    
    async def _rule_first_strategy(self, input_data: JudgmentInput) -> JudgmentResult:
        """Rule ìš°ì„  ì „ëµ: Rule ì‹¤íŒ¨ì‹œì—ë§Œ LLM ì‚¬ìš©"""
        
        # Rule ì‹œë„
        rule_result = await self.rule_engine.judge(input_data)
        
        if rule_result.error is None:
            # Rule ì„±ê³µ
            rule_result.method_used = JudgmentMethod.HYBRID
            rule_result.explanation = f"Rule-based: {rule_result.explanation}"
            return rule_result
        
        # Rule ì‹¤íŒ¨ì‹œ LLM ì‚¬ìš©
        llm_input = input_data.model_copy()
        llm_input.context = llm_input.context or {}
        llm_input.context['rule_error'] = rule_result.error
        
        llm_result = await self.llm_engine.judge(llm_input)
        llm_result.method_used = JudgmentMethod.HYBRID
        llm_result.explanation = f"LLM-based (Rule failed): {llm_result.explanation}"
        
        return llm_result
    
    async def _consensus_strategy(self, input_data: JudgmentInput) -> JudgmentResult:
        """í•©ì˜ ì „ëµ: Ruleê³¼ LLM ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… íŒë‹¨"""
        
        # ë³‘ë ¬ ì‹¤í–‰
        rule_task = asyncio.create_task(self.rule_engine.judge(input_data))
        llm_task = asyncio.create_task(self.llm_engine.judge(input_data))
        
        rule_result, llm_result = await asyncio.gather(rule_task, llm_task)
        
        # ê²°ê³¼ ë¶„ì„
        if rule_result.error and llm_result.error:
            # ë‘˜ ë‹¤ ì‹¤íŒ¨
            return JudgmentResult(
                result=None,
                confidence=0.0,
                method_used=JudgmentMethod.HYBRID,
                execution_time_ms=rule_result.execution_time_ms + llm_result.execution_time_ms,
                error="Both rule and LLM engines failed"
            )
        
        elif rule_result.error:
            # Rule ì‹¤íŒ¨, LLM ì„±ê³µ
            llm_result.method_used = JudgmentMethod.HYBRID
            return llm_result
        
        elif llm_result.error:
            # LLM ì‹¤íŒ¨, Rule ì„±ê³µ
            rule_result.method_used = JudgmentMethod.HYBRID
            return rule_result
        
        else:
            # ë‘˜ ë‹¤ ì„±ê³µ - í•©ì˜ ì•Œê³ ë¦¬ì¦˜ ì ìš©
            return self._merge_results(rule_result, llm_result)
    
    def _merge_results(self, rule_result: JudgmentResult, llm_result: JudgmentResult) -> JudgmentResult:
        """Ruleê³¼ LLM ê²°ê³¼ ë³‘í•©"""
        
        # ê²°ê³¼ê°€ ë™ì¼í•œ ê²½ìš°
        if rule_result.result == llm_result.result:
            combined_confidence = (rule_result.confidence + llm_result.confidence) / 2
            combined_confidence = min(combined_confidence * 1.2, 1.0)  # í•©ì˜ ë³´ë„ˆìŠ¤
            
            return JudgmentResult(
                result=rule_result.result,
                confidence=combined_confidence,
                method_used=JudgmentMethod.HYBRID,
                execution_time_ms=rule_result.execution_time_ms + llm_result.execution_time_ms,
                explanation=f"Consensus: Rule({rule_result.result}) + LLM({llm_result.result})"
            )
        
        # ê²°ê³¼ê°€ ë‹¤ë¥¸ ê²½ìš° - ì‹ ë¢°ë„ ê¸°ë°˜ ì„ íƒ
        else:
            if rule_result.confidence > llm_result.confidence:
                chosen_result = rule_result
                explanation = f"Rule-preferred: Rule({rule_result.confidence:.2f}) > LLM({llm_result.confidence:.2f})"
            else:
                chosen_result = llm_result
                explanation = f"LLM-preferred: LLM({llm_result.confidence:.2f}) > Rule({rule_result.confidence:.2f})"
            
            chosen_result.method_used = JudgmentMethod.HYBRID
            chosen_result.explanation = explanation
            chosen_result.execution_time_ms = rule_result.execution_time_ms + llm_result.execution_time_ms
            
            return chosen_result
    
    def validate_input(self, input_data: JudgmentInput) -> bool:
        """í•˜ì´ë¸Œë¦¬ë“œ ì…ë ¥ ê²€ì¦"""
        return (self.rule_engine.validate_input(input_data) or 
                self.llm_engine.validate_input(input_data))
```

## ğŸ“Š 6. Context Manager êµ¬í˜„

### 6.1 MCP ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘
```python
from typing import Dict, Any, List
import aiohttp


class MCPContextManager:
    """MCP(Model Context Protocol)ë¥¼ í†µí•œ ì»¨í…ìŠ¤íŠ¸ ë°ì´í„° ìˆ˜ì§‘"""
    
    def __init__(self, mcp_servers: List[Dict[str, str]]):
        self.mcp_servers = mcp_servers
        self.cache_ttl = 300  # 5ë¶„ ìºì‹œ
    
    async def gather_context(self, workflow_id: str, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì›Œí¬í”Œë¡œìš°ì™€ ì…ë ¥ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•„ìš”í•œ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘"""
        
        workflow = await self.get_workflow(workflow_id)
        required_context = workflow.get('required_context', [])
        
        context = {}
        
        for context_req in required_context:
            context_type = context_req.get('type')
            
            if context_type == 'machine_status':
                machine_id = input_data.get('machine_id')
                if machine_id:
                    context['machine_status'] = await self._get_machine_status(machine_id)
            
            elif context_type == 'historical_data':
                context['historical_data'] = await self._get_historical_data(
                    context_req.get('timeframe', '1h'),
                    input_data
                )
            
            elif context_type == 'policy_documents':
                context['policies'] = await self._get_relevant_policies(
                    context_req.get('category', 'safety')
                )
        
        return context
    
    async def _get_machine_status(self, machine_id: str) -> Dict[str, Any]:
        """MCPë¥¼ í†µí•œ ê¸°ê³„ ìƒíƒœ ì¡°íšŒ"""
        
        mcp_request = {
            "method": "tools/call",
            "params": {
                "name": "get_machine_status",
                "arguments": {"machine_id": machine_id}
            }
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                for server in self.mcp_servers:
                    try:
                        async with session.post(
                            f"{server['url']}/mcp",
                            json=mcp_request,
                            headers={"Authorization": f"Bearer {server['token']}"}
                        ) as response:
                            if response.status == 200:
                                result = await response.json()
                                return result.get('content', {})
                    except Exception:
                        continue
            
            return {"status": "unknown", "error": "No MCP server available"}
            
        except Exception as e:
            return {"status": "error", "message": str(e)}
    
    async def _get_historical_data(self, timeframe: str, input_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ê³¼ê±° ë°ì´í„° ì¡°íšŒ"""
        
        # ìºì‹œ í™•ì¸
        cache_key = f"historical:{timeframe}:{hash(str(input_data))}"
        cached_data = await self._get_from_cache(cache_key)
        
        if cached_data:
            return cached_data
        
        # MCPë¥¼ í†µí•œ ë°ì´í„° ì¡°íšŒ
        mcp_request = {
            "method": "tools/call",
            "params": {
                "name": "query_historical_data",
                "arguments": {
                    "timeframe": timeframe,
                    "filters": input_data
                }
            }
        }
        
        async def gather_dashboard_context(self, user_request: str):
        """ëŒ€ì‹œë³´ë“œ ìƒì„±ì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘"""
        context = {}
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° ìŠ¤í‚¤ë§ˆ ìˆ˜ì§‘
        context['available_schemas'] = await self.get_data_schemas()
        
        # ìµœê·¼ ë°ì´í„° ìƒ˜í”Œ ìˆ˜ì§‘
        context['sample_data'] = await self.get_sample_data()
        
        # ì‚¬ìš©ì íˆìŠ¤í† ë¦¬ ë¶„ì„
        context['user_preferences'] = await self.analyze_user_history()
        
        return context
        # ì‹¤ì œ êµ¬í˜„ì€ ìœ„ì™€ ë™ì¼í•œ íŒ¨í„´
        # ...
        
        return []
```

## ğŸ” 7. ì„¤ëª… ìƒì„±ê¸° (Explainer)

### 7.1 íŒë‹¨ ì„¤ëª… ìƒì„±
```python
class JudgmentExplainer:
    """íŒë‹¨ ê²°ê³¼ì— ëŒ€í•œ ì„¤ëª… ìƒì„±"""
    
    def __init__(self, llm_client: openai.AsyncOpenAI):
        self.llm_client = llm_client
    
    async def generate_explanation(
        self, 
        judgment_result: JudgmentResult,
        input_data: JudgmentInput,
        context: Dict[str, Any]
    ) -> str:
        """ìƒì„¸í•œ íŒë‹¨ ì„¤ëª… ìƒì„±"""
        
        if judgment_result.method_used == JudgmentMethod.RULE:
            return self._explain_rule_result(judgment_result, input_data)
        
        elif judgment_result.method_used == JudgmentMethod.LLM:
            return await self._enhance_llm_explanation(judgment_result, input_data, context)
        
        else:  # HYBRID
            return await self._explain_hybrid_result(judgment_result, input_data, context)
    
    def _explain_rule_result(self, result: JudgmentResult, input_data: JudgmentInput) -> str:
        """Rule ê¸°ë°˜ íŒë‹¨ ì„¤ëª…"""
        
        explanation = f"""
## ê·œì¹™ ê¸°ë°˜ íŒë‹¨ ê²°ê³¼

**íŒë‹¨ ê²°ê³¼**: {result.result}
**ì‹ ë¢°ë„**: {result.confidence:.2f}
**ì‹¤í–‰ ì‹œê°„**: {result.execution_time_ms}ms

### ì ìš©ëœ ê·œì¹™
{result.explanation}

### ì…ë ¥ ë°ì´í„°
{json.dumps(input_data.input_data, indent=2, ensure_ascii=False)}

### íŒë‹¨ ê³¼ì •
ê·œì¹™ ì—”ì§„ì´ ì •ì˜ëœ ì¡°ê±´ì‹ì„ í‰ê°€í•˜ì—¬ ëª…í™•í•œ ê²°ê³¼ë¥¼ ë„ì¶œí–ˆìŠµë‹ˆë‹¤.
ê·œì¹™ ê¸°ë°˜ íŒë‹¨ì€ ì¼ê´€ì„±ì´ ë†’ê³  ì˜ˆì¸¡ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
        """.strip()
        
        return explanation
    
    async def _enhance_llm_explanation(
        self, 
        result: JudgmentResult, 
        input_data: JudgmentInput,
        context: Dict[str, Any]
    ) -> str:
        """LLM ì„¤ëª… í–¥ìƒ"""
        
        prompt = f"""
ë‹¤ìŒ AI íŒë‹¨ ê²°ê³¼ì— ëŒ€í•´ ë” ìƒì„¸í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì„¤ëª…ì„ ì œê³µí•´ì£¼ì„¸ìš”.

## íŒë‹¨ ê²°ê³¼
- ê²°ê³¼: {result.result}
- ì‹ ë¢°ë„: {result.confidence:.2f}
- ê¸°ì¡´ ì„¤ëª…: {result.explanation}

## ì…ë ¥ ë°ì´í„°
{json.dumps(input_data.input_data, indent=2, ensure_ascii=False)}

## ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸
{json.dumps(context, indent=2, ensure_ascii=False)}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì„¤ëª…ì„ ì œê³µí•´ì£¼ì„¸ìš”:
1. íŒë‹¨ ìš”ì•½
2. í•µì‹¬ ê·¼ê±°
3. ê³ ë ¤ëœ ìš”ì¸ë“¤
4. ì‹ ë¢°ë„ í‰ê°€ ì´ìœ 
5. ê¶Œì¥ ì¡°ì¹˜ (í•´ë‹¹í•˜ëŠ” ê²½ìš°)
        """
        
        try:
            response = await self.llm_client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì œì¡°ì—… ì „ë¬¸ê°€ì…ë‹ˆë‹¤. íŒë‹¨ ê²°ê³¼ë¥¼ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=800
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            return f"ì„¤ëª… ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n\nì›ë³¸ ì„¤ëª…: {result.explanation}"
```

## ğŸ§ª 8. í…ŒìŠ¤íŠ¸ ì „ëµ

### 8.1 ìœ ë‹› í…ŒìŠ¤íŠ¸
```python
import pytest
from unittest.mock import Mock, AsyncMock


class TestSafeRuleEngine:
    
    @pytest.fixture
    def rule_engine(self):
        return SafeRuleEngine()
    
    @pytest.mark.asyncio
    async def test_simple_comparison(self, rule_engine):
        """ê°„ë‹¨í•œ ë¹„êµ ì—°ì‚° í…ŒìŠ¤íŠ¸"""
        
        rule_engine.variables = {"temperature": 90}
        result = rule_engine._evaluate_expression("temperature > 85")
        
        assert result is True
    
    @pytest.mark.asyncio
    async def test_complex_logic(self, rule_engine):
        """ë³µí•© ë…¼ë¦¬ ì—°ì‚° í…ŒìŠ¤íŠ¸"""
        
        rule_engine.variables = {
            "temperature": 90,
            "pressure": 95,
            "status": "RUNNING"
        }
        
        result = rule_engine._evaluate_expression(
            "(temperature > 85 or pressure > 100) and status != 'MAINTENANCE'"
        )
        
        assert result is True
    
    @pytest.mark.asyncio
    async def test_security_injection(self, rule_engine):
        """ë³´ì•ˆ ì·¨ì•½ì  í…ŒìŠ¤íŠ¸"""
        
        with pytest.raises(ValueError):
            rule_engine._evaluate_expression("__import__('os').system('rm -rf /')")
        
        with pytest.raises(ValueError):
            rule_engine._evaluate_expression("exec('print(1)')")


class TestLLMJudgmentEngine:
    
    @pytest.fixture
    def llm_engine(self):
        mock_client = AsyncMock()
        engine = LLMJudgmentEngine("test-key")
        engine.client = mock_client
        return engine
    
    @pytest.mark.asyncio
    async def test_successful_judgment(self, llm_engine):
        """LLM íŒë‹¨ ì„±ê³µ ì¼€ì´ìŠ¤"""
        
        # Mock LLM ì‘ë‹µ
        mock_response = Mock()
        mock_response.choices = [Mock()]
        mock_response.choices[0].message.content = '{"result": true, "confidence": 0.85, "explanation": "Temperature exceeds threshold"}'
        
        llm_engine.client.chat.completions.create.return_value = mock_response
        
        input_data = JudgmentInput(
            workflow_id="test-workflow",
            input_data={"temperature": 90}
        )
        
        result = await llm_engine.judge(input_data)
        
        assert result.result is True
        assert result.confidence == 0.85
        assert result.method_used == JudgmentMethod.LLM
        assert result.error is None
```

## ğŸ“ˆ 9. ì„±ëŠ¥ ìµœì í™”

### 9.1 ìºì‹± ì „ëµ
```python
class CachedJudgmentService:
    """ìºì‹±ì´ ì ìš©ëœ íŒë‹¨ ì„œë¹„ìŠ¤"""
    
    def __init__(self, redis_client, judgment_engine):
        self.redis = redis_client
        self.engine = judgment_engine
        self.cache_ttl = 300  # 5ë¶„
    
    async def judge_with_cache(self, input_data: JudgmentInput) -> JudgmentResult:
        """ìºì‹œë¥¼ ê³ ë ¤í•œ íŒë‹¨ ì‹¤í–‰"""
        
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = self._generate_cache_key(input_data)
        
        # ìºì‹œ í™•ì¸
        cached_result = await self.redis.get(cache_key)
        if cached_result:
            return JudgmentResult.parse_raw(cached_result)
        
        # ì‹¤ì œ íŒë‹¨ ì‹¤í–‰
        result = await self.engine.judge(input_data)
        
        # ì„±ê³µí•œ ê²°ê³¼ë§Œ ìºì‹œ
        if result.error is None:
            await self.redis.setex(
                cache_key, 
                self.cache_ttl, 
                result.json()
            )
        
        return result
    
    def _generate_cache_key(self, input_data: JudgmentInput) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        
        # ì…ë ¥ ë°ì´í„°ì˜ í•´ì‹œê°’ìœ¼ë¡œ í‚¤ ìƒì„±
        data_hash = hash(str(sorted(input_data.input_data.items())))
        return f"judgment:{input_data.workflow_id}:{data_hash}"
```

## ğŸ”„ 10. Few-shot í•™ìŠµ í†µí•© API (Ver2.0 Final - 2025-10-30 ì™„ì„±)

### 10.1 ê°œìš” ë° ì„¤ê³„ ëª©í‘œ

**í•µì‹¬ ê°œë…**: Learning Serviceì™€ ì—°ë™í•˜ì—¬ **ìœ ì‚¬í•œ ê³¼ê±° ì‚¬ë¡€ 10-20ê°œ**ë¥¼ ìë™ ê²€ìƒ‰í•˜ê³ , LLM íŒë‹¨ì‹œ Few-shot ì˜ˆì‹œë¡œ í™œìš©í•˜ì—¬ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

**ì„¤ê³„ ëª©í‘œ**:
1. **ìë™ í•™ìŠµ**: ì‚¬ìš©ì í”¼ë“œë°± ì—†ì´ë„ ê³¼ê±° íŒë‹¨ ê²°ê³¼ë¥¼ í•™ìŠµ
2. **ì‹ ë¢°ë„ í–¥ìƒ**: Few-shot ìƒ˜í”Œ ê°œìˆ˜ì— ë”°ë¼ LLM ì‹ ë¢°ë„ ë™ì  ë³´ì •
3. **3-Tier íŒë‹¨ ì „ëµ**: Rule â†’ LLM + Few-shot ìˆœì°¨ ì‹¤í–‰ìœ¼ë¡œ ë¹„ìš© ì ˆê°
4. **ì„±ëŠ¥ ìµœì í™”**: ì„ë² ë”© ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ 0.1ì´ˆ ë‚´ ìœ ì‚¬ ìƒ˜í”Œ ê²€ìƒ‰

### 10.2 Few-shot í†µí•© íŒë‹¨ íë¦„

**ì‹¤í–‰ íë¦„ ë‹¤ì´ì–´ê·¸ë¨**:
```mermaid
graph TD
    A[judge_with_few_shot í˜¸ì¶œ] --> B[Learning Serviceì—ì„œ<br/>Few-shot ìƒ˜í”Œ 15ê°œ ê²€ìƒ‰]
    B --> C{Rule Engine ì‹¤í–‰}
    C -->|ì‹ ë¢°ë„ â‰¥ 70%| D[âœ… Rule ê²°ê³¼ ì¦‰ì‹œ ë°˜í™˜<br/>Few-shot ìƒëµ]
    C -->|ì‹ ë¢°ë„ < 70%| E[âš ï¸ LLM + Few-shot ì‹¤í–‰]
    C -->|ì‹¤íŒ¨| F[âŒ LLM + Few-shotë§Œ ì‹¤í–‰]
    E --> G[Rule + LLM ê²°ê³¼ ë³‘í•©<br/>í•˜ì´ë¸Œë¦¬ë“œ íŒë‹¨]
    F --> H[LLM ê²°ê³¼ ë°˜í™˜]
    G --> I[ìµœì¢… ê²°ê³¼ DB ì €ì¥]
    H --> I
    D --> I
```

**í•µì‹¬ ë¡œì§ (Rust êµ¬í˜„)**:
```rust
/// Few-shot í•™ìŠµì„ í¬í•¨í•œ í•˜ì´ë¸Œë¦¬ë“œ íŒë‹¨ (ìƒˆë¡œìš´ ê¸°ë³¸ ë©”ì„œë“œ!)
pub async fn judge_with_few_shot(&self, input: JudgmentInput) -> anyhow::Result<JudgmentResult> {
    // 1. Few-shot ìƒ˜í”Œ ê²€ìƒ‰ (Learning Service)
    let few_shot_samples = self.learning_service
        .get_few_shot_samples(input.workflow_id.clone(), 15)?;

    println!("ğŸ“š Few-shot ìƒ˜í”Œ ê°œìˆ˜: {}", few_shot_samples.len());

    // 2. Rule Engine ì‹¤í–‰
    match self.rule_engine.evaluate(&input) {
        Ok(rule_result) if rule_result.confidence >= 0.7 => {
            // Rule ì„±ê³µ, Few-shot ë¶ˆí•„ìš”
            println!("âœ… Rule Engine ì„±ê³µ (ì‹ ë¢°ë„: {:.1}%), Few-shot ìƒëµ", rule_result.confidence * 100.0);
            self.save_result(&rule_result, &input)?;
            return Ok(rule_result);
        }
        Ok(rule_result) => {
            // Rule ì €ì‹ ë¢°ë„, LLM + Few-shot ì‹¤í–‰
            println!("âš ï¸  Rule Engine ì €ì‹ ë¢°ë„ ({:.1}%), LLM + Few-shot ì‹¤í–‰", rule_result.confidence * 100.0);

            match self.llm_engine.evaluate_with_few_shot(&input, &few_shot_samples).await {
                Ok(llm_result) => {
                    let final_result = self.combine_results(rule_result, llm_result);
                    self.save_result(&final_result, &input)?;
                    Ok(final_result)
                }
                Err(_) => {
                    // LLM ì‹¤íŒ¨, Rule ê²°ê³¼ ì‚¬ìš©
                    self.save_result(&rule_result, &input)?;
                    Ok(rule_result)
                }
            }
        }
        Err(_) => {
            // Rule ì‹¤íŒ¨, LLM + Few-shotë§Œ ì‹¤í–‰
            println!("âŒ Rule Engine ì‹¤íŒ¨, LLM + Few-shotë§Œ ì‚¬ìš©");
            let llm_result = self.llm_engine.evaluate_with_few_shot(&input, &few_shot_samples).await?;
            self.save_result(&llm_result, &input)?;
            Ok(llm_result)
        }
    }
}
```

### 10.3 Learning Service í†µí•© ìƒì„¸

**Few-shot ìƒ˜í”Œ ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤**:
```rust
// learning_service.rs ë‚´ë¶€ ë™ì‘
impl LearningService {
    pub fn get_few_shot_samples(
        &self,
        workflow_id: String,
        limit: usize,
    ) -> anyhow::Result<Vec<TrainingSample>> {
        // 1. SQL ì¿¼ë¦¬ë¡œ ìœ ì‚¬ ìƒ˜í”Œ ê²€ìƒ‰
        let samples = self.db.connection.prepare(
            "SELECT * FROM training_samples
             WHERE workflow_id = ?1
               AND accuracy IS NOT NULL
               AND accuracy >= 0.8
             ORDER BY accuracy DESC, created_at DESC
             LIMIT ?2"
        )?;

        // 2. TrainingSample ëª¨ë¸ë¡œ ë³€í™˜
        // 3. ì •í™•ë„ 0.8 ì´ìƒë§Œ í•„í„°ë§
        // 4. ìµœì‹ ìˆœ ì •ë ¬ í›„ limitê°œ ë°˜í™˜

        Ok(filtered_samples)
    }
}
```

**Few-shot ìƒ˜í”Œ ë°ì´í„° êµ¬ì¡°**:
```rust
pub struct TrainingSample {
    pub id: String,
    pub workflow_id: String,
    pub input_data: String,      // JSON: {"temperature": 90, "vibration": 43}
    pub expected_result: bool,    // ì˜ˆìƒ ê²°ê³¼
    pub actual_result: Option<bool>, // ì‹¤ì œ íŒë‹¨ ê²°ê³¼
    pub accuracy: Option<f64>,    // ì •í™•ë„ (0.8-1.0)
    pub created_at: DateTime<Utc>,
}
```

### 10.4 LLM Engine Few-shot í”„ë¡¬í”„íŠ¸ ìƒì„±

**LLM Engine ë‚´ë¶€ êµ¬í˜„**:
```rust
// llm_engine.rs
impl LLMEngine {
    /// Few-shot ìƒ˜í”Œì„ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬ë°›ëŠ” ë©”ì„œë“œ (Judgment Engine í†µí•©ìš©)
    pub async fn evaluate_with_few_shot(
        &self,
        input: &JudgmentInput,
        few_shot_samples: &[crate::database::TrainingSample],
    ) -> anyhow::Result<JudgmentResult> {
        // 1. Few-shot ìƒ˜í”Œì„ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
        let mut prompt = String::from("ë‹¤ìŒì€ ê³¼ê±° íŒë‹¨ ì‚¬ë¡€ì…ë‹ˆë‹¤:\n\n");

        for (i, sample) in few_shot_samples.iter().enumerate() {
            prompt.push_str(&format!(
                "ì‚¬ë¡€ {}:\nì…ë ¥: {}\níŒë‹¨ ê²°ê³¼: {}\nì •í™•ë„: {:.1}%\n\n",
                i + 1,
                sample.input_data,
                sample.expected_result,
                sample.accuracy.unwrap_or(0.0) * 100.0
            ));
        }

        prompt.push_str(&format!(
            "ìœ„ ì‚¬ë¡€ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹¤ìŒ ì…ë ¥ì„ íŒë‹¨í•´ì£¼ì„¸ìš”:\nì…ë ¥: {}",
            serde_json::to_string_pretty(&input.input_data)?
        ));

        // 2. OpenAI API í˜¸ì¶œ
        let response = self.call_openai_api(&prompt).await?;

        // 3. ì‹ ë¢°ë„ ë³´ì • (Few-shot ìƒ˜í”Œ ê°œìˆ˜ì— ë¹„ë¡€)
        let base_confidence = response.confidence;
        let boost = (few_shot_samples.len() as f64 / 20.0) * 0.15; // ìµœëŒ€ +15%
        let adjusted_confidence = (base_confidence + boost).min(1.0);

        Ok(JudgmentResult {
            confidence: adjusted_confidence,
            ..response
        })
    }
}
```

**í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œ** (ì‹¤ì œ LLM ì…ë ¥):
```
ë‹¤ìŒì€ ê³¼ê±° íŒë‹¨ ì‚¬ë¡€ì…ë‹ˆë‹¤:

ì‚¬ë¡€ 1:
ì…ë ¥: {"temperature": 88, "vibration": 42}
íŒë‹¨ ê²°ê³¼: true
ì •í™•ë„: 95.0%

ì‚¬ë¡€ 2:
ì…ë ¥: {"temperature": 91, "vibration": 45}
íŒë‹¨ ê²°ê³¼: true
ì •í™•ë„: 92.0%

ì‚¬ë¡€ 3:
ì…ë ¥: {"temperature": 75, "vibration": 30}
íŒë‹¨ ê²°ê³¼: false
ì •í™•ë„: 88.0%

ìœ„ ì‚¬ë¡€ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹¤ìŒ ì…ë ¥ì„ íŒë‹¨í•´ì£¼ì„¸ìš”:
ì…ë ¥: {
  "temperature": 90,
  "vibration": 43
}
```

### 10.5 3-Tier íŒë‹¨ ì „ëµ ìƒì„¸

**Tier 1: Rule Engine ì„±ê³µ (ì‹ ë¢°ë„ â‰¥ 70%)**
- **ì¡°ê±´**: Rule í‘œí˜„ì‹ í‰ê°€ ì„±ê³µ AND ì‹ ë¢°ë„ 0.7 ì´ìƒ
- **ë™ì‘**: ì¦‰ì‹œ ê²°ê³¼ ë°˜í™˜, Few-shot ìƒëµ (ë¹„ìš© ì ˆê°!)
- **ì˜ˆìƒ ë¹„ìœ¨**: ì „ì²´ íŒë‹¨ì˜ 60-70%
- **ì‹¤í–‰ ì‹œê°„**: í‰ê·  5-10ms

**Tier 2: Rule ì €ì‹ ë¢°ë„ (ì‹ ë¢°ë„ < 70%)**
- **ì¡°ê±´**: Rule í‰ê°€ ì„±ê³µ BUT ì‹ ë¢°ë„ 0.7 ë¯¸ë§Œ
- **ë™ì‘**: LLM + Few-shot ì‹¤í–‰ â†’ Rule + LLM ê²°ê³¼ ë³‘í•©
- **ì˜ˆìƒ ë¹„ìœ¨**: ì „ì²´ íŒë‹¨ì˜ 20-30%
- **ì‹¤í–‰ ì‹œê°„**: í‰ê·  200-500ms (LLM API í˜¸ì¶œ)
- **ë³‘í•© ë¡œì§**:
  ```rust
  fn combine_results(&self, rule: JudgmentResult, llm: JudgmentResult) -> JudgmentResult {
      if llm.confidence > rule.confidence {
          // LLM ì‹ ë¢°ë„ ìš°ì„ 
          JudgmentResult {
              method_used: "hybrid".to_string(),
              explanation: format!(
                  "í•˜ì´ë¸Œë¦¬ë“œ íŒë‹¨ ê²°ê³¼:\n\n[Rule Engine (ì‹ ë¢°ë„: {:.1}%)]\n{}\n\n[LLM Engine (ì‹ ë¢°ë„: {:.1}%)]\n{}",
                  rule.confidence * 100.0, rule.explanation,
                  llm.confidence * 100.0, llm.explanation
              ),
              ..llm
          }
      } else {
          // Rule ì‹ ë¢°ë„ ìš°ì„ 
          rule
      }
  }
  ```

**Tier 3: Rule ì‹¤íŒ¨**
- **ì¡°ê±´**: Rule í‘œí˜„ì‹ íŒŒì‹± ì˜¤ë¥˜ ë˜ëŠ” ì‹¤í–‰ ì˜ˆì™¸
- **ë™ì‘**: LLM + Few-shotë§Œ ì‹¤í–‰ (Rule ê²°ê³¼ ë¬´ì‹œ)
- **ì˜ˆìƒ ë¹„ìœ¨**: ì „ì²´ íŒë‹¨ì˜ 5-10%
- **ì‹¤í–‰ ì‹œê°„**: í‰ê·  200-500ms

### 10.6 í•˜ìœ„ í˜¸í™˜ì„± ë³´ì¥

**ê¸°ì¡´ execute() ë©”ì„œë“œ ìë™ ìœ„ì„**:
```rust
/// ê¸°ì¡´ execute() ë©”ì„œë“œ (í•˜ìœ„ í˜¸í™˜ì„±)
pub async fn execute(&self, input: JudgmentInput) -> anyhow::Result<JudgmentResult> {
    // ê¸°ë³¸ì ìœ¼ë¡œ Few-shot í•™ìŠµ í™œì„±í™”
    self.judge_with_few_shot(input).await
}
```

**ë§ˆì´ê·¸ë ˆì´ì…˜ ì˜í–¥**:
- âœ… ê¸°ì¡´ ì½”ë“œ ìˆ˜ì • ë¶ˆí•„ìš” (ìë™ ìœ„ì„)
- âœ… Frontendì—ì„œ `execute()` í˜¸ì¶œì‹œ ìë™ìœ¼ë¡œ Few-shot í™œì„±í™”
- âœ… ì„±ëŠ¥ ì €í•˜ ì—†ìŒ (Rule ì„±ê³µì‹œ ì¦‰ì‹œ ë°˜í™˜)

### 10.7 í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€

**í…ŒìŠ¤íŠ¸ 1: Few-shot ìƒ˜í”Œ ê²€ìƒ‰ ë° íŒë‹¨ ì‹¤í–‰**
```rust
#[tokio::test]
async fn test_judge_with_few_shot_basic() {
    let engine = JudgmentEngine::new().unwrap();
    let (workflow_id, samples) = setup_test_data(); // 3ê°œ ìƒ˜í”Œ ìƒì„±

    // Workflow ë° TrainingSample DB ì €ì¥
    engine.db.save_workflow(&workflow).unwrap();
    for sample in &samples {
        engine.db.save_training_sample(sample).unwrap();
    }

    // íŒë‹¨ ì‹¤í–‰
    let input = JudgmentInput {
        workflow_id: workflow_id.clone(),
        input_data: serde_json::json!({"temperature": 90, "vibration": 43}),
    };

    let result = engine.judge_with_few_shot(input).await;

    // ê²°ê³¼ ê²€ì¦
    assert!(result.is_ok());
    let judgment = result.unwrap();
    assert_eq!(judgment.workflow_id, workflow_id);
    assert!(judgment.confidence > 0.0);
}
```

**í…ŒìŠ¤íŠ¸ 2: Rule + LLM ê²°ê³¼ ë³‘í•© ë¡œì§**
```rust
#[test]
fn test_combine_results() {
    let engine = JudgmentEngine::new().unwrap();

    let rule_result = JudgmentResult {
        confidence: 0.6, // ì €ì‹ ë¢°ë„
        method_used: "rule".to_string(),
        ..
    };

    let llm_result = JudgmentResult {
        confidence: 0.9, // ê³ ì‹ ë¢°ë„
        method_used: "llm".to_string(),
        ..
    };

    let combined = engine.combine_results(rule_result, llm_result.clone());

    // LLM ì‹ ë¢°ë„ê°€ ë” ë†’ìœ¼ë©´ LLM ê²°ê³¼ ë°˜í™˜
    assert_eq!(combined.method_used, "hybrid");
    assert_eq!(combined.result, llm_result.result);
    assert!(combined.explanation.contains("í•˜ì´ë¸Œë¦¬ë“œ íŒë‹¨ ê²°ê³¼"));
}
```

**í…ŒìŠ¤íŠ¸ 3: íŒë‹¨ íˆìŠ¤í† ë¦¬ ì €ì¥ ë° ì¡°íšŒ**
```rust
#[tokio::test]
async fn test_get_history() {
    let engine = JudgmentEngine::new().unwrap();
    let workflow_id = Uuid::new_v4().to_string();

    // íŒë‹¨ ê²°ê³¼ ì €ì¥
    let result = JudgmentResult { .. };
    engine.save_result(&result, &input).unwrap();

    // íˆìŠ¤í† ë¦¬ ì¡°íšŒ
    let history = engine.get_history(Some(workflow_id.clone()), 10).await.unwrap();

    assert!(!history.is_empty());
    assert_eq!(history[0].workflow_id, workflow_id);
}
```

**ì „ì²´ í…ŒìŠ¤íŠ¸ í˜„í™© (2025-10-30)**:
- âœ… ì´ 28ê°œ í…ŒìŠ¤íŠ¸ (100% í†µê³¼)
- âœ… Judgment Service: 3ê°œ (Few-shot í†µí•©)
- âœ… Learning Service: 25ê°œ (Rule ì €ì¥ í¬í•¨)

### 10.8 ì„±ëŠ¥ ë©”íŠ¸ë¦­

**ì˜ˆìƒ ì„±ëŠ¥ ì§€í‘œ**:
| ì‹œë‚˜ë¦¬ì˜¤ | Rule ì„±ê³µë¥  | í‰ê·  ì‘ë‹µ ì‹œê°„ | LLM í˜¸ì¶œ ë¹„ìœ¨ | ë¹„ìš© ì ˆê° |
|---------|------------|--------------|--------------|----------|
| **Tier 1 (Rule ì„±ê³µ)** | 60-70% | 5-10ms | 0% | 100% ì ˆê° |
| **Tier 2 (ì €ì‹ ë¢°ë„)** | 20-30% | 200-500ms | 30% | 70% ì ˆê° |
| **Tier 3 (Rule ì‹¤íŒ¨)** | 5-10% | 200-500ms | 10% | 0% ì ˆê° |
| **ì „ì²´ í‰ê· ** | - | **50-100ms** | **40%** | **76% ì ˆê°** |

**Few-shot í•™ìŠµ íš¨ê³¼**:
- LLM ì‹ ë¢°ë„: 0.5-0.6 â†’ **0.7-0.8** (+20-30% í–¥ìƒ)
- ì •í™•ë„: 85% â†’ **95%** (+10%p)
- í† í° ì‚¬ìš©ëŸ‰: ìƒ˜í”Œë‹¹ +50 tokens (ì „ì²´ ë¹„ìš© ëŒ€ë¹„ ë¯¸ë¯¸)

## ğŸŒ 11. Frontend API ë ˆí¼ëŸ°ìŠ¤ (Tauri Commands)

### 11.1 ê°œìš”

Judgment ServiceëŠ” **Tauri Commands**ë¥¼ í†µí•´ Frontend (TypeScript/React)ì™€ í†µì‹ í•©ë‹ˆë‹¤. ëª¨ë“  ëª…ë ¹ì–´ëŠ” ë¹„ë™ê¸°(async)ë¡œ ë™ì‘í•˜ë©°, ì—ëŸ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ Result íƒ€ì…ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

**ê¸°ìˆ  ìŠ¤íƒ**:
- **Backend**: Rust + Tauri Framework
- **Frontend**: TypeScript + React + Tauri API
- **í†µì‹ **: IPC (Inter-Process Communication)

### 11.2 Tauri Command 1: execute_judgment

**ëª©ì **: ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤í–‰í•˜ì—¬ íŒë‹¨ ê²°ê³¼ë¥¼ ì–»ìŠµë‹ˆë‹¤. (Few-shot í•™ìŠµ ìë™ ì ìš©)

**Backend êµ¬í˜„** (src-tauri/src/commands/judgment.rs:10-22):
```rust
#[tauri::command]
pub async fn execute_judgment(
    request: ExecuteJudgmentRequest,
) -> Result<JudgmentResult, String> {
    let engine = JudgmentEngine::new().map_err(|e| e.to_string())?;

    let input = JudgmentInput {
        workflow_id: request.workflow_id,
        input_data: request.input_data,
    };

    // execute()ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ judge_with_few_shot()ì„ í˜¸ì¶œ (ì„¹ì…˜ 10.6 ì°¸ì¡°)
    engine.execute(input).await.map_err(|e| e.to_string())
}
```

**Request íƒ€ì…**:
```rust
pub struct ExecuteJudgmentRequest {
    pub workflow_id: String,
    pub input_data: serde_json::Value, // JSON ê°ì²´
}
```

**Response íƒ€ì…**:
```rust
pub struct JudgmentResult {
    pub id: String,              // íŒë‹¨ ê²°ê³¼ UUID
    pub workflow_id: String,     // ì›Œí¬í”Œë¡œìš° ID
    pub result: bool,            // íŒë‹¨ ê²°ê³¼ (true/false)
    pub confidence: f64,         // ì‹ ë¢°ë„ (0.0-1.0)
    pub method_used: String,     // "rule" | "llm" | "hybrid"
    pub explanation: String,     // íŒë‹¨ ê·¼ê±° ì„¤ëª…
}
```

**Frontend ì‚¬ìš© ì˜ˆì‹œ** (TypeScript):
```typescript
// src/services/judgmentService.ts
import { invoke } from '@tauri-apps/api/tauri';

export interface ExecuteJudgmentRequest {
  workflow_id: string;
  input_data: Record<string, any>;
}

export interface JudgmentResult {
  id: string;
  workflow_id: string;
  result: boolean;
  confidence: number;
  method_used: 'rule' | 'llm' | 'hybrid';
  explanation: string;
}

export async function executeJudgment(
  workflowId: string,
  inputData: Record<string, any>
): Promise<JudgmentResult> {
  try {
    const result = await invoke<JudgmentResult>('execute_judgment', {
      request: {
        workflow_id: workflowId,
        input_data: inputData,
      },
    });

    console.log('ğŸ“Š íŒë‹¨ ê²°ê³¼:', {
      result: result.result,
      confidence: `${(result.confidence * 100).toFixed(1)}%`,
      method: result.method_used,
    });

    return result;
  } catch (error) {
    console.error('âŒ íŒë‹¨ ì‹¤í–‰ ì‹¤íŒ¨:', error);
    throw new Error(`íŒë‹¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: ${error}`);
  }
}
```

**React ì»´í¬ë„ŒíŠ¸ ì˜ˆì‹œ**:
```typescript
// src/components/JudgmentPanel.tsx
import React, { useState } from 'react';
import { executeJudgment, JudgmentResult } from '../services/judgmentService';

export const JudgmentPanel: React.FC = () => {
  const [workflowId, setWorkflowId] = useState('');
  const [temperature, setTemperature] = useState(85);
  const [vibration, setVibration] = useState(40);
  const [result, setResult] = useState<JudgmentResult | null>(null);
  const [loading, setLoading] = useState(false);

  const handleExecute = async () => {
    setLoading(true);
    try {
      const judgmentResult = await executeJudgment(workflowId, {
        temperature,
        vibration,
      });
      setResult(judgmentResult);
    } catch (error) {
      alert(`íŒë‹¨ ì‹¤í–‰ ì‹¤íŒ¨: ${error}`);
    } finally {
      setLoading(false);
    }
  };

  return (
    <div className="judgment-panel">
      <h2>í•˜ì´ë¸Œë¦¬ë“œ íŒë‹¨ ì‹¤í–‰</h2>

      <input
        type="text"
        placeholder="Workflow ID"
        value={workflowId}
        onChange={(e) => setWorkflowId(e.target.value)}
      />

      <input
        type="number"
        placeholder="Temperature"
        value={temperature}
        onChange={(e) => setTemperature(Number(e.target.value))}
      />

      <input
        type="number"
        placeholder="Vibration"
        value={vibration}
        onChange={(e) => setVibration(Number(e.target.value))}
      />

      <button onClick={handleExecute} disabled={loading || !workflowId}>
        {loading ? 'íŒë‹¨ ì¤‘...' : 'íŒë‹¨ ì‹¤í–‰'}
      </button>

      {result && (
        <div className="result-card">
          <h3>íŒë‹¨ ê²°ê³¼: {result.result ? 'âœ… ì–‘í˜¸' : 'âš ï¸ ê²½ê³ '}</h3>
          <p>ì‹ ë¢°ë„: {(result.confidence * 100).toFixed(1)}%</p>
          <p>íŒë‹¨ ë°©ì‹: {result.method_used}</p>
          <details>
            <summary>ìƒì„¸ ì„¤ëª…</summary>
            <pre>{result.explanation}</pre>
          </details>
        </div>
      )}
    </div>
  );
};
```

### 11.3 Tauri Command 2: get_judgment_history

**ëª©ì **: ê³¼ê±° íŒë‹¨ ê²°ê³¼ ì´ë ¥ì„ ì¡°íšŒí•©ë‹ˆë‹¤.

**Backend êµ¬í˜„** (src-tauri/src/commands/judgment.rs:24-33):
```rust
#[tauri::command]
pub async fn get_judgment_history(
    workflow_id: Option<String>,
    limit: Option<u32>,
) -> Result<Vec<JudgmentResult>, String> {
    let engine = JudgmentEngine::new().map_err(|e| e.to_string())?;
    engine.get_history(workflow_id, limit.unwrap_or(50))
        .await
        .map_err(|e| e.to_string())
}
```

**Parameters**:
- `workflow_id` (optional): íŠ¹ì • ì›Œí¬í”Œë¡œìš°ì˜ ì´ë ¥ë§Œ ì¡°íšŒ (nullì´ë©´ ì „ì²´ ì¡°íšŒ)
- `limit` (optional): ì¡°íšŒ ê°œìˆ˜ ì œí•œ (ê¸°ë³¸ê°’: 50)

**Response**: `Vec<JudgmentResult>` (ë°°ì—´)

**Frontend ì‚¬ìš© ì˜ˆì‹œ** (TypeScript):
```typescript
// src/services/judgmentService.ts
export async function getJudgmentHistory(
  workflowId?: string,
  limit?: number
): Promise<JudgmentResult[]> {
  try {
    const history = await invoke<JudgmentResult[]>('get_judgment_history', {
      workflow_id: workflowId || null,
      limit: limit || 50,
    });

    console.log(`ğŸ“œ ì¡°íšŒëœ íŒë‹¨ ì´ë ¥: ${history.length}ê°œ`);
    return history;
  } catch (error) {
    console.error('âŒ ì´ë ¥ ì¡°íšŒ ì‹¤íŒ¨:', error);
    throw new Error(`ì´ë ¥ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: ${error}`);
  }
}
```

**React ì»´í¬ë„ŒíŠ¸ ì˜ˆì‹œ**:
```typescript
// src/components/JudgmentHistoryTable.tsx
import React, { useEffect, useState } from 'react';
import { getJudgmentHistory, JudgmentResult } from '../services/judgmentService';

export const JudgmentHistoryTable: React.FC<{ workflowId?: string }> = ({ workflowId }) => {
  const [history, setHistory] = useState<JudgmentResult[]>([]);
  const [loading, setLoading] = useState(true);

  useEffect(() => {
    const fetchHistory = async () => {
      setLoading(true);
      try {
        const data = await getJudgmentHistory(workflowId, 50);
        setHistory(data);
      } catch (error) {
        console.error('ì´ë ¥ ë¡œë“œ ì‹¤íŒ¨:', error);
      } finally {
        setLoading(false);
      }
    };

    fetchHistory();
  }, [workflowId]);

  if (loading) return <div>ë¡œë”© ì¤‘...</div>;

  return (
    <table className="history-table">
      <thead>
        <tr>
          <th>ID</th>
          <th>Workflow</th>
          <th>ê²°ê³¼</th>
          <th>ì‹ ë¢°ë„</th>
          <th>íŒë‹¨ ë°©ì‹</th>
          <th>ì„¤ëª…</th>
        </tr>
      </thead>
      <tbody>
        {history.map((item) => (
          <tr key={item.id}>
            <td>{item.id.substring(0, 8)}...</td>
            <td>{item.workflow_id}</td>
            <td>{item.result ? 'âœ…' : 'âš ï¸'}</td>
            <td>{(item.confidence * 100).toFixed(1)}%</td>
            <td>
              <span className={`badge badge-${item.method_used}`}>
                {item.method_used}
              </span>
            </td>
            <td>
              <details>
                <summary>ë³´ê¸°</summary>
                <pre>{item.explanation}</pre>
              </details>
            </td>
          </tr>
        ))}
      </tbody>
    </table>
  );
};
```

### 11.4 ì—ëŸ¬ ì²˜ë¦¬ ê°€ì´ë“œ

**Backend ì—ëŸ¬ íƒ€ì…** (Rust):
```rust
pub enum JudgmentError {
    DatabaseError(String),       // SQLite ì—°ê²° ì˜¤ë¥˜
    WorkflowNotFound(String),    // Workflow ID ì—†ìŒ
    InvalidInput(String),        // ì…ë ¥ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨
    RuleEngineError(String),     // Rule íŒŒì‹±/ì‹¤í–‰ ì˜¤ë¥˜
    LLMEngineError(String),      // OpenAI API í˜¸ì¶œ ì˜¤ë¥˜
    InternalError(String),       // ê¸°íƒ€ ë‚´ë¶€ ì˜¤ë¥˜
}
```

**Frontend ì—ëŸ¬ ì²˜ë¦¬ íŒ¨í„´**:
```typescript
// src/utils/errorHandler.ts
export function handleJudgmentError(error: unknown): string {
  const errorStr = String(error);

  if (errorStr.includes('Workflow not found')) {
    return 'ì›Œí¬í”Œë¡œìš°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. IDë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.';
  }

  if (errorStr.includes('Database')) {
    return 'ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.';
  }

  if (errorStr.includes('Rule')) {
    return 'Rule ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê·œì¹™ì„ í™•ì¸í•´ì£¼ì„¸ìš”.';
  }

  if (errorStr.includes('LLM') || errorStr.includes('OpenAI')) {
    return 'AI íŒë‹¨ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.';
  }

  return `ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: ${errorStr}`;
}

// ì‚¬ìš© ì˜ˆì‹œ
try {
  const result = await executeJudgment(workflowId, inputData);
} catch (error) {
  const userMessage = handleJudgmentError(error);
  alert(userMessage);
}
```

### 11.5 ì„±ëŠ¥ ìµœì í™” íŒ

**1. ìºì‹± ì „ëµ**:
```typescript
// src/utils/judgmentCache.ts
const cache = new Map<string, { result: JudgmentResult; timestamp: number }>();
const CACHE_TTL = 5 * 60 * 1000; // 5ë¶„

export async function executeJudgmentWithCache(
  workflowId: string,
  inputData: Record<string, any>
): Promise<JudgmentResult> {
  const cacheKey = `${workflowId}-${JSON.stringify(inputData)}`;
  const cached = cache.get(cacheKey);

  if (cached && Date.now() - cached.timestamp < CACHE_TTL) {
    console.log('ğŸ’¾ ìºì‹œëœ ê²°ê³¼ ì‚¬ìš©');
    return cached.result;
  }

  const result = await executeJudgment(workflowId, inputData);
  cache.set(cacheKey, { result, timestamp: Date.now() });
  return result;
}
```

**2. ë³‘ë ¬ ì²˜ë¦¬**:
```typescript
// ì—¬ëŸ¬ ì›Œí¬í”Œë¡œìš° ë™ì‹œ ì‹¤í–‰
const workflows = ['workflow-1', 'workflow-2', 'workflow-3'];
const inputData = { temperature: 90, vibration: 43 };

const results = await Promise.all(
  workflows.map(id => executeJudgment(id, inputData))
);

console.log('ë³‘ë ¬ íŒë‹¨ ì™„ë£Œ:', results);
```

**3. ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ (WebSocket ëŒ€ì•ˆ)**:
```typescript
// Polling ë°©ì‹ìœ¼ë¡œ ì‹¤ì‹œê°„ ì´ë ¥ ì—…ë°ì´íŠ¸
export function useRealtimeHistory(workflowId: string, intervalMs = 5000) {
  const [history, setHistory] = useState<JudgmentResult[]>([]);

  useEffect(() => {
    const fetchHistory = () => getJudgmentHistory(workflowId, 10);

    fetchHistory().then(setHistory);
    const interval = setInterval(() => {
      fetchHistory().then(setHistory);
    }, intervalMs);

    return () => clearInterval(interval);
  }, [workflowId, intervalMs]);

  return history;
}

// ì‚¬ìš© ì˜ˆì‹œ
const RealtimePanel = ({ workflowId }) => {
  const history = useRealtimeHistory(workflowId, 3000); // 3ì´ˆë§ˆë‹¤ ê°±ì‹ 
  return <JudgmentHistoryTable history={history} />;
};
```

### 11.6 íƒ€ì… ì •ì˜ (TypeScript)

**ì™„ì „í•œ íƒ€ì… ì •ì˜ íŒŒì¼** (src/types/judgment.d.ts):
```typescript
declare module '@tauri-apps/api/tauri' {
  function invoke<T>(cmd: string, args?: Record<string, any>): Promise<T>;
}

export type JudgmentMethod = 'rule' | 'llm' | 'hybrid';

export interface JudgmentInput {
  workflow_id: string;
  input_data: Record<string, any>;
}

export interface JudgmentResult {
  id: string;
  workflow_id: string;
  result: boolean;
  confidence: number;
  method_used: JudgmentMethod;
  explanation: string;
}

export interface ExecuteJudgmentRequest {
  workflow_id: string;
  input_data: Record<string, any>;
}

export interface GetHistoryRequest {
  workflow_id?: string;
  limit?: number;
}

// Tauri Commands
export function executeJudgment(
  workflowId: string,
  inputData: Record<string, any>
): Promise<JudgmentResult>;

export function getJudgmentHistory(
  workflowId?: string,
  limit?: number
): Promise<JudgmentResult[]>;
```

## ğŸš€ 12. ì‹¤ì „ ì‚¬ìš© ê°€ì´ë“œ

### 12.1 End-to-End ì‹œë‚˜ë¦¬ì˜¤

#### ì‹œë‚˜ë¦¬ì˜¤ 1: ê¸°ê³„ ìƒíƒœ ëª¨ë‹ˆí„°ë§ ë° íŒë‹¨

**ìš”êµ¬ì‚¬í•­**: ê³µì¥ ê¸°ê³„ì˜ ì˜¨ë„ì™€ ì§„ë™ ë°ì´í„°ë¥¼ ì‹¤ì‹œê°„ ìˆ˜ì§‘í•˜ì—¬ ì´ìƒ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ê³ , ë¬¸ì œ ë°œìƒì‹œ ì•Œë¦¼ì„ ë³´ëƒ…ë‹ˆë‹¤.

**êµ¬í˜„ íë¦„**:

**Step 1: Workflow ìƒì„±** (DBì— ì €ì¥)
```sql
INSERT INTO workflows (id, name, definition, rule_expression, version, is_active)
VALUES (
  'machine-monitor-001',
  'ê¸°ê³„ ìƒíƒœ ëª¨ë‹ˆí„°ë§',
  '{"type": "condition", "conditions": []}',
  'temperature > 85 && vibration > 40',
  1,
  true
);
```

**Step 2: Frontendì—ì„œ ì‹¤ì‹œê°„ íŒë‹¨ ì‹¤í–‰**
```typescript
// src/pages/MachineMonitor.tsx
import React, { useState, useEffect } from 'react';
import { executeJudgment } from '../services/judgmentService';

export const MachineMonitor: React.FC = () => {
  const [machineData, setMachineData] = useState({
    temperature: 0,
    vibration: 0,
  });
  const [alertStatus, setAlertStatus] = useState<'safe' | 'warning' | 'critical'>('safe');

  // 1ì´ˆë§ˆë‹¤ ì„¼ì„œ ë°ì´í„° ìˆ˜ì§‘ (ì‹¤ì œë¡œëŠ” WebSocket ë˜ëŠ” API)
  useEffect(() => {
    const interval = setInterval(async () => {
      // ì„¼ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ì˜ˆì‹œ: ëœë¤ ê°’)
      const newData = {
        temperature: Math.floor(Math.random() * 100),
        vibration: Math.floor(Math.random() * 60),
      };
      setMachineData(newData);

      // íŒë‹¨ ì‹¤í–‰
      try {
        const result = await executeJudgment('machine-monitor-001', newData);

        if (result.result) {
          // ì´ìƒ ê°ì§€
          setAlertStatus(result.confidence > 0.8 ? 'critical' : 'warning');
          console.warn('âš ï¸  ê¸°ê³„ ì´ìƒ ê°ì§€!', result);

          // ì•Œë¦¼ ì „ì†¡ (ì˜ˆ: Slack, Email)
          // await sendAlert(result);
        } else {
          setAlertStatus('safe');
        }
      } catch (error) {
        console.error('íŒë‹¨ ì‹¤í–‰ ì˜¤ë¥˜:', error);
      }
    }, 1000);

    return () => clearInterval(interval);
  }, []);

  return (
    <div className={`monitor-panel alert-${alertStatus}`}>
      <h2>ê¸°ê³„ ìƒíƒœ ëª¨ë‹ˆí„°ë§</h2>
      <div className="sensor-data">
        <div>ì˜¨ë„: {machineData.temperature}Â°C</div>
        <div>ì§„ë™: {machineData.vibration} Hz</div>
      </div>
      <div className={`status-indicator status-${alertStatus}`}>
        {alertStatus === 'safe' && 'âœ… ì •ìƒ'}
        {alertStatus === 'warning' && 'âš ï¸ ì£¼ì˜'}
        {alertStatus === 'critical' && 'ğŸš¨ ìœ„í—˜'}
      </div>
    </div>
  );
};
```

**ì˜ˆìƒ ê²°ê³¼**:
- **Rule ì„±ê³µ (60-70%)**: ì˜¨ë„ 90Â°C, ì§„ë™ 45Hz â†’ Rule Engineìœ¼ë¡œ ì¦‰ì‹œ íŒë‹¨ (5-10ms)
- **Rule ì €ì‹ ë¢°ë„ (20-30%)**: ì• ë§¤í•œ ì¼€ì´ìŠ¤ â†’ LLM + Few-shotìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ (200-500ms)
- **Rule ì‹¤íŒ¨ (5-10%)**: ë³µì¡í•œ íŒ¨í„´ â†’ LLM íŒë‹¨ (200-500ms)

---

#### ì‹œë‚˜ë¦¬ì˜¤ 2: í’ˆì§ˆ ê²€ì‚¬ ìë™í™” (Few-shot í•™ìŠµ í™œìš©)

**ìš”êµ¬ì‚¬í•­**: ì œí’ˆ ì‚¬ì§„ì„ ì—…ë¡œë“œí•˜ë©´ AIê°€ ë¶ˆëŸ‰ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ê³ , ì‚¬ìš©ì í”¼ë“œë°±ì„ í†µí•´ ìë™ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.

**êµ¬í˜„ íë¦„**:

**Step 1: Workflow ìƒì„± (LLM ì „ìš©)**
```sql
INSERT INTO workflows (id, name, definition, version, is_active)
VALUES (
  'quality-inspection-001',
  'ì œí’ˆ í’ˆì§ˆ ê²€ì‚¬',
  '{"type": "llm", "criteria": "ì œí’ˆ ì‚¬ì§„ì„ ë¶„ì„í•˜ì—¬ ë¶ˆëŸ‰ ì—¬ë¶€ íŒë‹¨"}',
  1,
  true
);
```

**Step 2: Frontendì—ì„œ ì´ë¯¸ì§€ ì—…ë¡œë“œ ë° íŒë‹¨**
```typescript
// src/components/QualityInspection.tsx
import React, { useState } from 'react';
import { executeJudgment } from '../services/judgmentService';
import { collectFeedback } from '../services/learningService';

export const QualityInspection: React.FC = () => {
  const [image, setImage] = useState<File | null>(null);
  const [result, setResult] = useState<JudgmentResult | null>(null);
  const [feedback, setFeedback] = useState<'like' | 'dislike' | null>(null);

  const handleInspect = async () => {
    if (!image) return;

    // 1. ì´ë¯¸ì§€ë¥¼ base64ë¡œ ë³€í™˜
    const base64Image = await convertToBase64(image);

    // 2. íŒë‹¨ ì‹¤í–‰ (Few-shot ìë™ ì ìš©)
    const inspectionResult = await executeJudgment('quality-inspection-001', {
      image: base64Image,
      timestamp: Date.now(),
    });

    setResult(inspectionResult);
  };

  const handleFeedback = async (isLike: boolean) => {
    if (!result) return;

    // 3. ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘ â†’ Learning Serviceì— ì €ì¥
    await collectFeedback({
      judgment_id: result.id,
      feedback_type: 'like_dislike',
      value: isLike ? 1 : 0,
    });

    setFeedback(isLike ? 'like' : 'dislike');

    // 4. í”¼ë“œë°±ì´ ê¸ì •(ğŸ‘)ì´ë©´ ìë™ìœ¼ë¡œ Few-shot ìƒ˜í”Œë¡œ ì¶”ê°€
    if (isLike) {
      console.log('âœ… Few-shot ìƒ˜í”Œ ìë™ ì¶”ê°€ ì™„ë£Œ!');
    }
  };

  return (
    <div className="quality-inspection">
      <h2>ì œí’ˆ í’ˆì§ˆ ê²€ì‚¬</h2>

      <input
        type="file"
        accept="image/*"
        onChange={(e) => setImage(e.target.files?.[0] || null)}
      />

      <button onClick={handleInspect} disabled={!image}>
        ê²€ì‚¬ ì‹¤í–‰
      </button>

      {result && (
        <div className="result-panel">
          <h3>{result.result ? 'âš ï¸ ë¶ˆëŸ‰' : 'âœ… ì–‘í˜¸'}</h3>
          <p>ì‹ ë¢°ë„: {(result.confidence * 100).toFixed(1)}%</p>
          <p>Few-shot ìƒ˜í”Œ {result.method_used === 'hybrid' ? 'í™œìš©' : 'ë¯¸í™œìš©'}</p>

          <div className="feedback-buttons">
            <button onClick={() => handleFeedback(true)}>ğŸ‘ ì •í™•í•¨</button>
            <button onClick={() => handleFeedback(false)}>ğŸ‘ ë¶€ì •í™•í•¨</button>
          </div>

          {feedback && (
            <p className="feedback-message">
              í”¼ë“œë°± ê°ì‚¬í•©ë‹ˆë‹¤! {feedback === 'like' && 'ì´ ìƒ˜í”Œì€ Few-shot í•™ìŠµì— ì¶”ê°€ë©ë‹ˆë‹¤.'}
            </p>
          )}
        </div>
      )}
    </div>
  );
};
```

**í•™ìŠµ íš¨ê³¼ ì¶”ì´**:
| ì‹œì  | Few-shot ìƒ˜í”Œ ìˆ˜ | LLM ì •í™•ë„ | í‰ê·  ì‹ ë¢°ë„ |
|------|-----------------|-----------|------------|
| **1ì¼ì°¨** | 0ê°œ | 75% | 0.60 |
| **7ì¼ì°¨** | 50ê°œ | 85% | 0.72 |
| **30ì¼ì°¨** | 200ê°œ | **95%** | **0.85** |

---

#### ì‹œë‚˜ë¦¬ì˜¤ 3: ë³µí•© íŒë‹¨ (Rule + LLM ë³‘í•©)

**ìš”êµ¬ì‚¬í•­**: ì°½ê³  ì¬ê³ ë¥¼ í™•ì¸í•˜ì—¬ ì£¼ë¬¸ ì—¬ë¶€ë¥¼ íŒë‹¨í•©ë‹ˆë‹¤. Ruleë¡œ ê¸°ë³¸ ì¡°ê±´ì„ ì²´í¬í•˜ê³ , LLMìœ¼ë¡œ ë³µì¡í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.

**Step 1: Workflow ìƒì„± (í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ)**
```sql
INSERT INTO workflows (id, name, definition, rule_expression, version, is_active)
VALUES (
  'inventory-order-001',
  'ì¬ê³  ì£¼ë¬¸ íŒë‹¨',
  '{"type": "hybrid", "strategy": "consensus"}',
  'stock_level < reorder_point && demand_forecast > 100',
  1,
  true
);
```

**Step 2: Frontend ì‹¤í–‰**
```typescript
const result = await executeJudgment('inventory-order-001', {
  stock_level: 50,
  reorder_point: 100,
  demand_forecast: 150,
  supplier_lead_time: 7,
  seasonal_factor: 1.2,
});

console.log('ì¬ê³  ì£¼ë¬¸ íŒë‹¨ ê²°ê³¼:', {
  shouldOrder: result.result,
  confidence: `${(result.confidence * 100).toFixed(1)}%`,
  method: result.method_used,
  explanation: result.explanation,
});
```

**ì˜ˆìƒ ê²°ê³¼** (í•˜ì´ë¸Œë¦¬ë“œ íŒë‹¨):
```
shouldOrder: true
confidence: 92.5%
method: "hybrid"
explanation:
  "í•˜ì´ë¸Œë¦¬ë“œ íŒë‹¨ ê²°ê³¼:

  [Rule Engine (ì‹ ë¢°ë„: 65.0%)]
  ì¬ê³  ìˆ˜ì¤€(50) < ì¬ì£¼ë¬¸ì (100) && ìˆ˜ìš” ì˜ˆì¸¡(150) > 100
  â†’ ì£¼ë¬¸ í•„ìš” (ê¸°ë³¸ ì¡°ê±´ ì¶©ì¡±)

  [LLM Engine (ì‹ ë¢°ë„: 92.5%)]
  ë¹„ì¦ˆë‹ˆìŠ¤ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„:
  - ê³µê¸‰ì ë¦¬ë“œíƒ€ì„ 7ì¼ ê³ ë ¤ì‹œ ì¬ê³  ë¶€ì¡± ìœ„í—˜ ë†’ìŒ
  - ê³„ì ˆ ìš”ì¸(1.2) ë°˜ì˜ì‹œ ìˆ˜ìš” ì¦ê°€ ì˜ˆìƒ
  - ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€(15ê°œ) ë¶„ì„ ê²°ê³¼ 93% ì£¼ë¬¸ ê²°ì •
  â†’ ì£¼ë¬¸ ê°•ë ¥ ê¶Œì¥ (ì‹ ë¢°ë„ 92.5%)"
```

### 12.2 ì„±ëŠ¥ ìµœì í™” ì „ëµ

#### ì „ëµ 1: Rule Engine ìš°ì„  í™œìš© (ë¹„ìš© ì ˆê°)

**ëª©í‘œ**: Rule ì„±ê³µë¥  70%ë¡œ LLM í˜¸ì¶œ 40%ê¹Œì§€ ê°ì†Œ

**ìµœì í™” ë°©ë²•**:
```typescript
// 1. Rule í‘œí˜„ì‹ ì •í™•ë„ í–¥ìƒ
const optimizedRule = `
  (temperature > 85 && vibration > 40) ||
  (temperature > 90 && vibration > 35) ||
  (pressure > 120)
`;

// 2. Rule ì„±ê³µë¥  ëª¨ë‹ˆí„°ë§
const metrics = await getJudgmentHistory('workflow-id', 100);
const ruleSuccessRate = metrics.filter(m => m.method_used === 'rule').length / metrics.length;

console.log(`Rule ì„±ê³µë¥ : ${(ruleSuccessRate * 100).toFixed(1)}%`);

// 3. ì„±ê³µë¥  70% ë¯¸ë§Œì´ë©´ Rule ê°œì„  í•„ìš”
if (ruleSuccessRate < 0.7) {
  console.warn('âš ï¸  Rule í‘œí˜„ì‹ ìµœì í™” í•„ìš”!');
  // Learning Serviceì˜ extract_rules() í˜¸ì¶œí•˜ì—¬ ìë™ Rule ì¶”ì¶œ
}
```

**ì˜ˆìƒ íš¨ê³¼**:
- ë¹„ìš©: $100/ì›” â†’ **$40/ì›”** (60% ì ˆê°)
- í‰ê·  ì‘ë‹µ ì‹œê°„: 250ms â†’ **50ms** (80% ê°œì„ )

---

#### ì „ëµ 2: Few-shot ìƒ˜í”Œ í’ˆì§ˆ ê´€ë¦¬

**ëª©í‘œ**: ì •í™•ë„ 0.8 ì´ìƒ ìƒ˜í”Œë§Œ ìœ ì§€í•˜ì—¬ LLM ì„±ëŠ¥ ê·¹ëŒ€í™”

**ìµœì í™” ì½”ë“œ**:
```rust
// learning_service.rs (Backend)
pub fn cleanup_low_quality_samples(&self) -> anyhow::Result<u32> {
    let deleted = self.db.connection.execute(
        "DELETE FROM training_samples
         WHERE accuracy IS NOT NULL AND accuracy < 0.8",
        [],
    )?;

    println!("ğŸ—‘ï¸  ì €í’ˆì§ˆ ìƒ˜í”Œ {}ê°œ ì‚­ì œ ì™„ë£Œ", deleted);
    Ok(deleted)
}
```

```typescript
// Frontend (ì£¼ê¸°ì  ì‹¤í–‰)
useEffect(() => {
  const cleanup = async () => {
    const deleted = await invoke('cleanup_low_quality_samples');
    console.log(`ì €í’ˆì§ˆ ìƒ˜í”Œ ${deleted}ê°œ ì‚­ì œ`);
  };

  // ë§¤ì¼ ìì • ì‹¤í–‰
  const interval = setInterval(cleanup, 24 * 60 * 60 * 1000);
  return () => clearInterval(interval);
}, []);
```

**ì˜ˆìƒ íš¨ê³¼**:
- LLM ì •í™•ë„: 85% â†’ **95%** (+10%p)
- Few-shot ìƒ˜í”Œ ê°œìˆ˜: 500ê°œ â†’ **300ê°œ** (40% ê°ì†Œ, í’ˆì§ˆ í–¥ìƒ)

---

#### ì „ëµ 3: ìºì‹± ë ˆì´ì–´ ì¶”ê°€

**ëª©í‘œ**: ë™ì¼ ì…ë ¥ íŒë‹¨ì‹œ ìºì‹œ í™œìš©ìœ¼ë¡œ ì‘ë‹µ ì‹œê°„ ë‹¨ì¶•

**êµ¬í˜„**:
```typescript
// src/services/judgmentCache.ts
import { LRUCache } from 'lru-cache';

const cache = new LRUCache<string, JudgmentResult>({
  max: 500, // ìµœëŒ€ 500ê°œ ìºì‹œ
  ttl: 5 * 60 * 1000, // 5ë¶„ TTL
  updateAgeOnGet: true,
});

export async function executeJudgmentWithCache(
  workflowId: string,
  inputData: Record<string, any>
): Promise<JudgmentResult> {
  const cacheKey = `${workflowId}:${JSON.stringify(inputData)}`;

  // 1. ìºì‹œ í™•ì¸
  const cached = cache.get(cacheKey);
  if (cached) {
    console.log('ğŸ’¾ ìºì‹œ ì ì¤‘! ì‘ë‹µ ì‹œê°„: <1ms');
    return cached;
  }

  // 2. ì‹¤ì œ íŒë‹¨ ì‹¤í–‰
  const result = await executeJudgment(workflowId, inputData);

  // 3. ìºì‹œ ì €ì¥ (ì„±ê³µí•œ ê²°ê³¼ë§Œ)
  if (result.confidence > 0.7) {
    cache.set(cacheKey, result);
  }

  return result;
}
```

**ì˜ˆìƒ íš¨ê³¼**:
- ìºì‹œ ì ì¤‘ë¥ : 30-40%
- ìºì‹œ ì ì¤‘ì‹œ ì‘ë‹µ ì‹œê°„: 200ms â†’ **<1ms** (99.5% ê°œì„ !)

### 12.3 íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ

#### ë¬¸ì œ 1: LLM í˜¸ì¶œ ì‹¤íŒ¨ (OpenAI API ì˜¤ë¥˜)

**ì¦ìƒ**:
```
Error: LLM judgment failed: Error code 401 - Invalid API key
```

**í•´ê²° ë°©ë²•**:
```bash
# 1. í™˜ê²½ ë³€ìˆ˜ í™•ì¸
echo $OPENAI_API_KEY

# 2. .env íŒŒì¼ ìˆ˜ì •
OPENAI_API_KEY=sk-your-api-key-here

# 3. Tauri ì•± ì¬ì‹œì‘
npm run tauri dev
```

**ì˜ˆë°©ì±…**:
- API í‚¤ ë§Œë£Œì¼ ì„¤ì •: 3ê°œì›”ë§ˆë‹¤ ê°±ì‹ 
- Fallback API í‚¤ ì¤€ë¹„ (ë¹„ìƒìš©)

---

#### ë¬¸ì œ 2: Few-shot ìƒ˜í”Œ ê²€ìƒ‰ ì‹¤íŒ¨ (DB ì˜¤ë¥˜)

**ì¦ìƒ**:
```
Error: Database locked
```

**í•´ê²° ë°©ë²•**:
```typescript
// 1. SQLite WAL ëª¨ë“œ í™œì„±í™” (ë™ì‹œì„± í–¥ìƒ)
// src-tauri/src/database/sqlite.rs
connection.execute("PRAGMA journal_mode=WAL;", [])?;

// 2. ì—°ê²° í’€ í¬ê¸° ì¦ê°€
let pool = Pool::new(10); // ê¸°ë³¸ 1 â†’ 10
```

**ì˜ˆë°©ì±…**:
- ì¥ê¸° íŠ¸ëœì­ì…˜ ê¸ˆì§€ (3ì´ˆ ì´ë‚´ ì™„ë£Œ)
- ì½ê¸° ì „ìš© ì‘ì—…ì€ ë³„ë„ ì—°ê²° ì‚¬ìš©

---

#### ë¬¸ì œ 3: Rule Engine íŒŒì‹± ì˜¤ë¥˜

**ì¦ìƒ**:
```
Error: Rule expression invalid: Syntax error at line 1
```

**í•´ê²° ë°©ë²•**:
```typescript
// 1. Rule í‘œí˜„ì‹ ê²€ì¦ ë„êµ¬ ì‚¬ìš©
function validateRuleExpression(expr: string): { valid: boolean; error?: string } {
  try {
    // Backend í˜¸ì¶œí•˜ì—¬ AST íŒŒì‹± í…ŒìŠ¤íŠ¸
    await invoke('validate_rule', { expression: expr });
    return { valid: true };
  } catch (error) {
    return { valid: false, error: String(error) };
  }
}

// 2. ì‚¬ìš©ì ì…ë ¥ ì „ ê²€ì¦
const validation = await validateRuleExpression('temperature > 85 &&');
if (!validation.valid) {
  alert(`Rule ì˜¤ë¥˜: ${validation.error}`);
}
```

**ì˜ˆë°©ì±…**:
- Rule ì‘ì„±ì‹œ ì‹¤ì‹œê°„ ê²€ì¦ UI ì œê³µ
- ìì£¼ ì‚¬ìš©í•˜ëŠ” íŒ¨í„´ í…œí”Œë¦¿í™”

### 12.4 ëª¨ë‹ˆí„°ë§ ë° ë©”íŠ¸ë¦­

**í•µì‹¬ ë©”íŠ¸ë¦­ ì¶”ì **:
```typescript
// src/services/metrics.ts
interface JudgmentMetrics {
  totalJudgments: number;
  ruleSuccessRate: number;    // Rule Engine ì„±ê³µë¥ 
  llmSuccessRate: number;      // LLM Engine ì„±ê³µë¥ 
  avgConfidence: number;       // í‰ê·  ì‹ ë¢°ë„
  avgResponseTime: number;     // í‰ê·  ì‘ë‹µ ì‹œê°„ (ms)
  costPerJudgment: number;     // íŒë‹¨ë‹¹ ë¹„ìš© ($)
}

export async function getMetrics(
  workflowId: string,
  days: number = 7
): Promise<JudgmentMetrics> {
  const history = await getJudgmentHistory(workflowId, days * 100);

  const ruleCount = history.filter(h => h.method_used === 'rule').length;
  const llmCount = history.filter(h => h.method_used === 'llm' || h.method_used === 'hybrid').length;

  return {
    totalJudgments: history.length,
    ruleSuccessRate: ruleCount / history.length,
    llmSuccessRate: llmCount / history.length,
    avgConfidence: history.reduce((sum, h) => sum + h.confidence, 0) / history.length,
    avgResponseTime: 50, // TODO: ì‹¤ì œ ì‘ë‹µ ì‹œê°„ ì¸¡ì • í•„ìš”
    costPerJudgment: (llmCount * 0.002) / history.length, // LLM í˜¸ì¶œë‹¹ $0.002
  };
}
```

**ëŒ€ì‹œë³´ë“œ ì˜ˆì‹œ**:
```typescript
// src/components/MetricsDashboard.tsx
export const MetricsDashboard: React.FC = () => {
  const [metrics, setMetrics] = useState<JudgmentMetrics | null>(null);

  useEffect(() => {
    getMetrics('machine-monitor-001', 7).then(setMetrics);
  }, []);

  if (!metrics) return <div>ë¡œë”© ì¤‘...</div>;

  return (
    <div className="metrics-dashboard">
      <h2>íŒë‹¨ ì„œë¹„ìŠ¤ ë©”íŠ¸ë¦­ (ìµœê·¼ 7ì¼)</h2>

      <div className="metric-card">
        <h3>ì´ íŒë‹¨ íšŸìˆ˜</h3>
        <p className="metric-value">{metrics.totalJudgments.toLocaleString()}íšŒ</p>
      </div>

      <div className="metric-card">
        <h3>Rule ì„±ê³µë¥ </h3>
        <p className="metric-value">{(metrics.ruleSuccessRate * 100).toFixed(1)}%</p>
        <p className="metric-target">ëª©í‘œ: 70% ì´ìƒ</p>
      </div>

      <div className="metric-card">
        <h3>í‰ê·  ì‹ ë¢°ë„</h3>
        <p className="metric-value">{(metrics.avgConfidence * 100).toFixed(1)}%</p>
      </div>

      <div className="metric-card">
        <h3>í‰ê·  ë¹„ìš©</h3>
        <p className="metric-value">${metrics.costPerJudgment.toFixed(4)}/íŒë‹¨</p>
      </div>
    </div>
  );
};
```

### 12.5 ë³´ì•ˆ ëª¨ë²” ì‚¬ë¡€

**1. API í‚¤ ë³´í˜¸**:
```typescript
// âŒ ì˜ëª»ëœ ë°©ë²•: Frontendì— í•˜ë“œì½”ë”©
const OPENAI_API_KEY = 'sk-...';

// âœ… ì˜¬ë°”ë¥¸ ë°©ë²•: Backend í™˜ê²½ ë³€ìˆ˜
// .env (Backend)
OPENAI_API_KEY=sk-...

// src-tauri/src/main.rs
let api_key = std::env::var("OPENAI_API_KEY").expect("API key not found");
```

**2. ì…ë ¥ ê²€ì¦**:
```rust
// src-tauri/src/commands/judgment.rs
#[tauri::command]
pub async fn execute_judgment(
    request: ExecuteJudgmentRequest,
) -> Result<JudgmentResult, String> {
    // 1. workflow_id ê²€ì¦
    if request.workflow_id.is_empty() || request.workflow_id.len() > 100 {
        return Err("Invalid workflow_id".to_string());
    }

    // 2. input_data í¬ê¸° ì œí•œ
    let input_size = serde_json::to_string(&request.input_data)
        .map_err(|e| e.to_string())?
        .len();
    if input_size > 10_000 {
        return Err("Input data too large".to_string());
    }

    // 3. ì‹¤í–‰
    let engine = JudgmentEngine::new().map_err(|e| e.to_string())?;
    // ...
}
```

**3. Rate Limiting**:
```rust
use std::time::{Duration, Instant};
use std::sync::Mutex;

lazy_static! {
    static ref LAST_CALL: Mutex<Instant> = Mutex::new(Instant::now());
}

#[tauri::command]
pub async fn execute_judgment_rate_limited(
    request: ExecuteJudgmentRequest,
) -> Result<JudgmentResult, String> {
    // ìµœì†Œ 100ms ê°„ê²© ê°•ì œ
    let mut last = LAST_CALL.lock().unwrap();
    let elapsed = last.elapsed();

    if elapsed < Duration::from_millis(100) {
        return Err("Rate limit exceeded".to_string());
    }

    *last = Instant::now();
    drop(last);

    execute_judgment(request).await
}
```

---

## 10.9 êµ¬í˜„ ì™„ë£Œ ë° ê²€ì¦ ê²°ê³¼ âœ…

### ìµœì¢… êµ¬í˜„ ìƒíƒœ (2025-10-31)

**ì™„ë£Œìœ¨**: 100% (Few-shot í•™ìŠµ í†µí•© ì™„ë£Œ)

**êµ¬í˜„ëœ í•µì‹¬ ê¸°ëŠ¥**:
1. âœ… Few-shot ìƒ˜í”Œ í’ˆì§ˆ í•„í„°ë§ (accuracy â‰¥ 0.8)
2. âœ… Learning Service â†” Judgment Service í†µí•©
3. âœ… 3-Tier íŒë‹¨ ì „ëµ (Rule â†’ LLM + Few-shot)
4. âœ… ì‹ ë¢°ë„ ìë™ ì¡°ì • (ìƒ˜í”Œ ê°œìˆ˜ ê¸°ë°˜)
5. âœ… Rule ì¶”ì¶œ ë° ìë™ ì €ì¥ ì›Œí¬í”Œë¡œìš°

### í…ŒìŠ¤íŠ¸ ê²€ì¦ ê²°ê³¼

**í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€**: 33ê°œ í…ŒìŠ¤íŠ¸ ëª¨ë‘ í†µê³¼ (100%)

**ì¶”ê°€ëœ í†µí•© í…ŒìŠ¤íŠ¸ (5ê°œ)**:

1. **test_few_shot_sample_quality_filter** (34 lines)
   - **ëª©ì **: Few-shot ìƒ˜í”Œ í’ˆì§ˆ í•„í„°ë§ ê²€ì¦
   - **ê²€ì¦ ë‚´ìš©**:
     - ê³ í’ˆì§ˆ ìƒ˜í”Œ (accuracy=0.95) âœ… í¬í•¨
     - ì €í’ˆì§ˆ ìƒ˜í”Œ (accuracy=0.5) âŒ ì œì™¸
   - **ê²°ê³¼**: PASS âœ…

2. **test_integration_learning_judgment** (47 lines)
   - **ëª©ì **: Learning â†’ Judgment ì „ì²´ í†µí•© í”Œë¡œìš° ê²€ì¦
   - **ê²€ì¦ ë‚´ìš©**:
     - Workflow ìƒì„± â†’ Training ìƒ˜í”Œ ì¶”ê°€ â†’ Few-shot íŒë‹¨ ì‹¤í–‰
     - 2ê°œ ê³ í’ˆì§ˆ ìƒ˜í”Œ (0.95, 0.92) í™œìš©
   - **ê²°ê³¼**: PASS âœ…

3. **test_rule_save_after_extraction** (33 lines)
   - **ëª©ì **: Rule ì¶”ì¶œ í›„ Workflow ì €ì¥ ê²€ì¦
   - **ê²€ì¦ ë‚´ìš©**:
     - ì´ˆê¸° ìƒíƒœ: Rule ì—†ìŒ (version=1)
     - Rule ì¶”ì¶œ â†’ ì €ì¥ â†’ version=2ë¡œ ì¦ê°€
   - **ê²°ê³¼**: PASS âœ…

4. **test_few_shot_confidence_boost** (48 lines)
   - **ëª©ì **: ì‹ ë¢°ë„ ìë™ ì¡°ì • ì•Œê³ ë¦¬ì¦˜ ê²€ì¦
   - **ê²€ì¦ ë‚´ìš©**:
     - 12ê°œ ìƒ˜í”Œ ìƒì„± (â‰¥10ê°œ)
     - ì‹ ë¢°ë„ ë¶€ìŠ¤íŠ¸ ì ìš© í™•ì¸ (confidence * 1.1)
   - **ê²°ê³¼**: PASS âœ…

5. **test_few_shot_method_naming** (31 lines)
   - **ëª©ì **: method_used í•„ë“œ ì •í™•ì„± ê²€ì¦
   - **ê²€ì¦ ë‚´ìš©**:
     - Few-shot ì‚¬ìš©ì‹œ: "llm_few_shot" âœ…
     - Few-shot ë¯¸ì‚¬ìš©ì‹œ: "llm" âœ…
   - **ê²°ê³¼**: PASS âœ…

### í•µì‹¬ ìˆ˜ì • ì‚¬í•­ (learning_service.rs)

**ìˆ˜ì • ì „** (í’ˆì§ˆ í•„í„° ì—†ìŒ):
```rust
pub fn get_few_shot_samples(&self, workflow_id: String, limit: u32)
    -> anyhow::Result<Vec<TrainingSample>> {
    self.db.get_training_samples(&workflow_id, limit)
        .map_err(|e| anyhow::anyhow!(e))
}
```

**ìˆ˜ì • í›„** (accuracy â‰¥ 0.8 í•„í„° ì ìš©):
```rust
pub fn get_few_shot_samples(&self, workflow_id: String, limit: u32)
    -> anyhow::Result<Vec<TrainingSample>> {
    // ì •í™•ë„ê°€ ë†’ì€ í›ˆë ¨ ìƒ˜í”Œë§Œ ê°€ì ¸ì˜¤ê¸° (accuracy >= 0.8)
    let samples = self.db.get_training_samples(&workflow_id, limit * 2)
        .map_err(|e| anyhow::anyhow!(e))?;

    Ok(samples
        .into_iter()
        .filter(|s| s.accuracy.unwrap_or(0.0) >= 0.8)
        .take(limit as usize)
        .collect())
}
```

### ì„±ëŠ¥ ë©”íŠ¸ë¦­ (ì˜ˆìƒ)

| ë©”íŠ¸ë¦­ | Few-shot ì—†ìŒ | Few-shot í™œìš© | ê°œì„ ìœ¨ |
|--------|--------------|--------------|--------|
| **íŒë‹¨ ì •í™•ë„** | 85% | 95% | +10%p |
| **ì‹ ë¢°ë„ ì ìˆ˜** | 0.75 | 0.85 | +13% |
| **LLM ë¹„ìš©** | $0.10/íŒë‹¨ | $0.12/íŒë‹¨ | +20% |
| **ì‘ë‹µ ì‹œê°„** | 800ms | 1,200ms | +50% |

**ë¹„ìš© ëŒ€ë¹„ íš¨ê³¼**:
- 20% ë¹„ìš© ì¦ê°€ â†’ 10%p ì •í™•ë„ í–¥ìƒ
- ROI: ì˜ëª»ëœ íŒë‹¨ ë°©ì§€ë¡œ ì¸í•œ ì†ì‹¤ ê°ì†Œ >> LLM ë¹„ìš© ì¦ê°€

### ë‹¤ìŒ ë‹¨ê³„ ê¶Œì¥ì‚¬í•­

1. **ì„±ëŠ¥ ìµœì í™”**:
   - Few-shot ìƒ˜í”Œ ìºì‹± (Redis)
   - ì„ë² ë”© ë¯¸ë¦¬ ê³„ì‚° (pgvector)
   - ë³‘ë ¬ ì²˜ë¦¬ (tokio::spawn)

2. **ê¸°ëŠ¥ í™•ì¥**:
   - ë™ì  ìƒ˜í”Œ ê°œìˆ˜ ì¡°ì • (ì›Œí¬í”Œë¡œìš°ë³„)
   - ìƒ˜í”Œ í’ˆì§ˆ ì„ê³„ê°’ ì„¤ì • UI
   - Few-shot íš¨ê³¼ ë¶„ì„ ëŒ€ì‹œë³´ë“œ

3. **ë¬¸ì„œí™”**:
   - ì‚¬ìš©ì ê°€ì´ë“œ ì¶”ê°€ (Few-shot í•™ìŠµ í™œìš©ë²•)
   - API ì˜ˆì‹œ ì½”ë“œ ì¶”ê°€
   - íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ

ì´ ì„¤ê³„ì„œëŠ” ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œ ë°”ë¡œ ì ìš© ê°€ëŠ¥í•œ ìƒì„¸í•œ ê¸°ìˆ  ëª…ì„¸ì™€ ì‹¤ì „ ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.
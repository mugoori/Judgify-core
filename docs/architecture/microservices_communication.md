# ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ê°„ í†µì‹  ì•„í‚¤í…ì²˜ ì„¤ê³„ì„œ

**ë¬¸ì„œ ë²„ì „**: v2.0  
**ì‘ì„±ì¼**: 2024.08.10  
**ëŒ€ìƒ**: ì‹œìŠ¤í…œ ì•„í‚¤í…íŠ¸, ë°±ì—”ë“œ ê°œë°œì, DevOps ì—”ì§€ë‹ˆì–´  
**ëª©ì **: 6ê°œ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ê°„ í†µì‹  íŒ¨í„´, API ì„¤ê³„ ë° ë°ì´í„° íë¦„ ìµœì í™”

## ğŸ“‹ 1. í†µì‹  ì•„í‚¤í…ì²˜ ê°œìš”

### 1.1 ì„œë¹„ìŠ¤ ê°„ í†µì‹  ë§¤íŠ¸ë¦­ìŠ¤

| ì„œë¹„ìŠ¤ | Workflow | Judgment | Action | Notification | Logging | Dashboard |
|---------|----------|----------|--------|--------------|---------|-----------|
| **Workflow (8001)** | - | Sync/REST | Event | Event | Async/Log | Sync/GraphQL |
| **Judgment (8002)** | Sync/REST | - | Event | Event | Async/Log | WebSocket |
| **Action (8003)** | Event | Event | - | Sync/REST | Async/Log | Event |
| **Notification (8004)** | Event | Event | Event | - | Async/Log | Event |
| **Logging (8005)** | Async/Log | Async/Log | Async/Log | Async/Log | - | GraphQL |
| **Dashboard (8006)** | GraphQL | WebSocket | Event | Event | GraphQL | - |

### 1.2 í†µì‹  íŒ¨í„´ ë¶„ë¥˜

```mermaid
graph TB
    subgraph "ë™ê¸° í†µì‹  (Synchronous)"
        SYNC_REST[REST API]
        SYNC_GRPC[gRPC]
        SYNC_GQL[GraphQL]
    end
    
    subgraph "ë¹„ë™ê¸° í†µì‹  (Asynchronous)"
        ASYNC_EVENT[Event Bus]
        ASYNC_MSG[Message Queue]
        ASYNC_WS[WebSocket]
    end
    
    subgraph "ë°ì´í„° íë¦„ (Data Flow)"
        STREAM[Real-time Stream]
        BATCH[Batch Processing]
        CACHE[Cache Layer]
    end
    
    SYNC_REST --> ASYNC_EVENT
    ASYNC_EVENT --> STREAM
    SYNC_GQL --> CACHE
    ASYNC_WS --> STREAM
```

## ğŸ”§ 2. API ì„¤ê³„ ì „ëµ

### 2.1 ì„œë¹„ìŠ¤ë³„ API ì „ëµ ë§¤í•‘

#### 2.1.1 Workflow Service (8001) - REST API ì¤‘ì‹¬
```yaml
API_Strategy: REST + GraphQL Federation
Reasoning: "CRUD ì‘ì—…ì´ ì£¼ìš”í•˜ë©°, ë³µì¡í•œ ì¿¼ë¦¬ ì§€ì› í•„ìš”"
Endpoints:
  - POST /api/v1/workflows (ì›Œí¬í”Œë¡œìš° ìƒì„±)
  - GET /api/v1/workflows/{id} (ë‹¨ì¼ ì¡°íšŒ)
  - PUT /api/v1/workflows/{id} (ìˆ˜ì •)
  - DELETE /api/v1/workflows/{id} (ì‚­ì œ)
  - POST /api/v1/workflows/{id}/simulate (ì‹œë®¬ë ˆì´ì…˜)
GraphQL_Federation: 
  - workflows íƒ€ì… ì œê³µ
  - Dashboard Serviceì—ì„œ ë³µì¡í•œ ì¿¼ë¦¬ ì§€ì›
```

#### 2.1.2 Judgment Service (8002) - gRPC + WebSocket
```yaml
API_Strategy: gRPC (ë‚´ë¶€) + REST (ì™¸ë¶€) + WebSocket (ì‹¤ì‹œê°„)
Reasoning: "ê³ ì„±ëŠ¥ íŒë‹¨ ì²˜ë¦¬ + ì‹¤ì‹œê°„ ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë°"
gRPC_Services:
  - JudgmentService.Execute (ë‚´ë¶€ í˜¸ì¶œìš©)
  - JudgmentService.ValidateInput
REST_Endpoints:
  - POST /api/v1/judgment/execute (ì™¸ë¶€ í˜¸ì¶œìš©)
  - GET /api/v1/judgment/status/{execution_id}
WebSocket:
  - /ws/judgment/realtime (ì‹¤ì‹œê°„ íŒë‹¨ ê²°ê³¼)
```

#### 2.1.3 Dashboard Service (8006) - GraphQL Federation
```yaml
API_Strategy: GraphQL Federation + WebSocket
Reasoning: "ë³µì¡í•œ ë°ì´í„° ì¡°í•© + ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸"
GraphQL_Schema:
  - ëª¨ë“  ì„œë¹„ìŠ¤ ë°ì´í„° í†µí•© ì¡°íšŒ
  - ìë™ ëŒ€ì‹œë³´ë“œ ìƒì„± ì¿¼ë¦¬
WebSocket:
  - /ws/dashboard/updates (ì‹¤ì‹œê°„ ë°ì´í„° ì—…ë°ì´íŠ¸)
  - /ws/dashboard/generation (ëŒ€ì‹œë³´ë“œ ìƒì„± ì§„í–‰ìƒí™©)
```

#### 2.1.4 Action/Notification/Logging - Event-Driven
```yaml
API_Strategy: Event-Driven + REST for Control
Event_Patterns:
  - judgment.completed â†’ action.trigger
  - action.executed â†’ notification.send
  - *.* â†’ logging.store
REST_Control:
  - Action Service: POST /api/v1/actions/retry/{id}
  - Notification: GET /api/v1/notifications/templates
  - Logging: GET /api/v1/logs/search
```

### 2.2 API ê²Œì´íŠ¸ì›¨ì´ ë¼ìš°íŒ… ì „ëµ

#### 2.2.1 Kong Gateway ì„¤ì •
```yaml
# kong-routes.yaml
services:
  - name: workflow-service
    url: http://workflow-service:8001
    routes:
      - name: workflow-api
        paths: ["/api/v1/workflows"]
        methods: ["GET", "POST", "PUT", "DELETE"]
        
  - name: judgment-service  
    url: http://judgment-service:8002
    routes:
      - name: judgment-api
        paths: ["/api/v1/judgment"]
        methods: ["POST", "GET"]
        
  - name: dashboard-graphql
    url: http://dashboard-service:8006
    routes:
      - name: graphql-endpoint
        paths: ["/graphql"]
        methods: ["POST"]

plugins:
  - name: rate-limiting
    config:
      minute: 1000
      hour: 10000
      policy: redis
      
  - name: jwt
    config:
      secret_is_base64: false
      claims_to_verify: ["exp", "sub"]
      
  - name: cors
    config:
      origins: ["https://judgify.app", "http://localhost:3000"]
      methods: ["GET", "POST", "PUT", "DELETE", "OPTIONS"]
```

#### 2.2.2 ë¼ìš°íŒ… ê·œì¹™
```python
# ë¼ìš°íŒ… ìš°ì„ ìˆœìœ„
ROUTING_RULES = {
    "high_priority": [
        "/api/v1/judgment/execute",  # ì‹¤ì‹œê°„ íŒë‹¨
        "/ws/judgment/realtime",     # WebSocket íŒë‹¨
        "/api/v1/workflows/{id}/simulate"  # ì‹œë®¬ë ˆì´ì…˜
    ],
    "medium_priority": [
        "/api/v1/workflows",         # ì›Œí¬í”Œë¡œìš° CRUD
        "/graphql",                  # GraphQL ì¿¼ë¦¬
        "/ws/dashboard/updates"      # ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸
    ],
    "low_priority": [
        "/api/v1/logs",              # ë¡œê·¸ ì¡°íšŒ
        "/api/v1/notifications"      # ì•Œë¦¼ ê´€ë¦¬
    ]
}

# ë¡œë“œë°¸ëŸ°ì‹± ì „ëµ
LOAD_BALANCING = {
    "judgment-service": "least_connections",  # CPU ì§‘ì•½ì 
    "dashboard-service": "round_robin",       # I/O ì§‘ì•½ì 
    "workflow-service": "consistent_hash",    # ì„¸ì…˜ ì¹œí™”ì 
    "default": "weighted_round_robin"
}
```

## ğŸ”„ 3. ì„œë¹„ìŠ¤ ê°„ í†µì‹  íŒ¨í„´

### 3.1 ë™ê¸° í†µì‹  (Synchronous Communication)

#### 3.1.1 REST API í†µì‹ 
```python
# Workflow â†’ Judgment ë™ê¸° í˜¸ì¶œ íŒ¨í„´
class WorkflowExecutor:
    def __init__(self, judgment_client: JudgmentServiceClient):
        self.judgment_client = judgment_client
    
    async def execute_workflow(self, workflow_id: str, input_data: dict):
        """ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ - íŒë‹¨ ì„œë¹„ìŠ¤ ë™ê¸° í˜¸ì¶œ"""
        
        # 1. ì›Œí¬í”Œë¡œìš° ì •ì˜ ë¡œë“œ
        workflow = await self.load_workflow(workflow_id)
        
        # 2. íŒë‹¨ ì„œë¹„ìŠ¤ í˜¸ì¶œ (ë™ê¸°)
        judgment_request = JudgmentRequest(
            workflow_id=workflow_id,
            input_data=input_data,
            method="hybrid"
        )
        
        # HTTP í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
        async with httpx.AsyncClient() as client:
            response = await client.post(
                "http://judgment-service:8002/api/v1/judgment/execute",
                json=judgment_request.dict(),
                timeout=30.0
            )
            
            if response.status_code == 200:
                judgment_result = JudgmentResult(**response.json())
                return await self.process_judgment_result(judgment_result)
            else:
                raise JudgmentServiceError(f"Judgment failed: {response.text}")
```

#### 3.1.2 gRPC í†µì‹  (ê³ ì„±ëŠ¥ ë‚´ë¶€ í†µì‹ )
```python
# judgment_service.proto
syntax = "proto3";

package judgment;

service JudgmentService {
    rpc ExecuteJudgment(JudgmentRequest) returns (JudgmentResponse);
    rpc ValidateInput(ValidationRequest) returns (ValidationResponse);
    rpc GetExecutionStatus(StatusRequest) returns (StatusResponse);
}

message JudgmentRequest {
    string workflow_id = 1;
    map<string, string> input_data = 2;
    string method = 3;  // rule, llm, hybrid
}

message JudgmentResponse {
    bool success = 1;
    string result = 2;
    double confidence = 3;
    int32 execution_time_ms = 4;
    string explanation = 5;
    string error = 6;
}
```

```python
# gRPC í´ë¼ì´ì–¸íŠ¸ êµ¬í˜„
import grpc
from generated import judgment_pb2, judgment_pb2_grpc

class JudgmentGRPCClient:
    def __init__(self, server_address: str):
        self.channel = grpc.aio.insecure_channel(server_address)
        self.stub = judgment_pb2_grpc.JudgmentServiceStub(self.channel)
    
    async def execute_judgment(self, workflow_id: str, input_data: dict) -> dict:
        """gRPCë¥¼ í†µí•œ ê³ ì„±ëŠ¥ íŒë‹¨ ì‹¤í–‰"""
        
        request = judgment_pb2.JudgmentRequest(
            workflow_id=workflow_id,
            input_data=input_data,
            method="hybrid"
        )
        
        try:
            response = await self.stub.ExecuteJudgment(
                request, 
                timeout=5.0
            )
            
            return {
                "success": response.success,
                "result": response.result,
                "confidence": response.confidence,
                "execution_time_ms": response.execution_time_ms,
                "explanation": response.explanation
            }
            
        except grpc.RpcError as e:
            raise JudgmentServiceError(f"gRPC error: {e.code()}: {e.details()}")
```

#### 3.1.3 GraphQL Federation
```graphql
# schema.graphql (Dashboard Service)
type Query {
  # ì›Œí¬í”Œë¡œìš° ê´€ë ¨ (Workflow Service)
  workflows(filter: WorkflowFilter): [Workflow]
  workflow(id: ID!): Workflow
  
  # íŒë‹¨ ê²°ê³¼ ê´€ë ¨ (Judgment Service)
  judgmentExecutions(filter: JudgmentFilter): [JudgmentExecution]
  judgmentStats(timeRange: TimeRange): JudgmentStats
  
  # í†µí•© ëŒ€ì‹œë³´ë“œ ë°ì´í„°
  dashboardData(request: DashboardRequest): DashboardData
}

type Workflow @key(fields: "id") {
  id: ID!
  name: String!
  definition: JSON!
  executions: [JudgmentExecution] @requires(fields: "id")
}

type JudgmentExecution @key(fields: "id") {
  id: ID!
  workflow: Workflow @provides(fields: "id")
  result: JSON!
  confidence: Float!
  executionTimeMs: Int!
}

type DashboardData {
  charts: [ChartComponent]
  metrics: [MetricCard]
  realTimeUpdates: Boolean
}
```

```python
# GraphQL Federation í´ë¼ì´ì–¸íŠ¸
import strawberry
from strawberry.federation import build_schema

@strawberry.type
class Query:
    async def dashboard_data(self, request: DashboardRequest) -> DashboardData:
        """ë‹¤ì¤‘ ì„œë¹„ìŠ¤ ë°ì´í„° í†µí•© ì¡°íšŒ"""
        
        # ë³‘ë ¬ë¡œ ì—¬ëŸ¬ ì„œë¹„ìŠ¤ì—ì„œ ë°ì´í„° ìˆ˜ì§‘
        workflow_data, judgment_data, action_data = await asyncio.gather(
            self.workflow_client.get_workflows(request.workflow_filter),
            self.judgment_client.get_executions(request.time_range),
            self.action_client.get_actions(request.action_filter)
        )
        
        # ëŒ€ì‹œë³´ë“œ ì»´í¬ë„ŒíŠ¸ ìƒì„±
        dashboard = await self.dashboard_generator.create_dashboard(
            user_request=request.description,
            available_data={
                "workflows": workflow_data,
                "judgments": judgment_data, 
                "actions": action_data
            }
        )
        
        return dashboard
```

### 3.2 ë¹„ë™ê¸° í†µì‹  (Asynchronous Communication)

#### 3.2.1 Event-Driven Architecture
```python
# ì´ë²¤íŠ¸ ì •ì˜
from pydantic import BaseModel
from typing import Any, Dict
from datetime import datetime

class BaseEvent(BaseModel):
    event_id: str
    event_type: str
    timestamp: datetime
    source_service: str
    correlation_id: str
    data: Dict[str, Any]

class JudgmentCompletedEvent(BaseEvent):
    event_type: str = "judgment.completed"
    data: Dict[str, Any]  # judgment_result, workflow_id, input_data

class ActionTriggerEvent(BaseEvent):
    event_type: str = "action.trigger"
    data: Dict[str, Any]  # action_type, target_system, command

class DashboardUpdateEvent(BaseEvent):
    event_type: str = "dashboard.update"
    data: Dict[str, Any]  # dashboard_id, update_type, data_changes
```

```python
# Redis Streams ê¸°ë°˜ Event Bus
import aioredis
import json

class EventBus:
    def __init__(self, redis_url: str):
        self.redis = aioredis.from_url(redis_url)
    
    async def publish_event(self, event: BaseEvent):
        """ì´ë²¤íŠ¸ ë°œí–‰"""
        
        stream_name = f"events:{event.event_type}"
        
        await self.redis.xadd(
            stream_name,
            {
                "event_id": event.event_id,
                "data": json.dumps(event.dict(), default=str),
                "timestamp": event.timestamp.isoformat()
            }
        )
    
    async def subscribe_to_events(self, event_types: list, consumer_group: str):
        """ì´ë²¤íŠ¸ êµ¬ë…"""
        
        streams = {f"events:{event_type}": ">" for event_type in event_types}
        
        try:
            # Consumer Group ìƒì„±
            for stream in streams.keys():
                try:
                    await self.redis.xgroup_create(stream, consumer_group, id="0")
                except aioredis.ResponseError:
                    pass  # Group already exists
            
            while True:
                messages = await self.redis.xreadgroup(
                    consumer_group,
                    "consumer-1",
                    streams,
                    count=10,
                    block=1000
                )
                
                for stream, msgs in messages:
                    for msg_id, fields in msgs:
                        event_data = json.loads(fields[b'data'])
                        event = BaseEvent(**event_data)
                        
                        await self.handle_event(event)
                        
                        # ë©”ì‹œì§€ í™•ì¸
                        await self.redis.xack(stream, consumer_group, msg_id)
                        
        except Exception as e:
            logger.error(f"Event subscription error: {e}")
    
    async def handle_event(self, event: BaseEvent):
        """ì´ë²¤íŠ¸ ì²˜ë¦¬ (ê° ì„œë¹„ìŠ¤ì—ì„œ êµ¬í˜„)"""
        pass
```

#### 3.2.2 WebSocket ì‹¤ì‹œê°„ í†µì‹ 
```python
# WebSocket Manager
from fastapi import WebSocket, WebSocketDisconnect
from typing import List, Dict
import json

class WebSocketManager:
    def __init__(self):
        self.active_connections: Dict[str, List[WebSocket]] = {}
    
    async def connect(self, websocket: WebSocket, client_type: str, client_id: str):
        """WebSocket ì—°ê²° ê´€ë¦¬"""
        await websocket.accept()
        
        if client_type not in self.active_connections:
            self.active_connections[client_type] = []
        
        self.active_connections[client_type].append(websocket)
        logger.info(f"Client {client_id} connected to {client_type}")
    
    def disconnect(self, websocket: WebSocket, client_type: str):
        """ì—°ê²° í•´ì œ"""
        if client_type in self.active_connections:
            self.active_connections[client_type].remove(websocket)
    
    async def broadcast_to_type(self, client_type: str, message: dict):
        """íŠ¹ì • íƒ€ì… í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ë¸Œë¡œë“œìºìŠ¤íŠ¸"""
        if client_type in self.active_connections:
            disconnected = []
            
            for connection in self.active_connections[client_type]:
                try:
                    await connection.send_text(json.dumps(message))
                except WebSocketDisconnect:
                    disconnected.append(connection)
            
            # ëŠì–´ì§„ ì—°ê²° ì •ë¦¬
            for conn in disconnected:
                self.active_connections[client_type].remove(conn)

# Judgment Service WebSocket
from fastapi import FastAPI, WebSocket
websocket_manager = WebSocketManager()

@app.websocket("/ws/judgment/realtime/{client_id}")
async def judgment_websocket(websocket: WebSocket, client_id: str):
    await websocket_manager.connect(websocket, "judgment", client_id)
    
    try:
        while True:
            # í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° ë©”ì‹œì§€ ëŒ€ê¸°
            data = await websocket.receive_text()
            request = json.loads(data)
            
            # íŒë‹¨ ì‹¤í–‰ ë° ì‹¤ì‹œê°„ ê²°ê³¼ ì „ì†¡
            result = await judgment_engine.execute(request)
            
            await websocket.send_text(json.dumps({
                "type": "judgment_result",
                "data": result.dict()
            }))
            
    except WebSocketDisconnect:
        websocket_manager.disconnect(websocket, "judgment")

# Dashboard Service WebSocket
@app.websocket("/ws/dashboard/updates/{dashboard_id}")
async def dashboard_websocket(websocket: WebSocket, dashboard_id: str):
    await websocket_manager.connect(websocket, "dashboard", dashboard_id)
    
    try:
        while True:
            # ì‹¤ì‹œê°„ ë°ì´í„° ì—…ë°ì´íŠ¸ ìŠ¤íŠ¸ë¦¬ë°
            updates = await dashboard_service.get_real_time_updates(dashboard_id)
            
            await websocket.send_text(json.dumps({
                "type": "data_update",
                "dashboard_id": dashboard_id,
                "data": updates
            }))
            
            await asyncio.sleep(5)  # 5ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸
            
    except WebSocketDisconnect:
        websocket_manager.disconnect(websocket, "dashboard")
```

### 3.3 ë°ì´í„° íë¦„ ìµœì í™”

#### 3.3.1 CQRS íŒ¨í„´ ì ìš©
```python
# Commandì™€ Query ë¶„ë¦¬
from abc import ABC, abstractmethod

class Command(ABC):
    """ë³€ê²½ ì‘ì—…ì„ ìœ„í•œ Command ì¸í„°í˜ì´ìŠ¤"""
    pass

class Query(ABC):
    """ì¡°íšŒ ì‘ì—…ì„ ìœ„í•œ Query ì¸í„°í˜ì´ìŠ¤"""
    pass

# Workflow Serviceì˜ CQRS êµ¬í˜„
class CreateWorkflowCommand(Command):
    def __init__(self, name: str, definition: dict, created_by: str):
        self.name = name
        self.definition = definition
        self.created_by = created_by

class GetWorkflowQuery(Query):
    def __init__(self, workflow_id: str):
        self.workflow_id = workflow_id

class WorkflowCommandHandler:
    """Command ì²˜ë¦¬ - ì“°ê¸° ì‘ì—…"""
    
    def __init__(self, write_db: PostgreSQLConnection, event_bus: EventBus):
        self.write_db = write_db
        self.event_bus = event_bus
    
    async def handle_create_workflow(self, command: CreateWorkflowCommand):
        """ì›Œí¬í”Œë¡œìš° ìƒì„± ì²˜ë¦¬"""
        
        # 1. ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        workflow = await self.write_db.execute(
            """
            INSERT INTO workflows (name, definition, created_by, created_at)
            VALUES ($1, $2, $3, NOW())
            RETURNING id, name, created_at
            """,
            command.name, command.definition, command.created_by
        )
        
        # 2. ì´ë²¤íŠ¸ ë°œí–‰
        event = BaseEvent(
            event_type="workflow.created",
            source_service="workflow-service",
            data={
                "workflow_id": workflow["id"],
                "name": workflow["name"],
                "created_by": command.created_by
            }
        )
        
        await self.event_bus.publish_event(event)
        
        return workflow

class WorkflowQueryHandler:
    """Query ì²˜ë¦¬ - ì½ê¸° ì‘ì—…"""
    
    def __init__(self, read_db: PostgreSQLReadReplica, cache: Redis):
        self.read_db = read_db
        self.cache = cache
    
    async def handle_get_workflow(self, query: GetWorkflowQuery):
        """ì›Œí¬í”Œë¡œìš° ì¡°íšŒ ì²˜ë¦¬"""
        
        # 1. ìºì‹œ í™•ì¸
        cache_key = f"workflow:{query.workflow_id}"
        cached_workflow = await self.cache.get(cache_key)
        
        if cached_workflow:
            return json.loads(cached_workflow)
        
        # 2. ì½ê¸° ì „ìš© DBì—ì„œ ì¡°íšŒ
        workflow = await self.read_db.fetchrow(
            "SELECT * FROM workflows WHERE id = $1",
            query.workflow_id
        )
        
        if workflow:
            # 3. ìºì‹œì— ì €ì¥ (TTL: 10ë¶„)
            await self.cache.setex(
                cache_key, 
                600, 
                json.dumps(dict(workflow))
            )
        
        return workflow
```

#### 3.3.2 Event Sourcing ì ìš©
```python
# Event Store êµ¬í˜„
class EventStore:
    def __init__(self, db: PostgreSQLConnection):
        self.db = db
    
    async def append_event(self, aggregate_id: str, event: BaseEvent, expected_version: int):
        """ì´ë²¤íŠ¸ ì €ì¥"""
        
        try:
            await self.db.execute(
                """
                INSERT INTO event_store (
                    aggregate_id, event_type, event_data, 
                    event_version, created_at
                )
                VALUES ($1, $2, $3, $4, NOW())
                """,
                aggregate_id,
                event.event_type,
                json.dumps(event.data),
                expected_version + 1
            )
        except Exception as e:
            if "duplicate key" in str(e):
                raise ConcurrencyError("Event version conflict")
            raise
    
    async def get_events(self, aggregate_id: str, from_version: int = 0):
        """ì´ë²¤íŠ¸ ì¡°íšŒ"""
        
        events = await self.db.fetch(
            """
            SELECT event_type, event_data, event_version, created_at
            FROM event_store
            WHERE aggregate_id = $1 AND event_version > $2
            ORDER BY event_version
            """,
            aggregate_id, from_version
        )
        
        return [
            {
                "event_type": event["event_type"],
                "data": json.loads(event["event_data"]),
                "version": event["event_version"],
                "timestamp": event["created_at"]
            }
            for event in events
        ]

# Aggregate ì¬êµ¬ì„±
class WorkflowAggregate:
    def __init__(self, workflow_id: str):
        self.workflow_id = workflow_id
        self.name = None
        self.definition = None
        self.version = 0
        self.is_active = True
    
    def apply_event(self, event: dict):
        """ì´ë²¤íŠ¸ ì ìš©í•˜ì—¬ ìƒíƒœ ì¬êµ¬ì„±"""
        
        if event["event_type"] == "workflow.created":
            self.name = event["data"]["name"]
            self.definition = event["data"]["definition"]
            
        elif event["event_type"] == "workflow.updated":
            self.definition = event["data"]["definition"]
            
        elif event["event_type"] == "workflow.deactivated":
            self.is_active = False
        
        self.version = event["version"]
    
    @classmethod
    async def load_from_events(cls, workflow_id: str, event_store: EventStore):
        """ì´ë²¤íŠ¸ë¡œë¶€í„° Aggregate ì¬êµ¬ì„±"""
        
        aggregate = cls(workflow_id)
        events = await event_store.get_events(workflow_id)
        
        for event in events:
            aggregate.apply_event(event)
        
        return aggregate
```

## ğŸ“Š 4. ë°ì´í„° íë¦„ ì‹œë‚˜ë¦¬ì˜¤

### 4.1 í•˜ì´ë¸Œë¦¬ë“œ íŒë‹¨ ì‹¤í–‰ í”Œë¡œìš°
```mermaid
sequenceDiagram
    participant U as User/API
    participant AG as API Gateway
    participant WS as Workflow Service
    participant JS as Judgment Service
    participant AS as Action Service
    participant NS as Notification Service
    participant LS as Logging Service
    participant DS as Dashboard Service
    participant DB as PostgreSQL
    participant R as Redis
    participant EB as Event Bus

    U->>AG: POST /api/v1/workflows/execute
    AG->>WS: Forward request
    
    WS->>DB: Load workflow definition
    DB-->>WS: Workflow config
    
    WS->>JS: Execute judgment (gRPC)
    JS->>R: Check cache
    alt Cache miss
        JS->>DB: Load judgment history
        JS->>JS: Execute hybrid logic
        JS->>R: Store result
    end
    JS-->>WS: Judgment result
    
    WS->>EB: Publish judgment.completed
    
    par Parallel Processing
        EB->>AS: judgment.completed event
        AS->>AS: Process actions
        AS->>EB: Publish action.executed
        
        EB->>NS: action.executed event
        NS->>NS: Send notifications
        
        EB->>LS: Store all events
        LS->>DB: Persist logs
        
        EB->>DS: Data update event
        DS->>DS: Update dashboards
        DS-->>U: WebSocket update
    end
    
    WS-->>AG: Execution result
    AG-->>U: Response
```

### 4.2 ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ ìƒì„± í”Œë¡œìš°
```mermaid
sequenceDiagram
    participant U as User
    participant DS as Dashboard Service
    participant WS as Workflow Service
    participant JS as Judgment Service
    participant LS as Logging Service
    participant LLM as OpenAI API
    participant DB as PostgreSQL
    participant WS_Client as WebSocket

    U->>DS: "ì§€ë‚œ ì£¼ ì„±ê³µë¥  ì°¨íŠ¸ ë³´ì—¬ì¤˜"
    DS->>LLM: Analyze request
    LLM-->>DS: Data requirements
    
    par Data Collection
        DS->>WS: GraphQL: workflows query
        WS->>DB: Query workflows
        DB-->>WS: Workflow data
        WS-->>DS: Workflow response
        
        DS->>JS: GraphQL: judgment stats
        JS->>DB: Query executions
        DB-->>JS: Execution data
        JS-->>DS: Stats response
        
        DS->>LS: GraphQL: log analysis
        LS->>DB: Query logs
        DB-->>LS: Log data
        LS-->>DS: Analysis response
    end
    
    DS->>LLM: Generate dashboard
    LLM-->>DS: React components
    
    DS->>DB: Store dashboard config
    DS->>WS_Client: Real-time updates
    DS-->>U: Generated dashboard
```

### 4.3 ì¥ì•  ëŒ€ì‘ ë° ë³µêµ¬ í”Œë¡œìš°
```mermaid
sequenceDiagram
    participant JS as Judgment Service
    participant HM as Health Monitor
    participant LB as Load Balancer
    participant JS2 as Judgment Service (Backup)
    participant EB as Event Bus
    participant NS as Notification Service

    JS->>JS: Health check fails
    HM->>HM: Detect failure
    HM->>LB: Remove JS from pool
    LB->>JS2: Route traffic to backup
    
    HM->>EB: Publish service.down event
    EB->>NS: Service alert
    NS->>NS: Send urgent notification
    
    JS->>JS: Restart attempt
    JS->>HM: Health check OK
    HM->>LB: Add JS back to pool
    HM->>EB: Publish service.recovered
```

## ğŸ” 5. ë³´ì•ˆ ë° ì¸ì¦ ì „ëµ

### 5.1 ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ê°„ ì¸ì¦
```python
# JWT ê¸°ë°˜ ì„œë¹„ìŠ¤ ê°„ ì¸ì¦
import jwt
from datetime import datetime, timedelta

class ServiceAuthenticator:
    def __init__(self, secret_key: str, algorithm: str = "HS256"):
        self.secret_key = secret_key
        self.algorithm = algorithm
    
    def generate_service_token(self, service_name: str, permissions: list) -> str:
        """ì„œë¹„ìŠ¤ ê°„ í†µì‹ ìš© JWT ìƒì„±"""
        
        payload = {
            "sub": service_name,
            "iat": datetime.utcnow(),
            "exp": datetime.utcnow() + timedelta(hours=24),
            "type": "service",
            "permissions": permissions
        }
        
        return jwt.encode(payload, self.secret_key, algorithm=self.algorithm)
    
    def verify_service_token(self, token: str) -> dict:
        """ì„œë¹„ìŠ¤ í† í° ê²€ì¦"""
        
        try:
            payload = jwt.decode(token, self.secret_key, algorithms=[self.algorithm])
            
            if payload.get("type") != "service":
                raise jwt.InvalidTokenError("Invalid token type")
            
            return payload
            
        except jwt.ExpiredSignatureError:
            raise AuthenticationError("Service token expired")
        except jwt.InvalidTokenError:
            raise AuthenticationError("Invalid service token")

# FastAPI ë¯¸ë“¤ì›¨ì–´ë¡œ ì„œë¹„ìŠ¤ ì¸ì¦ ì ìš©
from fastapi import FastAPI, Request, HTTPException
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials

class ServiceAuthMiddleware:
    def __init__(self, auth: ServiceAuthenticator):
        self.auth = auth
        self.security = HTTPBearer()
    
    async def __call__(self, request: Request, call_next):
        # ë‚´ë¶€ ì„œë¹„ìŠ¤ í˜¸ì¶œì¸ì§€ í™•ì¸
        if self.is_internal_call(request):
            try:
                auth_header = request.headers.get("Authorization")
                if not auth_header:
                    raise HTTPException(401, "Missing service authentication")
                
                token = auth_header.replace("Bearer ", "")
                payload = self.auth.verify_service_token(token)
                
                # Request contextì— ì„œë¹„ìŠ¤ ì •ë³´ ì¶”ê°€
                request.state.service_name = payload["sub"]
                request.state.permissions = payload["permissions"]
                
            except AuthenticationError as e:
                raise HTTPException(401, str(e))
        
        response = await call_next(request)
        return response
    
    def is_internal_call(self, request: Request) -> bool:
        """ë‚´ë¶€ ì„œë¹„ìŠ¤ í˜¸ì¶œ ì—¬ë¶€ íŒë‹¨"""
        
        # X-Service-Call í—¤ë”ë¡œ ë‚´ë¶€ í˜¸ì¶œ ì‹ë³„
        return request.headers.get("X-Service-Call") == "true"
```

### 5.2 API ë³´ì•ˆ ê°•í™”
```python
# Rate Limiting êµ¬í˜„
from collections import defaultdict, deque
import time
import asyncio

class AdvancedRateLimiter:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.rate_limits = {
            "judgment.execute": {"requests": 100, "window": 60},    # 1ë¶„ì— 100íšŒ
            "dashboard.generate": {"requests": 10, "window": 60},   # 1ë¶„ì— 10íšŒ  
            "workflow.create": {"requests": 50, "window": 3600},    # 1ì‹œê°„ì— 50íšŒ
            "default": {"requests": 1000, "window": 3600}           # ê¸°ë³¸ ì œí•œ
        }
    
    async def check_rate_limit(self, client_id: str, endpoint: str) -> bool:
        """Rate limit í™•ì¸"""
        
        limit_config = self.rate_limits.get(endpoint, self.rate_limits["default"])
        
        # Redisë¥¼ ì‚¬ìš©í•œ sliding window êµ¬í˜„
        now = int(time.time())
        window_start = now - limit_config["window"]
        
        pipe = self.redis.pipeline()
        
        # ìœˆë„ìš° ë²”ìœ„ ë°–ì˜ ìš”ì²­ ì œê±°
        pipe.zremrangebyscore(
            f"rate_limit:{client_id}:{endpoint}",
            0, window_start
        )
        
        # í˜„ì¬ ìš”ì²­ ìˆ˜ í™•ì¸
        pipe.zcard(f"rate_limit:{client_id}:{endpoint}")
        
        # í˜„ì¬ ìš”ì²­ ì¶”ê°€
        pipe.zadd(
            f"rate_limit:{client_id}:{endpoint}",
            {str(now): now}
        )
        
        # TTL ì„¤ì •
        pipe.expire(f"rate_limit:{client_id}:{endpoint}", limit_config["window"])
        
        results = await pipe.execute()
        current_requests = results[1]
        
        return current_requests < limit_config["requests"]

# Input Validation ê°•í™”
from pydantic import BaseModel, validator
import re

class WorkflowCreateRequest(BaseModel):
    name: str
    definition: dict
    description: str = None
    
    @validator('name')
    def validate_name(cls, v):
        if not re.match(r'^[a-zA-Z0-9_-]{3,50}$', v):
            raise ValueError('Name must be alphanumeric, 3-50 characters')
        return v
    
    @validator('definition')
    def validate_definition(cls, v):
        required_fields = ['nodes', 'edges', 'startNode']
        
        if not all(field in v for field in required_fields):
            raise ValueError(f'Definition must contain: {required_fields}')
        
        # ë…¸ë“œ ìˆ˜ ì œí•œ
        if len(v.get('nodes', [])) > 100:
            raise ValueError('Too many nodes (max: 100)')
        
        return v
    
    @validator('description')
    def validate_description(cls, v):
        if v and len(v) > 500:
            raise ValueError('Description too long (max: 500 characters)')
        return v

class JudgmentExecuteRequest(BaseModel):
    workflow_id: str
    input_data: dict
    method: str = "hybrid"
    
    @validator('workflow_id')
    def validate_workflow_id(cls, v):
        # UUID í˜•ì‹ ê²€ì¦
        import uuid
        try:
            uuid.UUID(v)
        except ValueError:
            raise ValueError('Invalid workflow ID format')
        return v
    
    @validator('input_data')
    def validate_input_data(cls, v):
        # JSON í¬ê¸° ì œí•œ (1MB)
        import json
        if len(json.dumps(v)) > 1024 * 1024:
            raise ValueError('Input data too large (max: 1MB)')
        
        # ì¤‘ì²© ê¹Šì´ ì œí•œ
        def check_depth(obj, depth=0):
            if depth > 10:
                raise ValueError('Input data too nested (max depth: 10)')
            
            if isinstance(obj, dict):
                for value in obj.values():
                    check_depth(value, depth + 1)
            elif isinstance(obj, list):
                for item in obj:
                    check_depth(item, depth + 1)
        
        check_depth(v)
        return v
    
    @validator('method')
    def validate_method(cls, v):
        if v not in ['rule', 'llm', 'hybrid']:
            raise ValueError('Method must be: rule, llm, or hybrid')
        return v
```

## ğŸ“ˆ 6. ì„±ëŠ¥ ìµœì í™” ì „ëµ

### 6.1 ìºì‹± ì „ëµ
```python
# ê³„ì¸µí™”ëœ ìºì‹± ì‹œìŠ¤í…œ
class MultiLevelCache:
    def __init__(self, l1_cache, l2_cache, l3_cache):
        self.l1 = l1_cache  # Redis (ë¹ ë¦„)
        self.l2 = l2_cache  # PostgreSQL (ì¤‘ê°„)
        self.l3 = l3_cache  # íŒŒì¼ ì‹œìŠ¤í…œ (ëŠë¦¼)
    
    async def get(self, key: str):
        """ë‹¤ë‹¨ê³„ ìºì‹œ ì¡°íšŒ"""
        
        # L1 ìºì‹œ í™•ì¸ (Redis)
        value = await self.l1.get(key)
        if value:
            return json.loads(value)
        
        # L2 ìºì‹œ í™•ì¸ (DB)
        value = await self.l2.get(key)
        if value:
            # L1ì— ì—…ë°ì´íŠ¸
            await self.l1.setex(key, 300, json.dumps(value))
            return value
        
        # L3 ìºì‹œ í™•ì¸ (íŒŒì¼)
        value = await self.l3.get(key)
        if value:
            # L1, L2ì— ì—…ë°ì´íŠ¸
            await self.l1.setex(key, 300, json.dumps(value))
            await self.l2.set(key, value, ttl=3600)
            return value
        
        return None
    
    async def set(self, key: str, value: any, ttl: int = 3600):
        """ë‹¤ë‹¨ê³„ ìºì‹œ ì €ì¥"""
        
        # ëª¨ë“  ë ˆë²¨ì— ì €ì¥
        await asyncio.gather(
            self.l1.setex(key, min(ttl, 300), json.dumps(value)),
            self.l2.set(key, value, ttl=ttl),
            self.l3.set(key, value, ttl=ttl*2)
        )

# ì„œë¹„ìŠ¤ë³„ ìºì‹± ì „ëµ
CACHE_STRATEGIES = {
    "workflow_definitions": {
        "ttl": 3600,  # 1ì‹œê°„
        "invalidate_on": ["workflow.updated", "workflow.deleted"]
    },
    "judgment_results": {
        "ttl": 300,   # 5ë¶„
        "key_pattern": "judgment:{workflow_id}:{input_hash}",
        "invalidate_on": ["workflow.updated"]
    },
    "dashboard_data": {
        "ttl": 60,    # 1ë¶„
        "refresh_async": True,  # ë°±ê·¸ë¼ìš´ë“œ ê°±ì‹ 
        "invalidate_on": ["judgment.completed", "action.executed"]
    },
    "user_preferences": {
        "ttl": 7200,  # 2ì‹œê°„
        "invalidate_on": ["user.updated"]
    }
}
```

### 6.2 ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”
```sql
-- ì¸ë±ìŠ¤ ì „ëµ
-- 1. íŒë‹¨ ì‹¤í–‰ ì´ë ¥ (ì‹œê³„ì—´ ë°ì´í„°)
CREATE INDEX CONCURRENTLY idx_judgment_executions_created_at 
    ON judgment_executions USING BRIN (created_at);

CREATE INDEX CONCURRENTLY idx_judgment_executions_workflow_created 
    ON judgment_executions (workflow_id, created_at DESC);

-- 2. ì›Œí¬í”Œë¡œìš° ì¡°íšŒ ìµœì í™”  
CREATE INDEX CONCURRENTLY idx_workflows_active_name 
    ON workflows (is_active, name) WHERE is_active = true;

-- 3. ì•¡ì…˜ ì‹¤í–‰ ìƒíƒœë³„ ì¡°íšŒ
CREATE INDEX CONCURRENTLY idx_action_executions_status_created
    ON action_executions (status, created_at) 
    WHERE status IN ('pending', 'running');

-- 4. ë¡œê·¸ ë°ì´í„° íŒŒí‹°ì…”ë‹
CREATE TABLE logs_y2024m08 PARTITION OF logs
    FOR VALUES FROM ('2024-08-01') TO ('2024-09-01');

-- 5. pgvector ìµœì í™” (RAG)
CREATE INDEX ON judgment_explanations 
    USING ivfflat (embedding vector_cosine_ops) 
    WITH (lists = 100);

-- ì¿¼ë¦¬ ìµœì í™” ì˜ˆì‹œ
-- Before (ë¹„íš¨ìœ¨ì )
SELECT j.*, w.name as workflow_name
FROM judgment_executions j
JOIN workflows w ON j.workflow_id = w.id
WHERE j.created_at >= NOW() - INTERVAL '7 days'
ORDER BY j.created_at DESC;

-- After (ìµœì í™”)
WITH recent_judgments AS (
    SELECT workflow_id, final_result, confidence_score, created_at
    FROM judgment_executions
    WHERE created_at >= NOW() - INTERVAL '7 days'
      AND created_at >= (
          SELECT created_at 
          FROM judgment_executions 
          ORDER BY created_at DESC 
          LIMIT 1 OFFSET 10000
      ) -- ìµœëŒ€ 10k ë ˆì½”ë“œë§Œ ì¡°íšŒ
)
SELECT rj.*, w.name as workflow_name
FROM recent_judgments rj
JOIN workflows w ON rj.workflow_id = w.id
ORDER BY rj.created_at DESC
LIMIT 100;
```

### 6.3 ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼
```python
# ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
from prometheus_client import Counter, Histogram, Gauge
import time

# ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­
judgment_executions_total = Counter(
    'judgment_executions_total',
    'Total number of judgment executions',
    ['method', 'status', 'workflow_id']
)

judgment_execution_duration = Histogram(
    'judgment_execution_duration_seconds',
    'Duration of judgment execution',
    ['method']
)

active_websocket_connections = Gauge(
    'websocket_connections_active',
    'Number of active WebSocket connections',
    ['service', 'connection_type']
)

# ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­
service_health_status = Gauge(
    'service_health_status',
    'Health status of each service (1=healthy, 0=unhealthy)',
    ['service_name']
)

database_connections_active = Gauge(
    'database_connections_active',
    'Number of active database connections',
    ['service', 'database']
)

# ìë™ ì•Œë¦¼ ì‹œìŠ¤í…œ
class AlertManager:
    def __init__(self, slack_webhook: str):
        self.slack_webhook = slack_webhook
        self.alert_thresholds = {
            "judgment_latency_high": 5.0,      # 5ì´ˆ ì´ìƒ
            "error_rate_high": 0.05,           # 5% ì´ìƒ
            "memory_usage_high": 0.85,         # 85% ì´ìƒ
            "disk_usage_high": 0.90,           # 90% ì´ìƒ
        }
    
    async def check_and_alert(self, metrics: dict):
        """ë©”íŠ¸ë¦­ ê¸°ë°˜ ìë™ ì•Œë¦¼"""
        
        alerts = []
        
        # ì§€ì—°ì‹œê°„ ì²´í¬
        if metrics.get("avg_judgment_latency", 0) > self.alert_thresholds["judgment_latency_high"]:
            alerts.append({
                "severity": "warning",
                "message": f"Judgment latency high: {metrics['avg_judgment_latency']:.2f}s"
            })
        
        # ì—ëŸ¬ìœ¨ ì²´í¬
        error_rate = metrics.get("error_rate", 0)
        if error_rate > self.alert_thresholds["error_rate_high"]:
            alerts.append({
                "severity": "critical",
                "message": f"Error rate high: {error_rate:.2%}"
            })
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì²´í¬
        memory_usage = metrics.get("memory_usage", 0)
        if memory_usage > self.alert_thresholds["memory_usage_high"]:
            alerts.append({
                "severity": "warning",
                "message": f"Memory usage high: {memory_usage:.2%}"
            })
        
        # ì•Œë¦¼ ë°œì†¡
        for alert in alerts:
            await self.send_alert(alert)
    
    async def send_alert(self, alert: dict):
        """Slack ì•Œë¦¼ ë°œì†¡"""
        
        color_map = {
            "critical": "#FF0000",
            "warning": "#FFA500", 
            "info": "#0099CC"
        }
        
        payload = {
            "attachments": [{
                "color": color_map.get(alert["severity"], "#808080"),
                "title": f"ğŸš¨ {alert['severity'].upper()} Alert",
                "text": alert["message"],
                "ts": int(time.time())
            }]
        }
        
        async with httpx.AsyncClient() as client:
            await client.post(self.slack_webhook, json=payload)
```

## ğŸš€ 7. ìŠ¤í‚¤ë§ˆ ë²„ì „ ê´€ë¦¬ ë° í˜¸í™˜ì„±

### 7.1 API ë²„ì „ ê´€ë¦¬ ì „ëµ
```python
# API ë²„ì „ ê´€ë¦¬
from enum import Enum

class APIVersion(Enum):
    V1 = "v1"
    V2 = "v2"  # í–¥í›„ í™•ì¥

# ë²„ì „ë³„ ë¼ìš°í„° ë¶„ë¦¬
from fastapi import APIRouter

v1_router = APIRouter(prefix="/api/v1")
v2_router = APIRouter(prefix="/api/v2")  # í–¥í›„

# í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€
@v1_router.post("/judgment/execute")
async def execute_judgment_v1(request: JudgmentExecuteRequestV1):
    """V1 API - í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€"""
    
    # V2 í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    v2_request = convert_v1_to_v2(request)
    
    # V2 ë¡œì§ ì‹¤í–‰
    result = await judgment_service.execute(v2_request)
    
    # V1 í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ ë³€í™˜
    return convert_v2_to_v1_response(result)

# ìŠ¤í‚¤ë§ˆ ì§„í™” ì „ëµ
class SchemaEvolution:
    def __init__(self):
        self.migration_strategies = {
            "workflow_definition_v1_to_v2": self.migrate_workflow_v1_to_v2,
            "judgment_result_v1_to_v2": self.migrate_judgment_v1_to_v2
        }
    
    def migrate_workflow_v1_to_v2(self, old_data: dict) -> dict:
        """ì›Œí¬í”Œë¡œìš° ìŠ¤í‚¤ë§ˆ ë§ˆì´ê·¸ë ˆì´ì…˜"""
        
        new_data = old_data.copy()
        
        # V2ì—ì„œ ì¶”ê°€ëœ í•„ë“œë“¤ì˜ ê¸°ë³¸ê°’ ì„¤ì •
        if "hybrid_strategy" not in new_data:
            new_data["hybrid_strategy"] = "rule_first"
        
        if "required_context" not in new_data:
            new_data["required_context"] = []
        
        # V1ì—ì„œ ì œê±°ëœ í•„ë“œë“¤ ì •ë¦¬
        deprecated_fields = ["legacy_field1", "legacy_field2"]
        for field in deprecated_fields:
            new_data.pop(field, None)
        
        return new_data
```

### 7.2 GraphQL ìŠ¤í‚¤ë§ˆ Federation
```graphql
# Gateway Schema (í†µí•©)
type Query {
  # Workflow Service
  workflow(id: ID!): Workflow
  workflows(filter: WorkflowFilter): [Workflow!]!
  
  # Judgment Service  
  judgmentExecution(id: ID!): JudgmentExecution
  judgmentStats(filter: StatsFilter): JudgmentStats
  
  # Dashboard Service
  dashboard(id: ID!): Dashboard
  generateDashboard(request: String!): DashboardGenerationResult
  
  # ì—°ê²°ëœ ë°ì´í„° ì¡°íšŒ
  workflowWithStats(id: ID!): WorkflowWithStats
}

# Workflow Service Schema
extend type Query {
  workflow(id: ID!): Workflow @provides(fields: "id name definition")
}

type Workflow @key(fields: "id") {
  id: ID!
  name: String!
  definition: JSON!
  version: Int!
  executions: [JudgmentExecution!]! @requires(fields: "id")
}

# Judgment Service Schema  
extend type Query {
  judgmentExecution(id: ID!): JudgmentExecution
}

type JudgmentExecution @key(fields: "id") {
  id: ID!
  workflow: Workflow! @provides(fields: "id")
  result: JSON!
  confidence: Float!
  method: JudgmentMethod!
}

extend type Workflow @key(fields: "id") {
  executions: [JudgmentExecution!]!
  successRate: Float!
  avgExecutionTime: Int!
}
```

## ğŸ“Š 8. ìš´ì˜ ë° ëª¨ë‹ˆí„°ë§

### 8.1 Health Check êµ¬í˜„
```python
# ì¢…í•©ì ì¸ Health Check
from datetime import datetime, timedelta
import asyncio

class HealthChecker:
    def __init__(self, services: dict):
        self.services = services
        self.health_cache = {}
        self.cache_ttl = 30  # 30ì´ˆ ìºì‹œ
    
    async def check_all_services(self) -> dict:
        """ëª¨ë“  ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
        
        results = {}
        
        # ë³‘ë ¬ë¡œ ëª¨ë“  ì„œë¹„ìŠ¤ ì²´í¬
        tasks = [
            self.check_service(name, config)
            for name, config in self.services.items()
        ]
        
        service_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for i, (name, _) in enumerate(self.services.items()):
            results[name] = service_results[i] if not isinstance(service_results[i], Exception) else {
                "status": "unhealthy",
                "error": str(service_results[i])
            }
        
        # ì „ì²´ ì‹œìŠ¤í…œ ìƒíƒœ ê³„ì‚°
        healthy_count = sum(1 for r in results.values() if r.get("status") == "healthy")
        total_count = len(results)
        
        overall_status = {
            "status": "healthy" if healthy_count == total_count else "degraded" if healthy_count > 0 else "unhealthy",
            "healthy_services": healthy_count,
            "total_services": total_count,
            "timestamp": datetime.utcnow().isoformat(),
            "services": results
        }
        
        return overall_status
    
    async def check_service(self, name: str, config: dict) -> dict:
        """ê°œë³„ ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
        
        # ìºì‹œ í™•ì¸
        cache_key = f"health:{name}"
        if cache_key in self.health_cache:
            cached_time, cached_result = self.health_cache[cache_key]
            if datetime.utcnow() - cached_time < timedelta(seconds=self.cache_ttl):
                return cached_result
        
        try:
            start_time = time.time()
            
            # ì„œë¹„ìŠ¤ë³„ ì²´í¬ ë¡œì§
            if name == "database":
                result = await self.check_database(config)
            elif name == "redis":
                result = await self.check_redis(config)
            elif name.endswith("-service"):
                result = await self.check_microservice(config)
            else:
                result = await self.check_generic(config)
            
            response_time = int((time.time() - start_time) * 1000)
            result["response_time_ms"] = response_time
            
            # ìºì‹œì— ì €ì¥
            self.health_cache[cache_key] = (datetime.utcnow(), result)
            
            return result
            
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat()
            }
    
    async def check_database(self, config: dict) -> dict:
        """ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸"""
        
        async with asyncpg.connect(config["url"]) as conn:
            # ê¸°ë³¸ ì—°ê²° í™•ì¸
            await conn.fetchval("SELECT 1")
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­
            active_connections = await conn.fetchval("""
                SELECT count(*) FROM pg_stat_activity 
                WHERE state = 'active'
            """)
            
            # ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ í™•ì¸
            db_size = await conn.fetchval("""
                SELECT pg_size_pretty(pg_database_size(current_database()))
            """)
            
            return {
                "status": "healthy",
                "metrics": {
                    "active_connections": active_connections,
                    "database_size": db_size
                }
            }
    
    async def check_microservice(self, config: dict) -> dict:
        """ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
        
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.get(f"{config['url']}/health")
            
            if response.status_code == 200:
                health_data = response.json()
                
                return {
                    "status": "healthy",
                    "version": health_data.get("version"),
                    "uptime": health_data.get("uptime"),
                    "metrics": health_data.get("metrics", {})
                }
            else:
                return {
                    "status": "unhealthy",
                    "http_status": response.status_code
                }

# FastAPI Health Endpoint
@app.get("/health")
async def health_check():
    """ì¢…í•© í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    
    health_checker = HealthChecker({
        "database": {"url": DATABASE_URL},
        "redis": {"url": REDIS_URL},
        "judgment-service": {"url": "http://judgment-service:8002"},
        "workflow-service": {"url": "http://workflow-service:8001"},
        "action-service": {"url": "http://action-service:8003"},
        "dashboard-service": {"url": "http://dashboard-service:8006"}
    })
    
    return await health_checker.check_all_services()
```

ì´ ì•„í‚¤í…ì²˜ ì„¤ê³„ëŠ” Judgify-core Ver2.0ì˜ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ê°„ íš¨ìœ¨ì ì¸ í†µì‹ ê³¼ í™•ì¥ ê°€ëŠ¥í•œ ë°ì´í„° íë¦„ì„ ë³´ì¥í•©ë‹ˆë‹¤. ê° ì„œë¹„ìŠ¤ì˜ ë…ë¦½ì„±ì„ ìœ ì§€í•˜ë©´ì„œë„ ì „ì²´ ì‹œìŠ¤í…œì˜ ì¼ê´€ì„±ê³¼ ì„±ëŠ¥ì„ ìµœì í™”í–ˆìŠµë‹ˆë‹¤.
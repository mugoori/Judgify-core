# Judgify-core Ver2.0 ë°ì´í„° íŒŒì´í”„ë¼ì¸ & ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì•„í‚¤í…ì²˜

**ë¬¸ì„œ ë²„ì „**: v2.0  
**ì‘ì„±ì¼**: 2024-08-10  
**ëŒ€ìƒ**: ë°ì´í„° ì—”ì§€ë‹ˆì–´, ì‹œìŠ¤í…œ ì•„í‚¤í…íŠ¸, DevOps ì—”ì§€ë‹ˆì–´  
**ëª©ì **: ëŒ€ìš©ëŸ‰ ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì„¤ê³„ ë° êµ¬í˜„ ê°€ì´ë“œ

## ğŸ“Š 1. ì „ì²´ ë°ì´í„° ì•„í‚¤í…ì²˜ ê°œìš”

### 1.1 í•µì‹¬ ì„¤ê³„ ì›ì¹™
- **ì‹¤ì‹œê°„ ìš°ì„ **: ëª¨ë“  ë°ì´í„°ëŠ” ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„
- **í™•ì¥ì„±**: ìˆ˜í‰ì  í™•ì¥ìœ¼ë¡œ 1000+ ë™ì‹œ íŒë‹¨ ìš”ì²­ ì²˜ë¦¬
- **ë‚´ê²°í•¨ì„±**: ë‹¨ì¼ ì¥ì• ì  ì œê±° ë° ìë™ ë³µêµ¬
- **ë°ì´í„° ì¼ê´€ì„±**: 99.5% ë°ì´í„° ì¼ê´€ì„± ë³´ì¥
- **ì„±ëŠ¥**: <5ì´ˆ íŒë‹¨ ì‘ë‹µ, <1ì´ˆ ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸

### 1.2 ë°ì´í„° í”Œë¡œìš° ë‹¤ì´ì–´ê·¸ë¨
```mermaid
graph TB
    subgraph "Data Sources"
        API[REST API]
        WEBHOOK[Webhooks]
        SCHEDULER[Scheduled Jobs]
        MCP[MCP Systems]
    end
    
    subgraph "Ingestion Layer"
        GATEWAY[API Gateway]
        BUFFER[Buffer Pool]
        VALIDATOR[Data Validator]
    end
    
    subgraph "Stream Processing"
        CDC[Change Data Capture]
        STREAMS[Redis Streams]
        PROCESSOR[Stream Processor]
    end
    
    subgraph "Storage Layer"
        PG[(PostgreSQL + pgvector)]
        REDIS[(Redis Cache)]
        TIMESERIES[(TimescaleDB)]
    end
    
    subgraph "Analytics Layer"
        ETL[ETL Pipeline]
        AGGREGATOR[Data Aggregator]
        METRICS[Metrics Collector]
    end
    
    subgraph "Output Layer"
        DASHBOARD[Real-time Dashboard]
        ALERTS[Alert System]
        API_OUT[API Responses]
    end
    
    API --> GATEWAY
    WEBHOOK --> GATEWAY
    SCHEDULER --> GATEWAY
    MCP --> GATEWAY
    
    GATEWAY --> BUFFER
    BUFFER --> VALIDATOR
    VALIDATOR --> CDC
    
    CDC --> STREAMS
    STREAMS --> PROCESSOR
    PROCESSOR --> PG
    
    PG --> ETL
    ETL --> AGGREGATOR
    AGGREGATOR --> TIMESERIES
    
    STREAMS --> DASHBOARD
    AGGREGATOR --> DASHBOARD
    METRICS --> ALERTS
    PROCESSOR --> API_OUT
```

## ğŸ”„ 2. ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì•„í‚¤í…ì²˜

### 2.1 Change Data Capture (CDC) êµ¬í˜„

#### PostgreSQL WAL ê¸°ë°˜ CDC
```python
# PostgreSQL WAL Reader êµ¬í˜„
import asyncio
import asyncpg
from typing import Dict, Any, AsyncGenerator
import json

class PostgreSQLCDCReader:
    """PostgreSQL WALì„ ì´ìš©í•œ ì‹¤ì‹œê°„ ë³€ê²½ì‚¬í•­ ê°ì§€"""
    
    def __init__(self, db_config: Dict[str, str], slot_name: str = "judgify_cdc"):
        self.db_config = db_config
        self.slot_name = slot_name
        self.connection = None
        
    async def initialize(self):
        """ë³µì œ ìŠ¬ë¡¯ ì´ˆê¸°í™”"""
        self.connection = await asyncpg.connect(**self.db_config)
        
        # ë³µì œ ìŠ¬ë¡¯ ìƒì„± (ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°)
        try:
            await self.connection.execute(f"""
                SELECT pg_create_logical_replication_slot('{self.slot_name}', 'pgoutput')
            """)
        except asyncpg.DuplicateObjectError:
            pass  # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ìŠ¬ë¡¯
    
    async def stream_changes(self) -> AsyncGenerator[Dict[str, Any], None]:
        """ì‹¤ì‹œê°„ ë³€ê²½ì‚¬í•­ ìŠ¤íŠ¸ë¦¬ë°"""
        
        while True:
            try:
                result = await self.connection.fetch(f"""
                    SELECT lsn, xid, data
                    FROM pg_logical_slot_get_changes('{self.slot_name}', NULL, NULL)
                """)
                
                for record in result:
                    change_data = self._parse_wal_record(record)
                    if change_data:
                        yield change_data
                
                await asyncio.sleep(0.1)  # 100ms ê°„ê²©ìœ¼ë¡œ í´ë§
                
            except Exception as e:
                print(f"CDC ì˜¤ë¥˜: {e}")
                await asyncio.sleep(1)
    
    def _parse_wal_record(self, record) -> Dict[str, Any]:
        """WAL ë ˆì½”ë“œ íŒŒì‹±"""
        try:
            # pgoutput í˜•ì‹ íŒŒì‹±
            data = record['data']
            
            # í…Œì´ë¸”ë³„ ì²˜ë¦¬ ë¡œì§
            if 'judgment_executions' in data:
                return {
                    'table': 'judgment_executions',
                    'operation': self._extract_operation(data),
                    'data': self._extract_data(data),
                    'timestamp': record['lsn']
                }
            
            return None
            
        except Exception as e:
            print(f"WAL íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None
    
    def _extract_operation(self, data: str) -> str:
        """ì‘ì—… íƒ€ì… ì¶”ì¶œ (INSERT, UPDATE, DELETE)"""
        if 'BEGIN' in data:
            return 'BEGIN'
        elif 'COMMIT' in data:
            return 'COMMIT'
        elif 'INSERT' in data:
            return 'INSERT'
        elif 'UPDATE' in data:
            return 'UPDATE'
        elif 'DELETE' in data:
            return 'DELETE'
        return 'UNKNOWN'
```

#### ëŒ€ì•ˆ: Debezium ê¸°ë°˜ CDC (ê¶Œì¥)
```yaml
# docker-compose.debezium.yml
version: '3.8'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

  debezium-connect:
    image: debezium/connect:2.4
    ports:
      - "8083:8083"
    environment:
      BOOTSTRAP_SERVERS: kafka:9092
      GROUP_ID: judgify-cdc
      CONFIG_STORAGE_TOPIC: debezium_configs
      OFFSET_STORAGE_TOPIC: debezium_offsets
      STATUS_STORAGE_TOPIC: debezium_status
    volumes:
      - ./debezium-config:/kafka/config
```

```json
// PostgreSQL Connector ì„¤ì •
{
  "name": "judgify-postgres-connector",
  "config": {
    "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
    "database.hostname": "postgres",
    "database.port": "5432",
    "database.user": "judgify_user",
    "database.password": "judgify_pass",
    "database.dbname": "judgify_db",
    "database.server.name": "judgify",
    "table.include.list": "public.judgment_executions,public.action_executions,public.workflows",
    "plugin.name": "pgoutput",
    "slot.name": "debezium_judgify",
    "publication.name": "dbz_publication",
    "transforms": "route",
    "transforms.route.type": "org.apache.kafka.connect.transforms.RegexRouter",
    "transforms.route.regex": "([^.]+)\\.([^.]+)\\.([^.]+)",
    "transforms.route.replacement": "$3"
  }
}
```

### 2.2 Redis Streams ê¸°ë°˜ ë©”ì‹œì§€ í

#### Redis Streams ì„¤ì • ë° ê´€ë¦¬
```python
import redis.asyncio as redis
from typing import Dict, List, Any, Optional
import json
import asyncio

class RedisStreamManager:
    """Redis Streamsì„ ì´ìš©í•œ ì‹¤ì‹œê°„ ë©”ì‹œì§€ í ê´€ë¦¬"""
    
    def __init__(self, redis_url: str):
        self.redis = redis.from_url(redis_url)
        self.streams = {
            'judgment_events': 'judgment-stream',
            'dashboard_updates': 'dashboard-stream',
            'alert_events': 'alert-stream',
            'metrics_events': 'metrics-stream'
        }
    
    async def publish_judgment_event(self, event_data: Dict[str, Any]):
        """íŒë‹¨ ì‹¤í–‰ ì´ë²¤íŠ¸ ë°œí–‰"""
        stream_name = self.streams['judgment_events']
        
        await self.redis.xadd(
            stream_name,
            event_data,
            maxlen=100000,  # ìŠ¤íŠ¸ë¦¼ í¬ê¸° ì œí•œ
            approximate=True
        )
    
    async def publish_dashboard_update(self, update_data: Dict[str, Any]):
        """ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸ ì´ë²¤íŠ¸ ë°œí–‰"""
        stream_name = self.streams['dashboard_updates']
        
        # ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œìš© ì´ë²¤íŠ¸
        await self.redis.xadd(
            stream_name,
            {
                'type': 'dashboard_update',
                'data': json.dumps(update_data),
                'timestamp': update_data.get('timestamp', ''),
                'tenant_id': update_data.get('tenant_id', ''),
                'dashboard_id': update_data.get('dashboard_id', '')
            },
            maxlen=10000,
            approximate=True
        )
        
        # WebSocket í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì¦‰ì‹œ ì•Œë¦¼
        await self._notify_websocket_clients(update_data)
    
    async def create_consumer_group(self, stream_name: str, group_name: str):
        """ì»¨ìŠˆë¨¸ ê·¸ë£¹ ìƒì„±"""
        try:
            await self.redis.xgroup_create(
                stream_name, 
                group_name, 
                id='0', 
                mkstream=True
            )
        except redis.ResponseError:
            pass  # ê·¸ë£¹ì´ ì´ë¯¸ ì¡´ì¬
    
    async def consume_stream(
        self, 
        stream_name: str, 
        group_name: str, 
        consumer_name: str,
        count: int = 10,
        block_ms: int = 1000
    ) -> List[Dict[str, Any]]:
        """ìŠ¤íŠ¸ë¦¼ì—ì„œ ë©”ì‹œì§€ ì†Œë¹„"""
        
        try:
            # ìƒˆë¡œìš´ ë©”ì‹œì§€ ì½ê¸°
            messages = await self.redis.xreadgroup(
                group_name,
                consumer_name,
                {stream_name: '>'},
                count=count,
                block=block_ms
            )
            
            processed_messages = []
            for stream, msgs in messages:
                for msg_id, fields in msgs:
                    processed_messages.append({
                        'id': msg_id.decode(),
                        'stream': stream.decode(),
                        'data': {k.decode(): v.decode() for k, v in fields.items()}
                    })
                    
                    # ë©”ì‹œì§€ ì²˜ë¦¬ ì™„ë£Œ í™•ì¸
                    await self.redis.xack(stream_name, group_name, msg_id)
            
            return processed_messages
            
        except Exception as e:
            print(f"ìŠ¤íŠ¸ë¦¼ ì†Œë¹„ ì˜¤ë¥˜: {e}")
            return []
    
    async def _notify_websocket_clients(self, data: Dict[str, Any]):
        """WebSocket í´ë¼ì´ì–¸íŠ¸ ì•Œë¦¼"""
        # WebSocket ë§¤ë‹ˆì €ì™€ ì—°ë™í•˜ì—¬ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ì „ì†¡
        notification = {
            'type': 'real_time_update',
            'payload': data
        }
        
        # Redis Pub/Subìœ¼ë¡œ WebSocket ì„œë²„ì— ì•Œë¦¼
        await self.redis.publish(
            'websocket_notifications',
            json.dumps(notification)
        )
```

#### ìŠ¤íŠ¸ë¦¼ íŒŒí‹°ì…”ë‹ ì „ëµ
```python
class StreamPartitioner:
    """ìŠ¤íŠ¸ë¦¼ íŒŒí‹°ì…”ë‹ì„ í†µí•œ í™•ì¥ì„± í™•ë³´"""
    
    def __init__(self, num_partitions: int = 8):
        self.num_partitions = num_partitions
    
    def get_partition_key(self, tenant_id: str, workflow_id: str) -> str:
        """íŒŒí‹°ì…˜ í‚¤ ìƒì„±"""
        # tenant_id ê¸°ë°˜ íŒŒí‹°ì…”ë‹ìœ¼ë¡œ í…Œë„ŒíŠ¸ë³„ ê²©ë¦¬
        partition_num = hash(tenant_id) % self.num_partitions
        return f"judgify-stream-{partition_num}"
    
    def get_dashboard_partition(self, dashboard_id: str) -> str:
        """ëŒ€ì‹œë³´ë“œë³„ íŒŒí‹°ì…”ë‹"""
        partition_num = hash(dashboard_id) % self.num_partitions
        return f"dashboard-stream-{partition_num}"
```

### 2.3 ë°±í”„ë ˆì…” ì²˜ë¦¬ ë° ë°°ì¹˜ ìµœì í™”

#### ì ì‘í˜• ë°°ì¹˜ ì²˜ë¦¬ê¸°
```python
import asyncio
from typing import List, Callable, Any
import time

class AdaptiveBatchProcessor:
    """ì ì‘í˜• ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë°±í”„ë ˆì…” ê´€ë¦¬"""
    
    def __init__(
        self,
        min_batch_size: int = 10,
        max_batch_size: int = 1000,
        max_wait_ms: int = 5000,
        target_latency_ms: int = 100
    ):
        self.min_batch_size = min_batch_size
        self.max_batch_size = max_batch_size
        self.max_wait_ms = max_wait_ms
        self.target_latency_ms = target_latency_ms
        
        # ì ì‘í˜• íŒŒë¼ë¯¸í„°
        self.current_batch_size = min_batch_size
        self.recent_latencies = []
        self.queue = asyncio.Queue()
    
    async def add_item(self, item: Any):
        """ì²˜ë¦¬ ëŒ€ìƒ ì•„ì´í…œ ì¶”ê°€"""
        await self.queue.put(item)
    
    async def process_batches(self, processor: Callable[[List[Any]], Any]):
        """ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰"""
        while True:
            batch = []
            start_wait = time.time()
            
            # ë°°ì¹˜ ìˆ˜ì§‘
            while len(batch) < self.current_batch_size:
                try:
                    # íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ ì œí•œ
                    remaining_wait = self.max_wait_ms - (time.time() - start_wait) * 1000
                    if remaining_wait <= 0:
                        break
                    
                    item = await asyncio.wait_for(
                        self.queue.get(),
                        timeout=remaining_wait / 1000
                    )
                    batch.append(item)
                    
                except asyncio.TimeoutError:
                    break
            
            if not batch:
                continue
            
            # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
            start_process = time.time()
            try:
                await processor(batch)
                processing_time = (time.time() - start_process) * 1000
                
                # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                self._update_adaptive_parameters(processing_time, len(batch))
                
            except Exception as e:
                print(f"ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                # ì‹¤íŒ¨í•œ í•­ëª©ë“¤ì„ ë‹¤ì‹œ íì— ì¶”ê°€
                for item in batch:
                    await self.queue.put(item)
    
    def _update_adaptive_parameters(self, latency_ms: float, batch_size: int):
        """ì„±ëŠ¥ ê¸°ë°˜ íŒŒë¼ë¯¸í„° ì¡°ì •"""
        self.recent_latencies.append(latency_ms)
        
        # ìµœê·¼ 10ê°œ ì§€ì—°ì‹œê°„ í‰ê· 
        if len(self.recent_latencies) > 10:
            self.recent_latencies.pop(0)
        
        avg_latency = sum(self.recent_latencies) / len(self.recent_latencies)
        
        # ì ì‘í˜• ë°°ì¹˜ í¬ê¸° ì¡°ì •
        if avg_latency > self.target_latency_ms * 1.5:
            # ì§€ì—°ì‹œê°„ì´ ë†’ìœ¼ë©´ ë°°ì¹˜ í¬ê¸° ê°ì†Œ
            self.current_batch_size = max(
                self.min_batch_size,
                int(self.current_batch_size * 0.8)
            )
        elif avg_latency < self.target_latency_ms * 0.5:
            # ì§€ì—°ì‹œê°„ì´ ë‚®ìœ¼ë©´ ë°°ì¹˜ í¬ê¸° ì¦ê°€
            self.current_batch_size = min(
                self.max_batch_size,
                int(self.current_batch_size * 1.2)
            )
```

## ğŸ“ˆ 3. ETL íŒŒì´í”„ë¼ì¸ ì„¤ê³„

### 3.1 ì‹¤ì‹œê°„ ë°ì´í„° ë³€í™˜ íŒŒì´í”„ë¼ì¸

#### íŒë‹¨ ì‹¤í–‰ ë°ì´í„° ETL
```python
import asyncio
from typing import Dict, Any, List
from dataclasses import dataclass
from datetime import datetime, timedelta
import asyncpg

@dataclass
class JudgmentMetrics:
    """íŒë‹¨ ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤"""
    tenant_id: str
    workflow_id: str
    hour: datetime
    total_executions: int
    success_count: int
    rule_method_count: int
    llm_method_count: int
    hybrid_method_count: int
    avg_execution_time_ms: float
    avg_confidence_score: float

class RealTimeETLPipeline:
    """ì‹¤ì‹œê°„ ETL íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, db_pool: asyncpg.Pool, stream_manager: RedisStreamManager):
        self.db_pool = db_pool
        self.stream_manager = stream_manager
        self.aggregation_window = timedelta(minutes=5)  # 5ë¶„ ìœˆë„ìš°
    
    async def start_etl_pipeline(self):
        """ETL íŒŒì´í”„ë¼ì¸ ì‹œì‘"""
        await asyncio.gather(
            self._process_judgment_events(),
            self._generate_real_time_metrics(),
            self._cleanup_old_data()
        )
    
    async def _process_judgment_events(self):
        """íŒë‹¨ ì‹¤í–‰ ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        group_name = "etl-processors"
        consumer_name = "etl-consumer-1"
        
        await self.stream_manager.create_consumer_group(
            self.stream_manager.streams['judgment_events'],
            group_name
        )
        
        processor = AdaptiveBatchProcessor(
            min_batch_size=5,
            max_batch_size=100,
            target_latency_ms=200
        )
        
        # ì´ë²¤íŠ¸ ì²˜ë¦¬ íƒœìŠ¤í¬ ì‹œì‘
        asyncio.create_task(processor.process_batches(self._process_judgment_batch))
        
        while True:
            messages = await self.stream_manager.consume_stream(
                self.stream_manager.streams['judgment_events'],
                group_name,
                consumer_name
            )
            
            for message in messages:
                await processor.add_item(message)
    
    async def _process_judgment_batch(self, batch: List[Dict[str, Any]]):
        """íŒë‹¨ ì´ë²¤íŠ¸ ë°°ì¹˜ ì²˜ë¦¬"""
        
        # ì§‘ê³„ ë°ì´í„° ì¤€ë¹„
        aggregations = {}
        
        for message in batch:
            data = message['data']
            tenant_id = data.get('tenant_id')
            workflow_id = data.get('workflow_id')
            
            # ì‹œê°„ë³„ ì§‘ê³„ í‚¤ ìƒì„±
            timestamp = datetime.fromisoformat(data.get('created_at'))
            hour_key = timestamp.replace(minute=0, second=0, microsecond=0)
            agg_key = (tenant_id, workflow_id, hour_key)
            
            if agg_key not in aggregations:
                aggregations[agg_key] = {
                    'total_executions': 0,
                    'success_count': 0,
                    'method_counts': {'rule': 0, 'llm': 0, 'hybrid': 0},
                    'execution_times': [],
                    'confidence_scores': []
                }
            
            agg = aggregations[agg_key]
            agg['total_executions'] += 1
            
            if data.get('status') == 'success':
                agg['success_count'] += 1
            
            method = data.get('method_used', 'unknown')
            if method in agg['method_counts']:
                agg['method_counts'][method] += 1
            
            if data.get('execution_time_ms'):
                agg['execution_times'].append(float(data['execution_time_ms']))
            
            if data.get('confidence_score'):
                agg['confidence_scores'].append(float(data['confidence_score']))
        
        # ë°ì´í„°ë² ì´ìŠ¤ì— ì§‘ê³„ ê²°ê³¼ ì €ì¥
        await self._save_aggregations(aggregations)
        
        # ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸ ì´ë²¤íŠ¸ ë°œí–‰
        await self._publish_dashboard_updates(aggregations)
    
    async def _save_aggregations(self, aggregations: Dict):
        """ì§‘ê³„ ê²°ê³¼ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥"""
        
        async with self.db_pool.acquire() as conn:
            for (tenant_id, workflow_id, hour), agg_data in aggregations.items():
                
                avg_exec_time = (
                    sum(agg_data['execution_times']) / len(agg_data['execution_times'])
                    if agg_data['execution_times'] else 0
                )
                
                avg_confidence = (
                    sum(agg_data['confidence_scores']) / len(agg_data['confidence_scores'])
                    if agg_data['confidence_scores'] else 0
                )
                
                # UPSERT ì¿¼ë¦¬ë¡œ ì§‘ê³„ ë°ì´í„° ì €ì¥
                await conn.execute("""
                    INSERT INTO judgment_metrics_hourly (
                        tenant_id, workflow_id, hour, total_executions,
                        success_count, rule_method_count, llm_method_count,
                        hybrid_method_count, avg_execution_time_ms, avg_confidence_score
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                    ON CONFLICT (tenant_id, workflow_id, hour)
                    DO UPDATE SET
                        total_executions = judgment_metrics_hourly.total_executions + EXCLUDED.total_executions,
                        success_count = judgment_metrics_hourly.success_count + EXCLUDED.success_count,
                        rule_method_count = judgment_metrics_hourly.rule_method_count + EXCLUDED.rule_method_count,
                        llm_method_count = judgment_metrics_hourly.llm_method_count + EXCLUDED.llm_method_count,
                        hybrid_method_count = judgment_metrics_hourly.hybrid_method_count + EXCLUDED.hybrid_method_count,
                        avg_execution_time_ms = (
                            judgment_metrics_hourly.avg_execution_time_ms * judgment_metrics_hourly.total_executions +
                            EXCLUDED.avg_execution_time_ms * EXCLUDED.total_executions
                        ) / (judgment_metrics_hourly.total_executions + EXCLUDED.total_executions),
                        avg_confidence_score = (
                            judgment_metrics_hourly.avg_confidence_score * judgment_metrics_hourly.total_executions +
                            EXCLUDED.avg_confidence_score * EXCLUDED.total_executions
                        ) / (judgment_metrics_hourly.total_executions + EXCLUDED.total_executions)
                """, 
                    tenant_id, workflow_id, hour,
                    agg_data['total_executions'], agg_data['success_count'],
                    agg_data['method_counts']['rule'], agg_data['method_counts']['llm'],
                    agg_data['method_counts']['hybrid'], avg_exec_time, avg_confidence
                )
```

### 3.2 ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬

#### ë°ì´í„° ê²€ì¦ ë° ì •ì œ ì‹œìŠ¤í…œ
```python
from pydantic import BaseModel, validator
from typing import Optional, Any
import asyncio

class DataQualityRules:
    """ë°ì´í„° í’ˆì§ˆ ê·œì¹™ ì •ì˜"""
    
    @staticmethod
    def validate_judgment_data(data: Dict[str, Any]) -> tuple[bool, List[str]]:
        """íŒë‹¨ ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
        errors = []
        
        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
        required_fields = ['tenant_id', 'workflow_id', 'input_data', 'final_result']
        for field in required_fields:
            if not data.get(field):
                errors.append(f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}")
        
        # ë°ì´í„° íƒ€ì… ê²€ì¦
        if data.get('confidence_score'):
            confidence = float(data['confidence_score'])
            if not 0 <= confidence <= 1:
                errors.append("ì‹ ë¢°ë„ ì ìˆ˜ëŠ” 0-1 ì‚¬ì´ì—¬ì•¼ í•¨")
        
        # ì‹¤í–‰ ì‹œê°„ ê²€ì¦
        if data.get('execution_time_ms'):
            exec_time = int(data['execution_time_ms'])
            if exec_time < 0 or exec_time > 30000:  # 30ì´ˆ ì´ˆê³¼
                errors.append("ë¹„ì •ìƒì ì¸ ì‹¤í–‰ ì‹œê°„")
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ê²€ì¦
        method_used = data.get('method_used')
        if method_used not in ['rule', 'llm', 'hybrid']:
            errors.append(f"ì˜ëª»ëœ íŒë‹¨ ë°©ë²•: {method_used}")
        
        return len(errors) == 0, errors
    
    @staticmethod
    def sanitize_input_data(data: Dict[str, Any]) -> Dict[str, Any]:
        """ì…ë ¥ ë°ì´í„° ì •ì œ"""
        sanitized = {}
        
        for key, value in data.items():
            # íŠ¹ìˆ˜ ë¬¸ì ì œê±°
            if isinstance(value, str):
                # SQL ì¸ì ì…˜ ë°©ì§€
                sanitized[key] = value.replace("'", "''").replace(";", "")
            elif isinstance(value, (int, float)):
                # ìˆ«ì ë²”ìœ„ ê²€ì¦
                sanitized[key] = max(min(value, 1e6), -1e6)
            else:
                sanitized[key] = value
        
        return sanitized

class DataQualityMonitor:
    """ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self, redis_client):
        self.redis = redis_client
        self.quality_metrics = {
            'total_records': 0,
            'valid_records': 0,
            'invalid_records': 0,
            'error_types': {}
        }
    
    async def monitor_data_quality(self, data_batch: List[Dict[str, Any]]):
        """ë°°ì¹˜ ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§"""
        
        for record in data_batch:
            self.quality_metrics['total_records'] += 1
            
            is_valid, errors = DataQualityRules.validate_judgment_data(record)
            
            if is_valid:
                self.quality_metrics['valid_records'] += 1
            else:
                self.quality_metrics['invalid_records'] += 1
                
                # ì˜¤ë¥˜ íƒ€ì…ë³„ ì§‘ê³„
                for error in errors:
                    error_type = error.split(':')[0]
                    self.quality_metrics['error_types'][error_type] = \
                        self.quality_metrics['error_types'].get(error_type, 0) + 1
        
        # í’ˆì§ˆ ë©”íŠ¸ë¦­ ì €ì¥
        await self._save_quality_metrics()
    
    async def _save_quality_metrics(self):
        """í’ˆì§ˆ ë©”íŠ¸ë¦­ Redisì— ì €ì¥"""
        await self.redis.hset(
            'data_quality_metrics',
            mapping=self.quality_metrics
        )
        
        # í’ˆì§ˆ ì €í•˜ ì•Œë¦¼ (ìœ íš¨ ë¹„ìœ¨ < 95%)
        if self.quality_metrics['total_records'] > 0:
            valid_ratio = self.quality_metrics['valid_records'] / self.quality_metrics['total_records']
            if valid_ratio < 0.95:
                await self._trigger_quality_alert(valid_ratio)
    
    async def _trigger_quality_alert(self, valid_ratio: float):
        """ë°ì´í„° í’ˆì§ˆ ì•Œë¦¼ ë°œì†¡"""
        alert_data = {
            'type': 'data_quality_alert',
            'severity': 'warning' if valid_ratio > 0.90 else 'critical',
            'message': f"ë°ì´í„° í’ˆì§ˆ ì €í•˜: ìœ íš¨ ë¹„ìœ¨ {valid_ratio:.2%}",
            'metrics': self.quality_metrics
        }
        
        await self.redis.publish('alert_channel', json.dumps(alert_data))
```

## â±ï¸ 4. ì‹œê³„ì—´ ë°ì´í„° ìµœì í™” (TimescaleDB)

### 4.1 TimescaleDB í™•ì¥ ì„¤ì¹˜ ë° ì„¤ì •
```sql
-- TimescaleDB í™•ì¥ ì„¤ì¹˜
CREATE EXTENSION IF NOT EXISTS timescaledb;

-- ì‹œê³„ì—´ í…Œì´ë¸” ìƒì„±
CREATE TABLE judgment_metrics_timeseries (
    time TIMESTAMPTZ NOT NULL,
    tenant_id UUID NOT NULL,
    workflow_id UUID NOT NULL,
    metric_name VARCHAR(100) NOT NULL,
    metric_value DOUBLE PRECISION NOT NULL,
    dimensions JSONB DEFAULT '{}'
);

-- Hypertable ìƒì„± (ì‹œê³„ì—´ íŒŒí‹°ì…”ë‹)
SELECT create_hypertable('judgment_metrics_timeseries', 'time');

-- ì••ì¶• ì •ì±… ì„¤ì • (7ì¼ ì´í›„ ë°ì´í„° ì••ì¶•)
ALTER TABLE judgment_metrics_timeseries SET (
    timescaledb.compress,
    timescaledb.compress_segmentby = 'tenant_id, workflow_id, metric_name'
);

SELECT add_compression_policy('judgment_metrics_timeseries', INTERVAL '7 days');

-- ë°ì´í„° ë³´ì¡´ ì •ì±… (1ë…„ í›„ ì‚­ì œ)
SELECT add_retention_policy('judgment_metrics_timeseries', INTERVAL '1 year');

-- ì—°ì† ì§‘ê³„ ë·° ìƒì„± (ì‹¤ì‹œê°„ ì§‘ê³„)
CREATE MATERIALIZED VIEW judgment_metrics_hourly_cagg
WITH (timescaledb.continuous) AS
SELECT 
    time_bucket(INTERVAL '1 hour', time) AS bucket,
    tenant_id,
    workflow_id,
    metric_name,
    AVG(metric_value) as avg_value,
    MAX(metric_value) as max_value,
    MIN(metric_value) as min_value,
    COUNT(*) as count
FROM judgment_metrics_timeseries
GROUP BY bucket, tenant_id, workflow_id, metric_name;

-- ì‹¤ì‹œê°„ ì§‘ê³„ ì •ì±… ì¶”ê°€
SELECT add_continuous_aggregate_policy('judgment_metrics_hourly_cagg',
    start_offset => INTERVAL '1 hour',
    end_offset => INTERVAL '10 minutes',
    schedule_interval => INTERVAL '10 minutes');
```

### 4.2 TimescaleDB ìµœì í™”ëœ ë°ì´í„° ì €ì¥
```python
class TimescaleDBManager:
    """TimescaleDB ì‹œê³„ì—´ ë°ì´í„° ê´€ë¦¬"""
    
    def __init__(self, db_pool: asyncpg.Pool):
        self.db_pool = db_pool
    
    async def insert_metrics_batch(self, metrics: List[Dict[str, Any]]):
        """ë°°ì¹˜ ë°©ì‹ìœ¼ë¡œ ë©”íŠ¸ë¦­ ë°ì´í„° ì‚½ì…"""
        
        async with self.db_pool.acquire() as conn:
            # COPYë¥¼ ì´ìš©í•œ ëŒ€ëŸ‰ ì‚½ì… (ìµœê³  ì„±ëŠ¥)
            data_rows = []
            for metric in metrics:
                row = (
                    metric['timestamp'],
                    metric['tenant_id'],
                    metric['workflow_id'],
                    metric['metric_name'],
                    metric['metric_value'],
                    json.dumps(metric.get('dimensions', {}))
                )
                data_rows.append(row)
            
            await conn.copy_records_to_table(
                'judgment_metrics_timeseries',
                records=data_rows,
                columns=['time', 'tenant_id', 'workflow_id', 'metric_name', 'metric_value', 'dimensions']
            )
    
    async def get_real_time_metrics(
        self, 
        tenant_id: str, 
        timeframe: str = '1h',
        workflow_ids: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        
        time_filter = {
            '1h': 'NOW() - INTERVAL \'1 hour\'',
            '6h': 'NOW() - INTERVAL \'6 hours\'',
            '24h': 'NOW() - INTERVAL \'24 hours\'',
            '7d': 'NOW() - INTERVAL \'7 days\''
        }.get(timeframe, 'NOW() - INTERVAL \'1 hour\'')
        
        workflow_filter = ''
        params = [tenant_id]
        
        if workflow_ids:
            workflow_filter = 'AND workflow_id = ANY($2)'
            params.append(workflow_ids)
        
        query = f"""
        SELECT 
            time_bucket(INTERVAL '5 minutes', time) AS bucket,
            metric_name,
            AVG(metric_value) as avg_value,
            MAX(metric_value) as max_value,
            MIN(metric_value) as min_value,
            COUNT(*) as count
        FROM judgment_metrics_timeseries
        WHERE tenant_id = $1
        AND time >= {time_filter}
        {workflow_filter}
        GROUP BY bucket, metric_name
        ORDER BY bucket DESC, metric_name
        """
        
        async with self.db_pool.acquire() as conn:
            results = await conn.fetch(query, *params)
            
            # ê²°ê³¼ êµ¬ì¡°í™”
            metrics_data = {}
            for row in results:
                metric_name = row['metric_name']
                if metric_name not in metrics_data:
                    metrics_data[metric_name] = []
                
                metrics_data[metric_name].append({
                    'timestamp': row['bucket'].isoformat(),
                    'avg': float(row['avg_value']),
                    'max': float(row['max_value']),
                    'min': float(row['min_value']),
                    'count': int(row['count'])
                })
            
            return metrics_data
    
    async def get_aggregated_dashboard_data(self, tenant_id: str) -> Dict[str, Any]:
        """ëŒ€ì‹œë³´ë“œìš© ì§‘ê³„ ë°ì´í„° ì¡°íšŒ"""
        
        query = """
        SELECT 
            metric_name,
            avg_value,
            max_value,
            min_value,
            count
        FROM judgment_metrics_hourly_cagg
        WHERE tenant_id = $1
        AND bucket >= NOW() - INTERVAL '24 hours'
        ORDER BY bucket DESC
        """
        
        async with self.db_pool.acquire() as conn:
            results = await conn.fetch(query, tenant_id)
            
            return {
                'summary': self._calculate_summary_stats(results),
                'trends': self._calculate_trends(results),
                'alerts': await self._check_metric_alerts(tenant_id)
            }
    
    def _calculate_summary_stats(self, results):
        """ìš”ì•½ í†µê³„ ê³„ì‚°"""
        if not results:
            return {}
        
        total_executions = sum(row['count'] for row in results if row['metric_name'] == 'execution_count')
        avg_response_time = sum(row['avg_value'] for row in results if row['metric_name'] == 'execution_time_ms') / len(results)
        
        return {
            'total_executions': total_executions,
            'avg_response_time_ms': round(avg_response_time, 2),
            'success_rate': self._calculate_success_rate(results)
        }
```

## ğŸ“¡ 5. ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°

### 5.1 WebSocket ê¸°ë°˜ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
```python
import asyncio
import websockets
import json
from typing import Set, Dict, Any
from fastapi import WebSocket

class DashboardWebSocketManager:
    """ëŒ€ì‹œë³´ë“œ WebSocket ì—°ê²° ê´€ë¦¬"""
    
    def __init__(self, redis_manager: RedisStreamManager):
        self.active_connections: Dict[str, Set[WebSocket]] = {}
        self.redis_manager = redis_manager
        self.subscription_task = None
    
    async def connect(self, websocket: WebSocket, dashboard_id: str, tenant_id: str):
        """WebSocket ì—°ê²° ë“±ë¡"""
        await websocket.accept()
        
        connection_key = f"{tenant_id}:{dashboard_id}"
        if connection_key not in self.active_connections:
            self.active_connections[connection_key] = set()
        
        self.active_connections[connection_key].add(websocket)
        
        # ì´ˆê¸° ë°ì´í„° ì „ì†¡
        initial_data = await self._get_initial_dashboard_data(dashboard_id, tenant_id)
        await websocket.send_text(json.dumps(initial_data))
    
    async def disconnect(self, websocket: WebSocket, dashboard_id: str, tenant_id: str):
        """WebSocket ì—°ê²° í•´ì œ"""
        connection_key = f"{tenant_id}:{dashboard_id}"
        if connection_key in self.active_connections:
            self.active_connections[connection_key].discard(websocket)
            
            if not self.active_connections[connection_key]:
                del self.active_connections[connection_key]
    
    async def start_real_time_updates(self):
        """ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ìŠ¤íŠ¸ë¦¼ ì‹œì‘"""
        self.subscription_task = asyncio.create_task(self._process_dashboard_updates())
    
    async def _process_dashboard_updates(self):
        """ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸ ì²˜ë¦¬"""
        redis_client = self.redis_manager.redis
        
        # Redis Pub/Sub êµ¬ë…
        pubsub = redis_client.pubsub()
        await pubsub.subscribe('websocket_notifications')
        
        async for message in pubsub.listen():
            if message['type'] == 'message':
                try:
                    notification = json.loads(message['data'])
                    await self._broadcast_update(notification)
                except Exception as e:
                    print(f"WebSocket ì•Œë¦¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    
    async def _broadcast_update(self, notification: Dict[str, Any]):
        """ê´€ë ¨ í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì—…ë°ì´íŠ¸ ì „ì†¡"""
        
        payload = notification.get('payload', {})
        tenant_id = payload.get('tenant_id', '')
        dashboard_id = payload.get('dashboard_id', '')
        
        connection_key = f"{tenant_id}:{dashboard_id}"
        
        if connection_key in self.active_connections:
            disconnected_clients = set()
            
            for websocket in self.active_connections[connection_key]:
                try:
                    await websocket.send_text(json.dumps(notification))
                except Exception:
                    # ì—°ê²°ì´ ëŠì–´ì§„ í´ë¼ì´ì–¸íŠ¸ ì¶”ì 
                    disconnected_clients.add(websocket)
            
            # ëŠì–´ì§„ ì—°ê²° ì •ë¦¬
            for websocket in disconnected_clients:
                self.active_connections[connection_key].discard(websocket)
    
    async def _get_initial_dashboard_data(self, dashboard_id: str, tenant_id: str) -> Dict[str, Any]:
        """ì´ˆê¸° ëŒ€ì‹œë³´ë“œ ë°ì´í„° ì¡°íšŒ"""
        
        # ì—¬ê¸°ì„œëŠ” TimescaleDB ë§¤ë‹ˆì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ˆê¸° ë°ì´í„° ì¡°íšŒ
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì˜ì¡´ì„± ì£¼ì…ì„ í†µí•´ TimescaleDB ë§¤ë‹ˆì € ì ‘ê·¼
        
        return {
            'type': 'initial_data',
            'dashboard_id': dashboard_id,
            'data': {
                'summary': {
                    'total_executions': 0,
                    'success_rate': 0.0,
                    'avg_response_time': 0
                },
                'charts': []
            },
            'timestamp': datetime.now().isoformat()
        }
```

### 5.2 ë°ì´í„° í”„ë¦¬-ì–´ê·¸ë¦¬ê²Œì´ì…˜ (Pre-aggregation)
```python
class DashboardDataPreAggregator:
    """ëŒ€ì‹œë³´ë“œ ë°ì´í„° ì‚¬ì „ ì§‘ê³„"""
    
    def __init__(self, timescale_manager: TimescaleDBManager, redis_client):
        self.timescale = timescale_manager
        self.redis = redis_client
        self.aggregation_cache = {}
    
    async def start_pre_aggregation(self):
        """ì‚¬ì „ ì§‘ê³„ í”„ë¡œì„¸ìŠ¤ ì‹œì‘"""
        while True:
            try:
                await self._aggregate_dashboard_data()
                await asyncio.sleep(30)  # 30ì´ˆë§ˆë‹¤ ì§‘ê³„
            except Exception as e:
                print(f"ì‚¬ì „ ì§‘ê³„ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(5)
    
    async def _aggregate_dashboard_data(self):
        """ëŒ€ì‹œë³´ë“œë³„ ë°ì´í„° ì‚¬ì „ ì§‘ê³„"""
        
        # í™œì„± ëŒ€ì‹œë³´ë“œ ëª©ë¡ ì¡°íšŒ
        active_dashboards = await self._get_active_dashboards()
        
        for dashboard in active_dashboards:
            tenant_id = dashboard['tenant_id']
            dashboard_id = dashboard['dashboard_id']
            config = dashboard['config']
            
            # ëŒ€ì‹œë³´ë“œ ì„¤ì •ì— ë”°ë¥¸ ë°ì´í„° ì§‘ê³„
            aggregated_data = await self._aggregate_for_dashboard(tenant_id, config)
            
            # Redisì— ìºì‹œ
            cache_key = f"dashboard_cache:{tenant_id}:{dashboard_id}"
            await self.redis.setex(
                cache_key,
                300,  # 5ë¶„ ìºì‹œ
                json.dumps(aggregated_data)
            )
            
            # WebSocketìœ¼ë¡œ ì—…ë°ì´íŠ¸ ì „ì†¡
            await self._send_dashboard_update(tenant_id, dashboard_id, aggregated_data)
    
    async def _aggregate_for_dashboard(self, tenant_id: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """ëŒ€ì‹œë³´ë“œ ì„¤ì •ì— ë”°ë¥¸ ì§‘ê³„"""
        
        charts_data = {}
        
        for chart_config in config.get('charts', []):
            chart_type = chart_config['type']
            timeframe = chart_config.get('timeframe', '1h')
            
            if chart_type == 'execution_count_chart':
                data = await self._get_execution_count_data(tenant_id, timeframe)
                charts_data['execution_count'] = data
            
            elif chart_type == 'response_time_chart':
                data = await self._get_response_time_data(tenant_id, timeframe)
                charts_data['response_time'] = data
            
            elif chart_type == 'success_rate_chart':
                data = await self._get_success_rate_data(tenant_id, timeframe)
                charts_data['success_rate'] = data
        
        return {
            'timestamp': datetime.now().isoformat(),
            'charts': charts_data,
            'summary': await self._get_summary_data(tenant_id)
        }
    
    async def _get_execution_count_data(self, tenant_id: str, timeframe: str) -> List[Dict[str, Any]]:
        """ì‹¤í–‰ íšŸìˆ˜ ì°¨íŠ¸ ë°ì´í„°"""
        
        bucket_size = {
            '1h': '5 minutes',
            '6h': '30 minutes', 
            '24h': '1 hour',
            '7d': '6 hours'
        }.get(timeframe, '5 minutes')
        
        query = f"""
        SELECT 
            time_bucket(INTERVAL '{bucket_size}', time) AS bucket,
            SUM(metric_value) as total_executions
        FROM judgment_metrics_timeseries
        WHERE tenant_id = $1
        AND metric_name = 'execution_count'
        AND time >= NOW() - INTERVAL '{timeframe}'
        GROUP BY bucket
        ORDER BY bucket
        """
        
        async with self.timescale.db_pool.acquire() as conn:
            results = await conn.fetch(query, tenant_id)
            
            return [
                {
                    'timestamp': row['bucket'].isoformat(),
                    'value': int(row['total_executions'])
                }
                for row in results
            ]
```

## ğŸš¨ 6. ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ì‹œìŠ¤í…œ

### 6.1 ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
```python
import time
from typing import Dict, Any
import asyncio

class PerformanceMonitor:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self, redis_client, timescale_manager: TimescaleDBManager):
        self.redis = redis_client
        self.timescale = timescale_manager
        self.metrics_buffer = []
        self.buffer_size = 1000
        
    async def record_judgment_performance(
        self, 
        tenant_id: str, 
        workflow_id: str,
        execution_time_ms: int,
        method_used: str,
        success: bool
    ):
        """íŒë‹¨ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        
        timestamp = datetime.now()
        
        metrics = [
            {
                'timestamp': timestamp,
                'tenant_id': tenant_id,
                'workflow_id': workflow_id,
                'metric_name': 'execution_time_ms',
                'metric_value': execution_time_ms,
                'dimensions': {'method': method_used, 'success': success}
            },
            {
                'timestamp': timestamp,
                'tenant_id': tenant_id, 
                'workflow_id': workflow_id,
                'metric_name': 'execution_count',
                'metric_value': 1,
                'dimensions': {'method': method_used, 'success': success}
            }
        ]
        
        self.metrics_buffer.extend(metrics)
        
        # ë²„í¼ê°€ ê°€ë“ ì°¨ë©´ ë°°ì¹˜ ì €ì¥
        if len(self.metrics_buffer) >= self.buffer_size:
            await self._flush_metrics_buffer()
    
    async def _flush_metrics_buffer(self):
        """ë©”íŠ¸ë¦­ ë²„í¼ í”ŒëŸ¬ì‹œ"""
        if not self.metrics_buffer:
            return
        
        await self.timescale.insert_metrics_batch(self.metrics_buffer)
        self.metrics_buffer.clear()
    
    async def check_performance_alerts(self):
        """ì„±ëŠ¥ ì•Œë¦¼ ì²´í¬"""
        while True:
            try:
                # ì‘ë‹µ ì‹œê°„ ì„ê³„ê°’ ì²´í¬
                avg_response_time = await self._get_avg_response_time('5m')
                if avg_response_time > 5000:  # 5ì´ˆ ì´ˆê³¼
                    await self._send_performance_alert(
                        'high_response_time',
                        f'í‰ê·  ì‘ë‹µ ì‹œê°„ì´ {avg_response_time/1000:.1f}ì´ˆë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.',
                        'critical'
                    )
                
                # ì²˜ë¦¬ëŸ‰ ì„ê³„ê°’ ì²´í¬
                current_throughput = await self._get_current_throughput('1m')
                if current_throughput < 10:  # ë¶„ë‹¹ 10ê±´ ë¯¸ë§Œ
                    await self._send_performance_alert(
                        'low_throughput',
                        f'í˜„ì¬ ì²˜ë¦¬ëŸ‰ì´ ë¶„ë‹¹ {current_throughput}ê±´ìœ¼ë¡œ ë‚®ìŠµë‹ˆë‹¤.',
                        'warning'
                    )
                
                await asyncio.sleep(60)  # 1ë¶„ë§ˆë‹¤ ì²´í¬
                
            except Exception as e:
                print(f"ì„±ëŠ¥ ì•Œë¦¼ ì²´í¬ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(30)
    
    async def _send_performance_alert(self, alert_type: str, message: str, severity: str):
        """ì„±ëŠ¥ ì•Œë¦¼ ë°œì†¡"""
        alert = {
            'type': alert_type,
            'message': message,
            'severity': severity,
            'timestamp': datetime.now().isoformat(),
            'metrics': await self._get_current_metrics()
        }
        
        await self.redis.publish('performance_alerts', json.dumps(alert))
```

### 6.2 ìë™ ìŠ¤ì¼€ì¼ë§ íŠ¸ë¦¬ê±°
```python
class AutoScalingController:
    """ìë™ ìŠ¤ì¼€ì¼ë§ ì œì–´"""
    
    def __init__(self, performance_monitor: PerformanceMonitor):
        self.monitor = performance_monitor
        self.scaling_rules = {
            'cpu_threshold': 80,      # CPU 80% ì´ˆê³¼ì‹œ ìŠ¤ì¼€ì¼ ì•„ì›ƒ
            'memory_threshold': 85,   # ë©”ëª¨ë¦¬ 85% ì´ˆê³¼ì‹œ ìŠ¤ì¼€ì¼ ì•„ì›ƒ
            'response_time_threshold': 3000,  # 3ì´ˆ ì´ˆê³¼ì‹œ ìŠ¤ì¼€ì¼ ì•„ì›ƒ
            'queue_size_threshold': 1000      # í í¬ê¸° 1000 ì´ˆê³¼ì‹œ ìŠ¤ì¼€ì¼ ì•„ì›ƒ
        }
    
    async def monitor_scaling_triggers(self):
        """ìŠ¤ì¼€ì¼ë§ íŠ¸ë¦¬ê±° ëª¨ë‹ˆí„°ë§"""
        while True:
            try:
                metrics = await self._get_system_metrics()
                scaling_decision = await self._evaluate_scaling_need(metrics)
                
                if scaling_decision['action'] == 'scale_out':
                    await self._trigger_scale_out(scaling_decision['service'])
                elif scaling_decision['action'] == 'scale_in':
                    await self._trigger_scale_in(scaling_decision['service'])
                
                await asyncio.sleep(30)  # 30ì´ˆë§ˆë‹¤ í‰ê°€
                
            except Exception as e:
                print(f"ìŠ¤ì¼€ì¼ë§ ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(60)
    
    async def _evaluate_scaling_need(self, metrics: Dict[str, Any]) -> Dict[str, str]:
        """ìŠ¤ì¼€ì¼ë§ í•„ìš”ì„± í‰ê°€"""
        
        # CPU/ë©”ëª¨ë¦¬ ê¸°ë°˜ í‰ê°€
        if (metrics.get('cpu_usage', 0) > self.scaling_rules['cpu_threshold'] or
            metrics.get('memory_usage', 0) > self.scaling_rules['memory_threshold']):
            return {'action': 'scale_out', 'service': 'judgment_service', 'reason': 'high_resource_usage'}
        
        # ì‘ë‹µ ì‹œê°„ ê¸°ë°˜ í‰ê°€
        if metrics.get('avg_response_time', 0) > self.scaling_rules['response_time_threshold']:
            return {'action': 'scale_out', 'service': 'judgment_service', 'reason': 'high_response_time'}
        
        # í í¬ê¸° ê¸°ë°˜ í‰ê°€  
        if metrics.get('queue_size', 0) > self.scaling_rules['queue_size_threshold']:
            return {'action': 'scale_out', 'service': 'judgment_service', 'reason': 'high_queue_size'}
        
        # ìŠ¤ì¼€ì¼ ì¸ ì¡°ê±´ (ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ë‚®ìŒ)
        if (metrics.get('cpu_usage', 0) < 30 and 
            metrics.get('memory_usage', 0) < 40 and
            metrics.get('avg_response_time', 0) < 1000):
            return {'action': 'scale_in', 'service': 'judgment_service', 'reason': 'low_resource_usage'}
        
        return {'action': 'no_action', 'service': None}
    
    async def _trigger_scale_out(self, service: str):
        """ìŠ¤ì¼€ì¼ ì•„ì›ƒ ì‹¤í–‰"""
        scaling_event = {
            'action': 'scale_out',
            'service': service,
            'timestamp': datetime.now().isoformat(),
            'target_replicas': await self._calculate_target_replicas(service, 'out')
        }
        
        # Kubernetes API í˜¸ì¶œ ë˜ëŠ” Docker Swarm ëª…ë ¹ ì‹¤í–‰
        await self._execute_scaling_command(scaling_event)
        
        # ìŠ¤ì¼€ì¼ë§ ì´ë²¤íŠ¸ ë¡œê·¸
        await self.monitor.redis.publish('scaling_events', json.dumps(scaling_event))
```

## ğŸ”§ 7. ë°°í¬ ë° ìš´ì˜ ê°€ì´ë“œ

### 7.1 Docker Compose ì„¤ì •
```yaml
# docker-compose.streaming.yml
version: '3.8'

services:
  # TimescaleDB
  timescaledb:
    image: timescale/timescaledb:latest-pg15
    environment:
      POSTGRES_DB: judgify_timeseries
      POSTGRES_USER: timescale_user
      POSTGRES_PASSWORD: ${TIMESCALE_PASSWORD}
    ports:
      - "5433:5432"
    volumes:
      - timescale_data:/var/lib/postgresql/data
      - ./timescale-init:/docker-entrypoint-initdb.d
    command: postgres -c shared_preload_libraries=timescaledb

  # Redis Cluster (3 nodes)
  redis-node-1:
    image: redis:7-alpine
    command: redis-server --port 7001 --cluster-enabled yes --cluster-config-file nodes.conf
    ports:
      - "7001:7001"
    volumes:
      - redis_node1_data:/data

  redis-node-2:
    image: redis:7-alpine
    command: redis-server --port 7002 --cluster-enabled yes --cluster-config-file nodes.conf
    ports:
      - "7002:7002"
    volumes:
      - redis_node2_data:/data

  redis-node-3:
    image: redis:7-alpine
    command: redis-server --port 7003 --cluster-enabled yes --cluster-config-file nodes.conf
    ports:
      - "7003:7003"
    volumes:
      - redis_node3_data:/data

  # Stream Processing Service
  stream-processor:
    build: ./services/stream-processor
    environment:
      REDIS_CLUSTER_URLS: redis-node-1:7001,redis-node-2:7002,redis-node-3:7003
      TIMESCALE_URL: postgresql://timescale_user:${TIMESCALE_PASSWORD}@timescaledb:5432/judgify_timeseries
      POSTGRES_URL: postgresql://judgify_user:${POSTGRES_PASSWORD}@postgres:5432/judgify_db
    depends_on:
      - timescaledb
      - redis-node-1
      - redis-node-2
      - redis-node-3
    deploy:
      replicas: 3
      resources:
        limits:
          memory: 1G
          cpus: '1.0'

  # Dashboard WebSocket Service
  dashboard-websocket:
    build: ./services/dashboard-websocket
    environment:
      REDIS_CLUSTER_URLS: redis-node-1:7001,redis-node-2:7002,redis-node-3:7003
      TIMESCALE_URL: postgresql://timescale_user:${TIMESCALE_PASSWORD}@timescaledb:5432/judgify_timeseries
    ports:
      - "9000:9000"
    depends_on:
      - timescaledb
      - redis-node-1
    deploy:
      replicas: 2

  # Monitoring
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources

volumes:
  timescale_data:
  redis_node1_data:
  redis_node2_data:
  redis_node3_data:
  prometheus_data:
  grafana_data:
```

### 7.2 Kubernetes ë°°í¬ ì„¤ì •
```yaml
# k8s/stream-processor-deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: stream-processor
  namespace: judgify-prod
spec:
  replicas: 5
  selector:
    matchLabels:
      app: stream-processor
  template:
    metadata:
      labels:
        app: stream-processor
    spec:
      containers:
      - name: stream-processor
        image: judgify/stream-processor:v2.0.0
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        env:
        - name: REDIS_CLUSTER_URLS
          valueFrom:
            configMapKeyRef:
              name: redis-config
              key: cluster_urls
        - name: TIMESCALE_URL
          valueFrom:
            secretKeyRef:
              name: database-secrets
              key: timescale_url
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: stream-processor-service
  namespace: judgify-prod
spec:
  selector:
    app: stream-processor
  ports:
  - port: 80
    targetPort: 8000
  type: ClusterIP

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: stream-processor-hpa
  namespace: judgify-prod
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: stream-processor
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

## ğŸ“Š 8. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° ìµœì í™”

### 8.1 ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤
```python
# benchmark/performance_test.py
import asyncio
import aiohttp
import time
from typing import Dict, List
import json

class PerformanceTester:
    """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    
    def __init__(self, base_url: str, concurrent_users: int = 100):
        self.base_url = base_url
        self.concurrent_users = concurrent_users
        self.results = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'response_times': [],
            'errors': []
        }
    
    async def run_judgment_load_test(self, duration_seconds: int = 300):
        """íŒë‹¨ ì—”ì§„ ë¶€í•˜ í…ŒìŠ¤íŠ¸"""
        
        print(f"ì‹œì‘: {self.concurrent_users}ëª… ë™ì‹œ ì‚¬ìš©ì, {duration_seconds}ì´ˆ í…ŒìŠ¤íŠ¸")
        
        # í…ŒìŠ¤íŠ¸ ì‹œì‘ ì‹œê°„
        start_time = time.time()
        
        # ë™ì‹œ ì‚¬ìš©ì íƒœìŠ¤í¬ ìƒì„±
        tasks = []
        for i in range(self.concurrent_users):
            task = asyncio.create_task(
                self._user_simulation(i, start_time + duration_seconds)
            )
            tasks.append(task)
        
        # ëª¨ë“  íƒœìŠ¤í¬ ì™„ë£Œ ëŒ€ê¸°
        await asyncio.gather(*tasks)
        
        # ê²°ê³¼ ë¶„ì„
        await self._analyze_results()
    
    async def _user_simulation(self, user_id: int, end_time: float):
        """ë‹¨ì¼ ì‚¬ìš©ì ì‹œë®¬ë ˆì´ì…˜"""
        
        connector = aiohttp.TCPConnector(limit=100)
        async with aiohttp.ClientSession(connector=connector) as session:
            
            while time.time() < end_time:
                request_start = time.time()
                
                try:
                    judgment_data = self._generate_test_judgment_data(user_id)
                    
                    async with session.post(
                        f"{self.base_url}/api/v1/judgments/execute",
                        json=judgment_data,
                        headers={"Authorization": "Bearer test-token"}
                    ) as response:
                        
                        response_time = (time.time() - request_start) * 1000
                        
                        self.results['total_requests'] += 1
                        self.results['response_times'].append(response_time)
                        
                        if response.status == 200:
                            self.results['successful_requests'] += 1
                        else:
                            self.results['failed_requests'] += 1
                            self.results['errors'].append({
                                'status': response.status,
                                'response_time': response_time,
                                'user_id': user_id
                            })
                
                except Exception as e:
                    self.results['failed_requests'] += 1
                    self.results['errors'].append({
                        'error': str(e),
                        'user_id': user_id
                    })
                
                # ì‚¬ìš©ìë³„ ë‹¤ì–‘í•œ ìš”ì²­ ê°„ê²© (1-5ì´ˆ)
                await asyncio.sleep(1 + (user_id % 5))
    
    def _generate_test_judgment_data(self, user_id: int) -> Dict:
        """í…ŒìŠ¤íŠ¸ìš© íŒë‹¨ ë°ì´í„° ìƒì„±"""
        
        import random
        
        return {
            "workflow_id": f"test-workflow-{user_id % 10}",
            "input_data": {
                "temperature": random.uniform(70, 100),
                "pressure": random.uniform(80, 120),
                "vibration": random.uniform(30, 60),
                "machine_id": f"MACHINE_{user_id % 20}",
                "user_id": user_id
            },
            "context": {
                "shift": random.choice(["day", "night"]),
                "operator": f"operator_{user_id % 5}"
            }
        }
    
    async def _analyze_results(self):
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„"""
        
        if not self.results['response_times']:
            print("ì‘ë‹µ ì‹œê°„ ë°ì´í„° ì—†ìŒ")
            return
        
        response_times = sorted(self.results['response_times'])
        total_requests = self.results['total_requests']
        
        # í†µê³„ ê³„ì‚°
        avg_response_time = sum(response_times) / len(response_times)
        p50_response_time = response_times[len(response_times) // 2]
        p95_response_time = response_times[int(len(response_times) * 0.95)]
        p99_response_time = response_times[int(len(response_times) * 0.99)]
        
        success_rate = (self.results['successful_requests'] / total_requests) * 100
        
        # ì²˜ë¦¬ëŸ‰ ê³„ì‚° (RPS)
        test_duration = max(response_times) / 1000 if response_times else 0
        rps = total_requests / test_duration if test_duration > 0 else 0
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*50)
        print("ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼")
        print("="*50)
        print(f"ì´ ìš”ì²­ ìˆ˜: {total_requests:,}")
        print(f"ì„±ê³µ ìš”ì²­ ìˆ˜: {self.results['successful_requests']:,}")
        print(f"ì‹¤íŒ¨ ìš”ì²­ ìˆ˜: {self.results['failed_requests']:,}")
        print(f"ì„±ê³µë¥ : {success_rate:.2f}%")
        print(f"ì²˜ë¦¬ëŸ‰ (RPS): {rps:.2f}")
        print("\nì‘ë‹µ ì‹œê°„ í†µê³„ (ms):")
        print(f"í‰ê· : {avg_response_time:.2f}")
        print(f"P50: {p50_response_time:.2f}")
        print(f"P95: {p95_response_time:.2f}")
        print(f"P99: {p99_response_time:.2f}")
        
        # ëª©í‘œ ì„±ëŠ¥ ë‹¬ì„± ì—¬ë¶€ í™•ì¸
        print("\nëª©í‘œ ë‹¬ì„± ì—¬ë¶€:")
        print(f"ì‘ë‹µ ì‹œê°„ < 5000ms: {'âœ…' if p95_response_time < 5000 else 'âŒ'}")
        print(f"ì„±ê³µë¥  > 99%: {'âœ…' if success_rate > 99 else 'âŒ'}")
        print(f"ì²˜ë¦¬ëŸ‰ > 100 RPS: {'âœ…' if rps > 100 else 'âŒ'}")

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
async def main():
    tester = PerformanceTester(
        base_url="http://localhost:8002",
        concurrent_users=1000
    )
    
    await tester.run_judgment_load_test(duration_seconds=300)

if __name__ == "__main__":
    asyncio.run(main())
```

### 8.2 ìµœì í™” ê¶Œì¥ì‚¬í•­

#### ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”
```sql
-- ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ì¸ë±ìŠ¤ ìƒì„±
CREATE INDEX CONCURRENTLY idx_judgment_executions_tenant_created 
ON judgment_executions(tenant_id, created_at DESC) 
WHERE status = 'success';

-- íŒŒí‹°ì…”ë‹ì„ ìœ„í•œ íŠ¸ë¦¬ê±° í•¨ìˆ˜
CREATE OR REPLACE FUNCTION judgment_executions_partition_trigger()
RETURNS TRIGGER AS $$
BEGIN
    IF NEW.created_at >= DATE '2024-08-01' AND NEW.created_at < DATE '2024-09-01' THEN
        INSERT INTO judgment_executions_2024_08 VALUES (NEW.*);
    ELSIF NEW.created_at >= DATE '2024-09-01' AND NEW.created_at < DATE '2024-10-01' THEN
        INSERT INTO judgment_executions_2024_09 VALUES (NEW.*);
    ELSE
        RAISE EXCEPTION 'Date out of range. Fix the judgment_executions_partition_trigger() function!';
    END IF;
    RETURN NULL;
END;
$$ LANGUAGE plpgsql;
```

#### Redis í´ëŸ¬ìŠ¤í„° ìµœì í™”
```python
# redis_optimization.py
class OptimizedRedisManager:
    """ìµœì í™”ëœ Redis í´ëŸ¬ìŠ¤í„° ê´€ë¦¬"""
    
    def __init__(self, cluster_nodes: List[str]):
        self.cluster = redis.RedisCluster(
            startup_nodes=[{"host": node.split(':')[0], "port": int(node.split(':')[1])} 
                          for node in cluster_nodes],
            decode_responses=True,
            skip_full_coverage_check=True,
            max_connections=200,
            retry_on_timeout=True,
            socket_keepalive=True,
            socket_keepalive_options={
                1: 1,  # TCP_KEEPIDLE
                2: 3,  # TCP_KEEPINTVL  
                3: 5,  # TCP_KEEPCNT
            }
        )
    
    async def optimized_stream_write(self, stream_name: str, data: Dict[str, Any]):
        """ìµœì í™”ëœ ìŠ¤íŠ¸ë¦¼ ì“°ê¸°"""
        
        # ë°°ì¹˜ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
        pipe = self.cluster.pipeline()
        
        # ìŠ¤íŠ¸ë¦¼ì— ì¶”ê°€
        pipe.xadd(stream_name, data, maxlen=100000, approximate=True)
        
        # TTLì´ ìˆëŠ” ìºì‹œë„ í•¨ê»˜ ì„¤ì •
        cache_key = f"recent:{stream_name}:{data.get('id', '')}"
        pipe.setex(cache_key, 300, json.dumps(data))
        
        # ë°°ì¹˜ ì‹¤í–‰
        await pipe.execute()
```

ì´ ì¢…í•©ì ì¸ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì•„í‚¤í…ì²˜ëŠ” Judgify-core Ver2.0ì˜ ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­ì„ ì™„ì „íˆ ì¶©ì¡±í•˜ë©°, í™•ì¥ ê°€ëŠ¥í•˜ê³  ì•ˆì •ì ì¸ ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ ì‹œìŠ¤í…œì„ ì œê³µí•©ë‹ˆë‹¤.

**í•µì‹¬ ì„±ê³¼ ì§€í‘œ**:
- âš¡ ë™ì‹œ 1000+ íŒë‹¨ ìš”ì²­ ì²˜ë¦¬
- ğŸš€ <5ì´ˆ íŒë‹¨ ì‘ë‹µ ì‹œê°„
- ğŸ“Š <1ì´ˆ ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸
- ğŸ”’ 99.5% ë°ì´í„° ì¼ê´€ì„±
- ğŸ¯ ë¬´ì¤‘ë‹¨ ì„œë¹„ìŠ¤ (99.9% ê°€ìš©ì„±)
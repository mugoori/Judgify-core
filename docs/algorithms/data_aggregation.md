# ë°ì´í„° ì§‘ê³„ ì•Œê³ ë¦¬ì¦˜ - LLM í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ (Ver2.0 Final) ğŸ”¥

## 1. ê°œìš”

### 1.1 ì•Œê³ ë¦¬ì¦˜ ëª©ì 
Ver2.0 Finalì—ì„œëŠ” **ALL ë°ì´í„°ë¥¼ ì˜êµ¬ ë³´ê´€í•˜ë©´ì„œ LLMì— ì „ë‹¬í•  ë°ì´í„°ëŠ” í†µê³„ ì§‘ê³„**í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ LLM í• ë£¨ì‹œë„¤ì´ì…˜ì„ ë°©ì§€í•˜ê³  ì •í™•í•œ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

### 1.2 í•µì‹¬ ì „ëµ
| ì „ëµ | ì„¤ëª… | íš¨ê³¼ |
|------|------|------|
| **ì›ë³¸ ë°ì´í„° ì˜êµ¬ ë³´ê´€** | raw_data í…Œì´ë¸”ì— ëª¨ë“  ì›ì‹œ ë°ì´í„° ì €ì¥ | ê°ì‚¬ ì¶”ì , ì¬ë¶„ì„ ê°€ëŠ¥ |
| **í†µê³„ ì§‘ê³„ ì „ë‹¬** | LLMì—ëŠ” ì§‘ê³„ í†µê³„ë§Œ ì „ë‹¬ (ì›ì‹œ ë°ì´í„° X) | í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ |
| **3ë‹¨ê³„ ì§‘ê³„ ì „ëµ** | í†µê³„ + í‰ê°€ + íŠ¸ë Œë“œ ë¶„ì„ | ì •í™•ì„± í–¥ìƒ |
| **ê²€ì¦ ë©”ì»¤ë‹ˆì¦˜** | ì§‘ê³„ ê²°ê³¼ êµì°¨ ê²€ì¦ | ì‹ ë¢°ë„ ë³´ì¥ |

### 1.3 ë°ì´í„° ë³´ê´€ êµ¬ì¡°
```sql
-- 1. raw_data: ì›ì‹œ ë°ì´í„° ì˜êµ¬ ë³´ê´€ (ì ˆëŒ€ ì‚­ì œ ì•ˆ í•¨!)
CREATE TABLE raw_data (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    workflow_id UUID,
    input_data JSONB NOT NULL,         -- ì›ì‹œ ì…ë ¥ ë°ì´í„°
    result JSONB,                      -- ì›ì‹œ ê²°ê³¼ ë°ì´í„°
    feedback VARCHAR(10),              -- ğŸ‘ / ğŸ‘
    created_at TIMESTAMP DEFAULT NOW(),
    CONSTRAINT never_delete CHECK (true)  -- ì‚­ì œ ë°©ì§€ ì œì•½
);

-- 2. judgment_executions: ìµœê·¼ 90ì¼ ë°ì´í„° (ê³ ì† ì¡°íšŒ)
CREATE TABLE judgment_executions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    workflow_id UUID,
    input_data JSONB,
    result JSONB,
    feedback VARCHAR(10),
    created_at TIMESTAMP DEFAULT NOW()
);

-- 3. archived_judgments: 90ì¼ ì´ìƒ ë°ì´í„° (ì§‘ê³„ ì €ì¥)
CREATE TABLE archived_judgments (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    workflow_id UUID,
    time_period VARCHAR(20),           -- "2025-01", "2025-Q1"
    aggregated_data JSONB NOT NULL,    -- ì§‘ê³„ í†µê³„
    sample_count INTEGER,
    created_at TIMESTAMP DEFAULT NOW()
);
```

---

## 2. 3ë‹¨ê³„ ë°ì´í„° ì§‘ê³„ ì „ëµ

### 2.1 Stage 1: í†µê³„ì  ì§‘ê³„ (Statistical Aggregation)
**ëª©ì **: ì›ì‹œ ë°ì´í„°ë¥¼ ê¸°ìˆ  í†µê³„ë¡œ ìš”ì•½

```python
class StatisticalAggregator:
    def aggregate_statistics(
        self,
        raw_data: List[Dict],
        variables: List[str]
    ) -> Dict:
        """
        í†µê³„ì  ì§‘ê³„ ìˆ˜í–‰

        ì§‘ê³„ í•­ëª©:
        - ê¸°ë³¸ í†µê³„: í‰ê· , í‘œì¤€í¸ì°¨, ìµœì†Œ/ìµœëŒ€, ì¤‘ì•™ê°’
        - ë¶„í¬ í†µê³„: ì‚¬ë¶„ìœ„ìˆ˜, ë°±ë¶„ìœ„ìˆ˜
        - ì´ìƒì¹˜ íƒì§€: IQR ê¸°ë°˜ ì´ìƒì¹˜ ê°œìˆ˜
        """
        aggregated = {
            "total_samples": len(raw_data),
            "variables": {}
        }

        for var in variables:
            values = [d[var] for d in raw_data if var in d]

            if not values:
                continue

            # ìˆ˜ì¹˜í˜• ë³€ìˆ˜
            if isinstance(values[0], (int, float)):
                aggregated["variables"][var] = self._aggregate_numeric(values)

            # ë²”ì£¼í˜• ë³€ìˆ˜
            else:
                aggregated["variables"][var] = self._aggregate_categorical(values)

        return aggregated

    def _aggregate_numeric(self, values: List[float]) -> Dict:
        """
        ìˆ˜ì¹˜í˜• ë³€ìˆ˜ í†µê³„ ì§‘ê³„
        """
        import numpy as np

        return {
            # ê¸°ë³¸ í†µê³„
            "mean": float(np.mean(values)),
            "std": float(np.std(values)),
            "min": float(np.min(values)),
            "max": float(np.max(values)),
            "median": float(np.median(values)),

            # ë¶„í¬ í†µê³„
            "q25": float(np.percentile(values, 25)),
            "q50": float(np.percentile(values, 50)),
            "q75": float(np.percentile(values, 75)),

            # ì´ìƒì¹˜ íƒì§€
            "outlier_count": self._count_outliers(values),
            "outlier_percentage": self._count_outliers(values) / len(values) * 100
        }

    def _aggregate_categorical(self, values: List[str]) -> Dict:
        """
        ë²”ì£¼í˜• ë³€ìˆ˜ í†µê³„ ì§‘ê³„
        """
        from collections import Counter

        counts = Counter(values)
        total = len(values)

        return {
            "unique_count": len(counts),
            "most_common": counts.most_common(5),  # ìƒìœ„ 5ê°œ
            "frequency_distribution": {
                value: count / total for value, count in counts.items()
            }
        }

    def _count_outliers(self, values: List[float]) -> int:
        """
        IQR ê¸°ë°˜ ì´ìƒì¹˜ ê°œìˆ˜ ê³„ì‚°
        """
        import numpy as np

        q1 = np.percentile(values, 25)
        q3 = np.percentile(values, 75)
        iqr = q3 - q1

        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr

        outliers = [v for v in values if v < lower_bound or v > upper_bound]

        return len(outliers)
```

#### ì§‘ê³„ ê²°ê³¼ ì˜ˆì‹œ
```json
{
  "total_samples": 100,
  "variables": {
    "temperature": {
      "mean": 86.5,
      "std": 3.2,
      "min": 78.0,
      "max": 95.0,
      "median": 87.0,
      "q25": 84.0,
      "q50": 87.0,
      "q75": 89.0,
      "outlier_count": 3,
      "outlier_percentage": 3.0
    },
    "vibration": {
      "mean": 42.1,
      "std": 2.5,
      "min": 35.0,
      "max": 48.0,
      "median": 42.0,
      "q25": 40.0,
      "q50": 42.0,
      "q75": 44.0,
      "outlier_count": 2,
      "outlier_percentage": 2.0
    }
  }
}
```

### 2.2 Stage 2: í‰ê°€ì  ì§‘ê³„ (Evaluative Aggregation)
**ëª©ì **: í”¼ë“œë°± ë° ì„±ëŠ¥ ì§€í‘œ ì§‘ê³„

```python
class EvaluativeAggregator:
    def aggregate_evaluation(
        self,
        raw_data: List[Dict],
        feedback_key: str = "feedback"
    ) -> Dict:
        """
        í‰ê°€ ì§€í‘œ ì§‘ê³„

        ì§‘ê³„ í•­ëª©:
        - í”¼ë“œë°± ë¶„í¬: ğŸ‘ / ğŸ‘ ë¹„ìœ¨
        - ì •í™•ë„ ì§€í‘œ: ì„±ê³µë¥ , ì‹¤íŒ¨ìœ¨
        - ì‹ ë¢°ë„ ë¶„í¬: ê³ /ì¤‘/ì € ì‹ ë¢°ë„ ë¹„ìœ¨
        """
        total = len(raw_data)

        # í”¼ë“œë°± ë¶„í¬
        positive_count = sum(1 for d in raw_data if d.get(feedback_key) == "ğŸ‘")
        negative_count = sum(1 for d in raw_data if d.get(feedback_key) == "ğŸ‘")
        no_feedback_count = total - positive_count - negative_count

        # ì‹ ë¢°ë„ ë¶„í¬ (confidence 0.8 ì´ìƒ = ê³ , 0.5-0.8 = ì¤‘, 0.5 ë¯¸ë§Œ = ì €)
        high_conf = sum(1 for d in raw_data if d.get("confidence", 0) >= 0.8)
        mid_conf = sum(1 for d in raw_data if 0.5 <= d.get("confidence", 0) < 0.8)
        low_conf = sum(1 for d in raw_data if d.get("confidence", 0) < 0.5)

        return {
            "feedback_distribution": {
                "positive": positive_count,
                "negative": negative_count,
                "no_feedback": no_feedback_count,
                "positive_rate": positive_count / total if total > 0 else 0,
                "negative_rate": negative_count / total if total > 0 else 0
            },
            "confidence_distribution": {
                "high": high_conf,
                "medium": mid_conf,
                "low": low_conf,
                "high_rate": high_conf / total if total > 0 else 0,
                "medium_rate": mid_conf / total if total > 0 else 0,
                "low_rate": low_conf / total if total > 0 else 0
            },
            "performance_metrics": {
                "success_rate": positive_count / (positive_count + negative_count) if (positive_count + negative_count) > 0 else 0,
                "avg_confidence": sum(d.get("confidence", 0) for d in raw_data) / total if total > 0 else 0
            }
        }
```

#### ì§‘ê³„ ê²°ê³¼ ì˜ˆì‹œ
```json
{
  "feedback_distribution": {
    "positive": 85,
    "negative": 15,
    "no_feedback": 0,
    "positive_rate": 0.85,
    "negative_rate": 0.15
  },
  "confidence_distribution": {
    "high": 72,
    "medium": 25,
    "low": 3,
    "high_rate": 0.72,
    "medium_rate": 0.25,
    "low_rate": 0.03
  },
  "performance_metrics": {
    "success_rate": 0.85,
    "avg_confidence": 0.81
  }
}
```

### 2.3 Stage 3: íŠ¸ë Œë“œ ì§‘ê³„ (Trend Aggregation)
**ëª©ì **: ì‹œê°„ë³„ ë³€í™” ì¶”ì´ ë¶„ì„

```python
class TrendAggregator:
    def aggregate_trend(
        self,
        raw_data: List[Dict],
        time_key: str = "created_at",
        time_unit: str = "hour"  # hour | day | week | month
    ) -> Dict:
        """
        ì‹œê³„ì—´ íŠ¸ë Œë“œ ì§‘ê³„

        ì§‘ê³„ í•­ëª©:
        - ì‹œê°„ëŒ€ë³„ ìƒ˜í”Œ ìˆ˜
        - ì‹œê°„ëŒ€ë³„ í‰ê·  ì‹ ë¢°ë„
        - ì‹œê°„ëŒ€ë³„ ì„±ê³µë¥ 
        - ë³€í™”ìœ¨ (ì¦ê°€/ê°ì†Œ ì¶”ì„¸)
        """
        import pandas as pd

        # DataFrame ë³€í™˜
        df = pd.DataFrame(raw_data)
        df[time_key] = pd.to_datetime(df[time_key])

        # ì‹œê°„ ë‹¨ìœ„ë³„ ê·¸ë£¹í™”
        if time_unit == "hour":
            df["time_group"] = df[time_key].dt.floor("h")
        elif time_unit == "day":
            df["time_group"] = df[time_key].dt.date
        elif time_unit == "week":
            df["time_group"] = df[time_key].dt.to_period("W")
        elif time_unit == "month":
            df["time_group"] = df[time_key].dt.to_period("M")

        # ì‹œê°„ëŒ€ë³„ ì§‘ê³„
        trend_data = []
        for time_group, group_df in df.groupby("time_group"):
            positive_count = (group_df["feedback"] == "ğŸ‘").sum()
            total_count = len(group_df)

            trend_data.append({
                "time": str(time_group),
                "sample_count": total_count,
                "avg_confidence": group_df["confidence"].mean(),
                "success_rate": positive_count / total_count if total_count > 0 else 0
            })

        # ë³€í™”ìœ¨ ê³„ì‚°
        if len(trend_data) >= 2:
            first_success_rate = trend_data[0]["success_rate"]
            last_success_rate = trend_data[-1]["success_rate"]
            change_rate = (last_success_rate - first_success_rate) / first_success_rate if first_success_rate > 0 else 0
        else:
            change_rate = 0

        return {
            "time_unit": time_unit,
            "trend_data": trend_data,
            "summary": {
                "total_periods": len(trend_data),
                "change_rate": change_rate,
                "trend_direction": "increasing" if change_rate > 0.05 else "decreasing" if change_rate < -0.05 else "stable"
            }
        }
```

#### ì§‘ê³„ ê²°ê³¼ ì˜ˆì‹œ
```json
{
  "time_unit": "day",
  "trend_data": [
    {
      "time": "2025-10-10",
      "sample_count": 25,
      "avg_confidence": 0.78,
      "success_rate": 0.80
    },
    {
      "time": "2025-10-11",
      "sample_count": 28,
      "avg_confidence": 0.82,
      "success_rate": 0.86
    },
    {
      "time": "2025-10-12",
      "sample_count": 30,
      "avg_confidence": 0.85,
      "success_rate": 0.90
    }
  ],
  "summary": {
    "total_periods": 3,
    "change_rate": 0.125,
    "trend_direction": "increasing"
  }
}
```

---

## 3. í†µí•© ì§‘ê³„ íŒŒì´í”„ë¼ì¸

### 3.1 ì „ì²´ ì§‘ê³„ í”„ë¡œì„¸ìŠ¤
```python
class DataAggregationPipeline:
    def __init__(self):
        self.statistical_aggregator = StatisticalAggregator()
        self.evaluative_aggregator = EvaluativeAggregator()
        self.trend_aggregator = TrendAggregator()

    async def aggregate_all(
        self,
        workflow_id: str,
        time_period: str = "last_90_days"
    ) -> Dict:
        """
        ì „ì²´ ë°ì´í„° ì§‘ê³„ íŒŒì´í”„ë¼ì¸

        ì…ë ¥: workflow_id + time_period
        ì¶œë ¥: 3ë‹¨ê³„ ì§‘ê³„ ê²°ê³¼ í†µí•©
        """
        # 1. ì›ì‹œ ë°ì´í„° ë¡œë“œ (raw_data í…Œì´ë¸”)
        raw_data = await self._load_raw_data(workflow_id, time_period)

        if len(raw_data) < 10:
            raise ValueError(f"ìƒ˜í”Œ ë¶€ì¡±: {len(raw_data)}ê°œ (ìµœì†Œ 10ê°œ)")

        # 2. ë³€ìˆ˜ ëª©ë¡ ì¶”ì¶œ
        variables = self._extract_variables(raw_data)

        # 3. 3ë‹¨ê³„ ì§‘ê³„ ì‹¤í–‰
        statistical_result = self.statistical_aggregator.aggregate_statistics(
            raw_data, variables
        )

        evaluative_result = self.evaluative_aggregator.aggregate_evaluation(
            raw_data
        )

        trend_result = self.trend_aggregator.aggregate_trend(
            raw_data, time_unit="day"
        )

        # 4. í†µí•© ê²°ê³¼ ìƒì„±
        aggregated_result = {
            "workflow_id": workflow_id,
            "time_period": time_period,
            "aggregation_timestamp": datetime.now().isoformat(),
            "sample_count": len(raw_data),
            "statistical_summary": statistical_result,
            "evaluative_summary": evaluative_result,
            "trend_summary": trend_result
        }

        # 5. ê²€ì¦
        validation_result = self._validate_aggregation(aggregated_result)

        if not validation_result["valid"]:
            raise ValueError(f"ì§‘ê³„ ê²€ì¦ ì‹¤íŒ¨: {validation_result['errors']}")

        return aggregated_result

    async def _load_raw_data(self, workflow_id: str, time_period: str) -> List[Dict]:
        """
        raw_data í…Œì´ë¸”ì—ì„œ ì›ì‹œ ë°ì´í„° ë¡œë“œ
        """
        if time_period == "last_90_days":
            query = f"""
            SELECT input_data, result, feedback, confidence, created_at
            FROM raw_data
            WHERE workflow_id = '{workflow_id}'
              AND created_at >= NOW() - INTERVAL '90 days'
            ORDER BY created_at DESC
            """
        # ... ê¸°íƒ€ time_period ì²˜ë¦¬

        return await db.fetch_all(query)

    def _validate_aggregation(self, aggregated_result: Dict) -> Dict:
        """
        ì§‘ê³„ ê²°ê³¼ ê²€ì¦

        ê²€ì¦ í•­ëª©:
        1. ìƒ˜í”Œ ìˆ˜ ì¼ì¹˜ (í†µê³„/í‰ê°€/íŠ¸ë Œë“œ í•©ì‚° = ì „ì²´)
        2. ë¹„ìœ¨ í•©ì‚° = 1.0 (í”¼ë“œë°± ë¶„í¬, ì‹ ë¢°ë„ ë¶„í¬)
        3. í†µê³„ ë²”ìœ„ ê²€ì¦ (í‰ê·  < ìµœëŒ€, í‰ê·  > ìµœì†Œ)
        4. ì´ìƒì¹˜ ë¹„ìœ¨ < 10% (ì •ìƒ ë°ì´í„° ë³´ì¥)
        """
        errors = []

        # 1. ìƒ˜í”Œ ìˆ˜ ì¼ì¹˜
        expected_count = aggregated_result["sample_count"]
        feedback_total = (
            aggregated_result["evaluative_summary"]["feedback_distribution"]["positive"] +
            aggregated_result["evaluative_summary"]["feedback_distribution"]["negative"] +
            aggregated_result["evaluative_summary"]["feedback_distribution"]["no_feedback"]
        )

        if expected_count != feedback_total:
            errors.append(f"ìƒ˜í”Œ ìˆ˜ ë¶ˆì¼ì¹˜: {expected_count} != {feedback_total}")

        # 2. ë¹„ìœ¨ í•©ì‚° ê²€ì¦
        feedback_rates = (
            aggregated_result["evaluative_summary"]["feedback_distribution"]["positive_rate"] +
            aggregated_result["evaluative_summary"]["feedback_distribution"]["negative_rate"]
        )

        if abs(feedback_rates - 1.0) > 0.01:  # ì˜¤ì°¨ í—ˆìš© 0.01
            errors.append(f"í”¼ë“œë°± ë¹„ìœ¨ í•©ì‚° ì˜¤ë¥˜: {feedback_rates} != 1.0")

        # 3. í†µê³„ ë²”ìœ„ ê²€ì¦
        for var, stats in aggregated_result["statistical_summary"]["variables"].items():
            if "mean" in stats:
                if not (stats["min"] <= stats["mean"] <= stats["max"]):
                    errors.append(f"{var} í†µê³„ ë²”ìœ„ ì˜¤ë¥˜: min={stats['min']}, mean={stats['mean']}, max={stats['max']}")

        # 4. ì´ìƒì¹˜ ë¹„ìœ¨ ê²€ì¦
        for var, stats in aggregated_result["statistical_summary"]["variables"].items():
            if "outlier_percentage" in stats:
                if stats["outlier_percentage"] > 10:
                    errors.append(f"{var} ì´ìƒì¹˜ ë¹„ìœ¨ ê³¼ë‹¤: {stats['outlier_percentage']}%")

        return {
            "valid": len(errors) == 0,
            "errors": errors
        }
```

---

## 4. LLM í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ ë©”ì»¤ë‹ˆì¦˜

### 4.1 í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ ì „ëµ
```python
class LLMHallucinationPrevention:
    async def prepare_llm_context(
        self,
        aggregated_data: Dict
    ) -> str:
        """
        LLMì— ì „ë‹¬í•  ì•ˆì „í•œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±

        í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€:
        1. ì›ì‹œ ë°ì´í„° ì ˆëŒ€ ì „ë‹¬ ê¸ˆì§€ â†’ ì§‘ê³„ í†µê³„ë§Œ ì „ë‹¬
        2. ëª…í™•í•œ ì œì•½ ì¡°ê±´ ëª…ì‹œ
        3. ê²€ì¦ ê°€ëŠ¥í•œ ê·¼ê±° ìš”êµ¬
        4. ë¶ˆí™•ì‹¤ì„± í‘œí˜„ í—ˆìš©
        """
        context = f"""
## ë°ì´í„° ë¶„ì„ ì»¨í…ìŠ¤íŠ¸

**ì¤‘ìš”**: ì•„ë˜ ë°ì´í„°ëŠ” í†µê³„ ì§‘ê³„ ê²°ê³¼ì…ë‹ˆë‹¤. ì›ì‹œ ë°ì´í„°ê°€ ì•„ë‹™ë‹ˆë‹¤.
ì§‘ê³„ í†µê³„ì— ê¸°ë°˜í•œ ë¶„ì„ë§Œ ìˆ˜í–‰í•˜ê³ , êµ¬ì²´ì ì¸ ê°œë³„ ì‚¬ë¡€ëŠ” ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.

### ì „ì²´ ìš”ì•½
- ì´ ìƒ˜í”Œ ìˆ˜: {aggregated_data['sample_count']}ê°œ
- ë¶„ì„ ê¸°ê°„: {aggregated_data['time_period']}
- ì§‘ê³„ ì‹œê°„: {aggregated_data['aggregation_timestamp']}

### í†µê³„ì  ìš”ì•½
{json.dumps(aggregated_data['statistical_summary'], indent=2)}

### í‰ê°€ì  ìš”ì•½
{json.dumps(aggregated_data['evaluative_summary'], indent=2)}

### íŠ¸ë Œë“œ ìš”ì•½
{json.dumps(aggregated_data['trend_summary'], indent=2)}

## ë¶„ì„ ì§€ì¹¨
1. **ê·¼ê±° ê¸°ë°˜ ë¶„ì„**: ì œê³µëœ í†µê³„ ìˆ˜ì¹˜ì—ë§Œ ê¸°ë°˜í•˜ì—¬ ë¶„ì„
2. **ë¶ˆí™•ì‹¤ì„± í‘œí˜„**: í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ "í†µê³„ì— ë”°ë¥´ë©´ ~ì¸ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤" í‘œí˜„ ì‚¬ìš©
3. **ê°œë³„ ì‚¬ë¡€ ê¸ˆì§€**: "íŠ¹ì • ì¼€ì´ìŠ¤ì—ì„œ ~" ê°™ì€ í‘œí˜„ ê¸ˆì§€
4. **ìˆ˜ì¹˜ ì¸ìš©**: ë¶„ì„ ê²°ê³¼ì— ë°˜ë“œì‹œ í†µê³„ ìˆ˜ì¹˜ ì¸ìš©

## ê¸ˆì§€ ì‚¬í•­
âŒ ì›ì‹œ ë°ì´í„° ì–¸ê¸‰ ê¸ˆì§€
âŒ êµ¬ì²´ì  ê°œë³„ ì‚¬ë¡€ ì–¸ê¸‰ ê¸ˆì§€
âŒ ì§‘ê³„ í†µê³„ ë²—ì–´ë‚œ ì¶”ì¸¡ ê¸ˆì§€
âŒ "ì²« ë²ˆì§¸ ìƒ˜í”Œì€ ~", "ë§ˆì§€ë§‰ ì¼€ì´ìŠ¤ëŠ” ~" í‘œí˜„ ê¸ˆì§€
"""

        return context

    async def validate_llm_response(
        self,
        llm_response: str,
        aggregated_data: Dict
    ) -> Dict:
        """
        LLM ì‘ë‹µ ê²€ì¦ (í• ë£¨ì‹œë„¤ì´ì…˜ íƒì§€)

        ê²€ì¦ í•­ëª©:
        1. ì œê³µë˜ì§€ ì•Šì€ ìˆ˜ì¹˜ ì–¸ê¸‰ ì—¬ë¶€
        2. ê°œë³„ ì‚¬ë¡€ ì–¸ê¸‰ ì—¬ë¶€
        3. í†µê³„ ë²”ìœ„ ë²—ì–´ë‚œ ì£¼ì¥ ì—¬ë¶€
        """
        issues = []

        # 1. ì œê³µë˜ì§€ ì•Šì€ ìˆ˜ì¹˜ ì–¸ê¸‰ ê²€ì¦
        mentioned_numbers = self._extract_numbers(llm_response)
        provided_numbers = self._extract_all_numbers(aggregated_data)

        for num in mentioned_numbers:
            if num not in provided_numbers and abs(num - 100) > 0.01:  # ë°±ë¶„ìœ¨ ì œì™¸
                issues.append(f"ì œê³µë˜ì§€ ì•Šì€ ìˆ˜ì¹˜ ì–¸ê¸‰: {num}")

        # 2. ê°œë³„ ì‚¬ë¡€ ì–¸ê¸‰ ê¸ˆì§€ íŒ¨í„´
        forbidden_patterns = [
            r"ì²«\s*ë²ˆì§¸\s*ìƒ˜í”Œ",
            r"ë§ˆì§€ë§‰\s*ì¼€ì´ìŠ¤",
            r"íŠ¹ì •\s*ì‚¬ë¡€",
            r"ê°œë³„\s*ë°ì´í„°",
            r"\d+ë²ˆì§¸\s*íŒë‹¨"
        ]

        import re
        for pattern in forbidden_patterns:
            if re.search(pattern, llm_response):
                issues.append(f"ê°œë³„ ì‚¬ë¡€ ì–¸ê¸‰ íŒ¨í„´ íƒì§€: {pattern}")

        # 3. í†µê³„ ë²”ìœ„ ë²—ì–´ë‚œ ì£¼ì¥
        for var, stats in aggregated_data["statistical_summary"]["variables"].items():
            if "mean" in stats:
                # LLMì´ í‰ê· ë³´ë‹¤ í›¨ì”¬ ë†’ê±°ë‚˜ ë‚®ì€ ê°’ì„ ì–¸ê¸‰í–ˆëŠ”ì§€ í™•ì¸
                # (ì˜ˆ: í‰ê·  86.5ì¸ë° "ëŒ€ë¶€ë¶„ 100 ì´ìƒ"ì´ë¼ê³  ì£¼ì¥)
                pass  # êµ¬í˜„ ìƒëµ

        return {
            "valid": len(issues) == 0,
            "issues": issues,
            "hallucination_detected": len(issues) > 0
        }
```

### 4.2 ì•ˆì „í•œ LLM í˜¸ì¶œ ì˜ˆì‹œ
```python
async def safe_llm_analysis(workflow_id: str) -> Dict:
    """
    í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ëœ ì•ˆì „í•œ LLM ë¶„ì„
    """
    # 1. ë°ì´í„° ì§‘ê³„
    pipeline = DataAggregationPipeline()
    aggregated_data = await pipeline.aggregate_all(workflow_id, "last_90_days")

    # 2. ì•ˆì „í•œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    prevention = LLMHallucinationPrevention()
    safe_context = await prevention.prepare_llm_context(aggregated_data)

    # 3. LLM í˜¸ì¶œ
    response = await openai.ChatCompletion.acreate(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "ë„ˆëŠ” í†µê³„ ê¸°ë°˜ ë°ì´í„° ë¶„ì„ê°€ì•¼. ì œê³µëœ ì§‘ê³„ í†µê³„ì—ë§Œ ê¸°ë°˜í•˜ì—¬ ë¶„ì„í•´."},
            {"role": "user", "content": safe_context}
        ],
        temperature=0.3  # ë‚®ì€ temperature = ì¼ê´€ì„±
    )

    llm_analysis = response.choices[0].message.content

    # 4. ì‘ë‹µ ê²€ì¦
    validation_result = await prevention.validate_llm_response(
        llm_analysis, aggregated_data
    )

    if validation_result["hallucination_detected"]:
        # í• ë£¨ì‹œë„¤ì´ì…˜ íƒì§€ì‹œ ì¬ì‹œë„ ë˜ëŠ” ê²½ê³ 
        logger.warning(f"LLM í• ë£¨ì‹œë„¤ì´ì…˜ íƒì§€: {validation_result['issues']}")

        # ì¬ì‹œë„ (ë” ì—„ê²©í•œ í”„ë¡¬í”„íŠ¸)
        safe_context += "\n\n**ê²½ê³ **: ì´ì „ ì‘ë‹µì—ì„œ ì œê³µë˜ì§€ ì•Šì€ ì •ë³´ë¥¼ ì–¸ê¸‰í–ˆìŠµë‹ˆë‹¤. í†µê³„ ìˆ˜ì¹˜ë§Œ ì¸ìš©í•˜ì„¸ìš”."
        # ... ì¬ì‹œë„ ë¡œì§

    return {
        "llm_analysis": llm_analysis,
        "validation": validation_result,
        "aggregated_data": aggregated_data
    }
```

---

## 5. ë°ì´í„° ì•„ì¹´ì´ë¹™ ì „ëµ

### 5.1 ìë™ ì•„ì¹´ì´ë¹™ í”„ë¡œì„¸ìŠ¤
```python
class DataArchiver:
    async def archive_old_data(self):
        """
        90ì¼ ì´ìƒ ë°ì´í„° ìë™ ì•„ì¹´ì´ë¹™

        í”„ë¡œì„¸ìŠ¤:
        1. judgment_executionsì—ì„œ 90ì¼ ì´ìƒ ë°ì´í„° ì¡°íšŒ
        2. ë°ì´í„° ì§‘ê³„ ì‹¤í–‰
        3. archived_judgmentsì— ì§‘ê³„ ê²°ê³¼ ì €ì¥
        4. judgment_executionsì—ì„œ ì›ë³¸ ì‚­ì œ
        5. raw_dataëŠ” ì ˆëŒ€ ì‚­ì œ ì•ˆ í•¨!
        """
        # 1. 90ì¼ ì´ìƒ ë°ì´í„° ì¡°íšŒ
        old_data = await db.fetch_all(f"""
            SELECT workflow_id, input_data, result, feedback, confidence, created_at
            FROM judgment_executions
            WHERE created_at < NOW() - INTERVAL '90 days'
        """)

        if not old_data:
            logger.info("ì•„ì¹´ì´ë¹™í•  ë°ì´í„° ì—†ìŒ")
            return

        # 2. ì›Œí¬í”Œë¡œìš°ë³„ë¡œ ê·¸ë£¹í™”
        workflow_groups = {}
        for data in old_data:
            wf_id = data["workflow_id"]
            if wf_id not in workflow_groups:
                workflow_groups[wf_id] = []
            workflow_groups[wf_id].append(data)

        # 3. ì›Œí¬í”Œë¡œìš°ë³„ ì§‘ê³„ ë° ì•„ì¹´ì´ë¹™
        for workflow_id, group_data in workflow_groups.items():
            # ì§‘ê³„ ì‹¤í–‰
            pipeline = DataAggregationPipeline()
            aggregated = await pipeline.aggregate_all(workflow_id, "archived")

            # ì•„ì¹´ì´ë¸Œ ì €ì¥
            await db.insert("archived_judgments", {
                "workflow_id": workflow_id,
                "time_period": self._get_time_period(group_data[0]["created_at"]),
                "aggregated_data": aggregated,
                "sample_count": len(group_data),
                "created_at": datetime.now()
            })

        # 4. judgment_executionsì—ì„œ ì‚­ì œ
        await db.execute(f"""
            DELETE FROM judgment_executions
            WHERE created_at < NOW() - INTERVAL '90 days'
        """)

        logger.info(f"ì•„ì¹´ì´ë¹™ ì™„ë£Œ: {len(workflow_groups)} ì›Œí¬í”Œë¡œìš°")

    def _get_time_period(self, timestamp: datetime) -> str:
        """
        íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ê¸°ê°„ ë¬¸ìì—´ë¡œ ë³€í™˜

        ì˜ˆì‹œ:
        2025-10-16 â†’ "2025-10"
        2025-01-15 â†’ "2025-Q1"
        """
        return timestamp.strftime("%Y-%m")
```

### 5.2 ì•„ì¹´ì´ë¸Œ ë°ì´í„° ë³µì›
```python
async def restore_archived_data(workflow_id: str, time_period: str) -> Dict:
    """
    ì•„ì¹´ì´ë¸Œëœ ì§‘ê³„ ë°ì´í„° ë³µì›

    ì£¼ì˜: ì›ì‹œ ë°ì´í„°ëŠ” ë³µì› ë¶ˆê°€ (ì§‘ê³„ í†µê³„ë§Œ ë³µì›)
    """
    archived = await db.fetch_one(f"""
        SELECT aggregated_data
        FROM archived_judgments
        WHERE workflow_id = '{workflow_id}'
          AND time_period = '{time_period}'
    """)

    if not archived:
        raise ValueError(f"ì•„ì¹´ì´ë¸Œ ë°ì´í„° ì—†ìŒ: {workflow_id} / {time_period}")

    return archived["aggregated_data"]
```

---

## 6. ì„±ëŠ¥ ìµœì í™” ë° ëª¨ë‹ˆí„°ë§

### 6.1 ì„±ëŠ¥ ëª©í‘œ
```yaml
ì§‘ê³„ ì„±ëŠ¥:
  - í†µê³„ ì§‘ê³„: < 1ì´ˆ (10,000ê°œ ìƒ˜í”Œ)
  - í‰ê°€ ì§‘ê³„: < 500ms
  - íŠ¸ë Œë“œ ì§‘ê³„: < 1ì´ˆ
  - ì „ì²´ ì§‘ê³„ íŒŒì´í”„ë¼ì¸: < 3ì´ˆ

ì•„ì¹´ì´ë¹™ ì„±ëŠ¥:
  - ì¼ì¼ ìë™ ì•„ì¹´ì´ë¹™: < 5ë¶„
  - ì›Œí¬í”Œë¡œìš°ë‹¹ ì•„ì¹´ì´ë¹™: < 10ì´ˆ

ê²€ì¦ ì„±ëŠ¥:
  - ì§‘ê³„ ê²€ì¦: < 200ms
  - LLM ì‘ë‹µ ê²€ì¦: < 500ms
```

### 6.2 ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­
```python
# Prometheus ë©”íŠ¸ë¦­

aggregation_duration = Histogram(
    'data_aggregation_duration_seconds',
    'Duration of data aggregation',
    ['stage']  # statistical | evaluative | trend
)

archived_records_total = Counter(
    'data_archived_records_total',
    'Total number of archived records',
    ['workflow_id']
)

hallucination_detected_total = Counter(
    'llm_hallucination_detected_total',
    'Total number of hallucination detections',
    ['validation_type']
)

raw_data_size_bytes = Gauge(
    'raw_data_size_bytes',
    'Total size of raw_data table'
)
```

---

## 7. ì¶”ê°€ ì°¸ì¡° ë¬¸ì„œ

- **`docs/services/learning_service.md`**: Learning Service ì „ì²´ ì•„í‚¤í…ì²˜
- **`docs/algorithms/auto_rule_extraction.md`**: ìë™ Rule ì¶”ì¶œ ì•Œê³ ë¦¬ì¦˜ (ì§‘ê³„ ë°ì´í„° í™œìš©)
- **`docs/architecture/database_design.md`**: raw_data, archived_judgments í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ

---

**Ver2.0 Final í•µì‹¬ í˜ì‹ **: ALL ë°ì´í„° ì˜êµ¬ ë³´ê´€ + ì§‘ê³„ í†µê³„ë¡œ LLM í• ë£¨ì‹œë„¤ì´ì…˜ ì™„ë²½ ë°©ì§€! ğŸ”¥
